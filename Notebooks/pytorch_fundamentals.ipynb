{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsw7zRxM90S4x09bo/H7Vg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManantenaKiady/Pytorch-fundamentals/blob/master/Notebooks/pytorch_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What we will cover\n",
        "\n",
        "In this notebook, we will explore the building blocks of Pytorch \n",
        "- What is Pytorch ?\n",
        "- Why Pytorch ?\n",
        "- Installing Pytorch\n",
        "- Tensors\n",
        "- Learning Algorithms (Backpropagation)\n",
        "  - Forward and Backward Pass\n",
        "  - Auto-Grad\n",
        "  - Optimizers\n",
        "- Workflow\n",
        "  - Toy Project: Linear Regression\n",
        "  - DataLoader"
      ],
      "metadata": {
        "id": "qW0i2hltaTvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- What is Pytorch\n",
        "\n",
        "It is an open source deep learning and machine learning framework developed by Meta (Facebook) and now in maintained by Linux Fundation community. \n",
        "\n",
        "Link: https://pytorch.org/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fmQHHG-aEo_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Why Pytorch ?\n",
        "\n",
        "- Used by the worlds largest tech companies such as Meta (Facebook), Tesla, Microsoft and Open AI.\n",
        "\n",
        "- The most used deep learning framework in research.\n",
        "\n",
        "- Pythonic"
      ],
      "metadata": {
        "id": "8bkvGeIkF1Iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Setup\n",
        "\n",
        "Setting up Pytorch 1.13.0, the latest stable release along with other needed libraries and packages."
      ],
      "metadata": {
        "id": "3CMW9YBCK2fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install Pytorch\n",
        "\n",
        "* Go to https://pytorch.org/get-started/locally/ and download the latest Pytorch release. Follows the instructions"
      ],
      "metadata": {
        "id": "mjffCWtOGlr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio\n",
        "\n",
        "# Check the installed version \n",
        "import torch\n",
        "torch.__version__\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "T1e84HnmO-A7",
        "outputId": "25e5ef96-8180-468f-9c5f-8657febe04f2"
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.0+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure"
      ],
      "metadata": {
        "id": "D5PucLM_PFLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU are available and set the device to use it\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WOymlHgPEsO",
        "outputId": "af336c0f-e7d7-42c5-fa41-a2e44220c74c"
      },
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Tensors\n",
        "\n",
        "Tensors are a specialized data structure that are very similar to arrays and matrices. (Multidimentional Arrays)\n",
        " \n",
        "*Source: https://pytorch.org/tutorials/*\n",
        "\n",
        "eg of tensors: \n",
        "\n",
        "`scalar: 1`\n",
        "\n",
        "`vector: [1, 2, 3]`\n",
        "\n",
        "`matrices: [[1,2,3][4,5,6]]`"
      ],
      "metadata": {
        "id": "2jzRRc2fe_0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {
        "id": "tWn-SqNVaMtX"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Tensors with `torch`"
      ],
      "metadata": {
        "id": "jOnP5mbghqp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Python list**"
      ],
      "metadata": {
        "id": "PFeLQaeai5h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of list in Python \n",
        "m = [[1,2,3],[4,5,6]]\n",
        "# Creating a tensor from list of list\n",
        "M = torch.tensor(m)\n",
        "\n",
        "print(f\"Type of m: {type(m)}\")\n",
        "print(f\"Type of M: {type(M)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWrfT_rnhZfd",
        "outputId": "64cd612e-8f5a-4a00-e47c-b038c108e90e"
      },
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of m: <class 'list'>\n",
            "Type of M: <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Arrays (Numpy)**\n",
        "\n",
        "What is numpy, who knows ?\n",
        "\n",
        "Source: https://numpy.org/"
      ],
      "metadata": {
        "id": "HwmnQftGjBS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "# Convert m into numpy array\n",
        "arr = np.array(m)\n",
        "print(f\"Type of arr: {type(arr)}\")\n",
        "# Transform arr into tensor\n",
        "ts_arr = torch.tensor(arr)\n",
        "print(f\"Type of ts_arr: {type(ts_arr)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL5gfvUrijmF",
        "outputId": "f1c29619-c84d-4d7a-83be-f6f30ecd8e5a"
      },
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of arr: <class 'numpy.ndarray'>\n",
            "Type of ts_arr: <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From another Tensor ?"
      ],
      "metadata": {
        "id": "jLxIsznMQcrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor Attributes**\n",
        "\n",
        "shape, dtype, device"
      ],
      "metadata": {
        "id": "_mm_5WVcPb0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"green\"> Q1: Create a tensor from a python list and print all attributes ? </font>"
      ],
      "metadata": {
        "id": "OFzgoU7tPrx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- Write your answer here --------"
      ],
      "metadata": {
        "id": "gqR2iI8yakLa"
      },
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make a tensor use of a specific device, we can use the `to(device)` method."
      ],
      "metadata": {
        "id": "LnIdjdk-RSD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_arr.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38P6eJDbRcvj",
        "outputId": "18ff184d-e29b-4bfb-f7ca-ae1049d27132"
      },
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 369
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ts_arr = ts_arr.to(device)\n",
        "print(f\"Tensor ts_arr is stored on: {ts_arr.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MWrZpioRjI_",
        "outputId": "3797998c-bec2-48cb-ce3d-611f2df4bf15"
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor ts_arr is stored on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Operations with Tensors**\n",
        "\n",
        "Indexing, Slicing, Sampling, math Operations, etc More [here](https://pytorch.org/docs/stable/torch.html)"
      ],
      "metadata": {
        "id": "6CTg-FTnQeJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing"
      ],
      "metadata": {
        "id": "jCByfTNRVUFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing\n",
        "\n",
        "ts_rand = torch.rand(4,4)\n",
        "print(f\"Tensor rand = {ts_rand}\")\n",
        "print()\n",
        "# All rows of column 1\n",
        "print(f\"ts_rand[:,1] = {ts_rand[:,1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1okPsY3RBLX",
        "outputId": "bdb1cba4-cbe3-43a2-cf1d-0fa9b6ab865c"
      },
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor rand = tensor([[0.8112, 0.1291, 0.5881, 0.4353],\n",
            "        [0.1916, 0.6025, 0.6592, 0.4759],\n",
            "        [0.7123, 0.7401, 0.9535, 0.5290],\n",
            "        [0.5178, 0.4928, 0.5690, 0.1758]])\n",
            "\n",
            "ts_rand[:,1] = tensor([0.1291, 0.6025, 0.7401, 0.4928])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate or join"
      ],
      "metadata": {
        "id": "naE9jY2UVW9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate or join\n",
        "# Along the column\n",
        "ts_ccat = torch.cat([ts_rand, torch.ones(4,4)], dim=1)\n",
        "print(ts_ccat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeYLCbwYTG2e",
        "outputId": "3b21f604-171d-4090-af2c-793fb63cf41d"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8112, 0.1291, 0.5881, 0.4353, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [0.1916, 0.6025, 0.6592, 0.4759, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [0.7123, 0.7401, 0.9535, 0.5290, 1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [0.5178, 0.4928, 0.5690, 0.1758, 1.0000, 1.0000, 1.0000, 1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Math operations"
      ],
      "metadata": {
        "id": "6JzdmJ_uVZxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> Q2: Using multiplication operators with tensors ? </font>"
      ],
      "metadata": {
        "id": "Q47R7otgVIQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ts_res = ts_rand * ts_ccat\n",
        "# What going on ?"
      ],
      "metadata": {
        "id": "49d5wKutUpvP"
      },
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_rand.matmul(ts_ccat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQQZfxvzUxo-",
        "outputId": "7ac43e69-8f09-4c90-8908-9623827d300c"
      },
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3271, 0.8322, 1.3706, 0.8022, 1.9637, 1.9637, 1.9637, 1.9637],\n",
              "        [0.9868, 1.1101, 1.4091, 0.8025, 1.9292, 1.9292, 1.9292, 1.9292],\n",
              "        [1.6728, 1.5042, 2.1169, 1.2596, 2.9349, 2.9349, 2.9349, 2.9349],\n",
              "        [1.0108, 0.8715, 1.2720, 0.7918, 1.7555, 1.7555, 1.7555, 1.7555]])"
            ]
          },
          "metadata": {},
          "execution_count": 374
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ts_x1 = torch.tensor([[1,2,3]])\n",
        "# Addition\n",
        "ts_x1 = ts_x1 + 1\n",
        "print(f\"ts_x1 = {ts_x1}\")\n",
        "# Substrcact\n",
        "ts_x1 = ts_x1 - 2\n",
        "print(f\"ts_x1 = {ts_x1}\")\n",
        "# Division\n",
        "ts_x1 = ts_x1 / 2\n",
        "print(f\"ts_x1 = {ts_x1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo8tvDxXHmfw",
        "outputId": "a0ccc948-9c49-4f85-957a-11a565b51fbf"
      },
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ts_x1 = tensor([[2, 3, 4]])\n",
            "ts_x1 = tensor([[0, 1, 2]])\n",
            "ts_x1 = tensor([[0.0000, 0.5000, 1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inplace operations"
      ],
      "metadata": {
        "id": "klw8p-muaGK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_ccat.add_(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rtRdaleZ8UF",
        "outputId": "a543fdf5-8aa7-4ad3-f663-185131158c5b"
      },
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5.8112, 5.1291, 5.5881, 5.4353, 6.0000, 6.0000, 6.0000, 6.0000],\n",
              "        [5.1916, 5.6025, 5.6592, 5.4759, 6.0000, 6.0000, 6.0000, 6.0000],\n",
              "        [5.7123, 5.7401, 5.9535, 5.5290, 6.0000, 6.0000, 6.0000, 6.0000],\n",
              "        [5.5178, 5.4928, 5.5690, 5.1758, 6.0000, 6.0000, 6.0000, 6.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 376
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the max and min in tensors"
      ],
      "metadata": {
        "id": "26gt0sm1IRN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "ts_range = torch.arange(0, 100, 10)\n",
        "ts_range"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVSbCWw9IVLr",
        "outputId": "b1967280-1a8c-4c36-b447-39db3c1b68b9"
      },
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
            ]
          },
          "metadata": {},
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Minimum: {ts_range.min()}\")\n",
        "print(f\"Maximum: {ts_range.max()}\")\n",
        "print(f\"Mean: {ts_range.type(torch.float32).mean()}\") # won't work without float datatype\n",
        "print(f\"Sum: {ts_range.sum()}\")\n",
        "\n",
        "# or\n",
        "torch.max(ts_range), torch.min(ts_range), torch.mean(ts_range.type(torch.float32)), torch.sum(ts_range)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpvpgtMkIfnj",
        "outputId": "b61d8643-5ea9-4875-eefb-149c496d3594"
      },
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum: 0\n",
            "Maximum: 90\n",
            "Mean: 45.0\n",
            "Sum: 450\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(90), tensor(0), tensor(45.), tensor(450))"
            ]
          },
          "metadata": {},
          "execution_count": 378
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional min/max"
      ],
      "metadata": {
        "id": "SGyYf1E2I2K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns index of max and min values\n",
        "print(f\"Index of the max value: {ts_range.argmax()}\")\n",
        "print(f\"Index of the min value: {ts_range.argmin()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W-y8Z95I4Ny",
        "outputId": "6370cc4c-a46b-44c0-fb59-fd6834fe21e4"
      },
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the max value: 9\n",
            "Index of the min value: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors to Numpy ndrrays"
      ],
      "metadata": {
        "id": "DYHZpoHzgSMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_x = torch.tensor([1,1,1])\n",
        "arr_x = ts_x.numpy()\n",
        "ts_x.add_(2)\n",
        "print(\"Check if the value of the array has changed as well\")\n",
        "print(ts_x.numpy() == arr_x)"
      ],
      "metadata": {
        "id": "48gK5pMOaP-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5184272-ac20-4617-bfc4-3be8192afb09"
      },
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if the value of the array has changed as well\n",
            "[ True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casting Tensor Types"
      ],
      "metadata": {
        "id": "EWbDAdApJEiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_range.dtype\n",
        "# Change type to float32\n",
        "ts_range = ts_range.type(torch.float32)\n",
        "print(f\"New tensor type: {ts_range.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ3dJkOwJDw6",
        "outputId": "4173abf3-9a61-47be-e5e7-814c2db2834c"
      },
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New tensor type: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshape Tensors"
      ],
      "metadata": {
        "id": "jnJtHsEIJueb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(1, 10).reshape(1, 3, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvjmdwkVJ4Ui",
        "outputId": "a3ddab5d-b634-46da-e292-d0f09f9f058e"
      },
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3],\n",
              "         [4, 5, 6],\n",
              "         [7, 8, 9]]])"
            ]
          },
          "metadata": {},
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> Q3: Hum whats going on if we try to reshape ts_range? </font>"
      ],
      "metadata": {
        "id": "GD1Z72MqKS5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------- Find out ----------------"
      ],
      "metadata": {
        "id": "9F0p8rJ-KaAa"
      },
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Squeeze and Unsqueeze\n",
        "\n",
        "* Squeeze: Remove one dimension to a tensor\n",
        "* Unsqueeze: Add a new dimension to a tensor\n"
      ],
      "metadata": {
        "id": "v9AAQTzqWFDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_sq = torch.tensor([[[2],[1]],[[3],[1]]])\n",
        "print(ts_sq.shape)\n",
        "ts_sq = ts_sq.squeeze(dim=2)\n",
        "print(ts_sq)\n",
        "print(ts_sq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EinZyzbWEp9",
        "outputId": "ece34b83-5185-48a6-c885-40d727d95e1d"
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 1])\n",
            "tensor([[2, 1],\n",
            "        [3, 1]])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsqueeze\n",
        "ts_sq = ts_sq.unsqueeze(dim=2)\n",
        "print(ts_sq)\n",
        "print(ts_sq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee-wXm2xW8Wd",
        "outputId": "73cb2aeb-5478-420f-cb00-5e6ed1cd38d9"
      },
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2],\n",
            "         [1]],\n",
            "\n",
            "        [[3],\n",
            "         [1]]])\n",
            "torch.Size([2, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='light_blue'> In deep learning and with Pytorch, inputs and outputs as well as weights and biases are represented with tensors </font>"
      ],
      "metadata": {
        "id": "S2BxIvFdb4T6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Designing Neural Network with Pytorch\n",
        "\n",
        "- import nn from torch\n",
        "- inherite nn.Module class\n",
        "- initialize layers under the`__init__` method\n",
        "- Add a ```forward``` method and specify how the data will pass throught the network.\n"
      ],
      "metadata": {
        "id": "BC3P1oulLFtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layers**\n",
        "\n",
        "Layers are defined in the `nn` module of pytorch, named based on their activation function.\n"
      ],
      "metadata": {
        "id": "dUIeEWRmOELp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = nn.Linear(1,1)\n",
        "l2 = nn.ReLU(l1)\n",
        "l3 = nn.Sigmoid()\n",
        "l3 = nn.Conv2d(1, 28, 3)\n",
        "# Every function has its required parameters, always refers to the docs\n",
        "# For convolution find more on: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n"
      ],
      "metadata": {
        "id": "4AWUKPlbOrUz"
      },
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    # The super() function is used to give access to methods and properties of a parent or sibling class\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(in_features=2, out_features=2)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    # Here we pass the inputs through layer1\n",
        "    logits = self.layer1(X)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNet()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qOfm2jlLuW2",
        "outputId": "4b888a45-39a6-44ca-e17c-4bd56a22af17"
      },
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNet(\n",
            "  (layer1): Linear(in_features=2, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5- Learning Algorithm\n",
        "\n",
        "Training a Neural Network happens in two steps:\n",
        "\n",
        "*   **Forward Propagation**: It runs the input data through each layer and each activation of the network.\n",
        "*   **Backward Propagation**: The NN adjusts its parameters proportionate to the error in its guess. Traversing backwards from the output, *collecting the derivatives of the error with respect to the parameters of the functions*, and optimizing the parameters using gradient descent.\n",
        "\n",
        "More details [here](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
        "\n"
      ],
      "metadata": {
        "id": "1p-pBMzNaFwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> Q5: Complete the following code </font>"
      ],
      "metadata": {
        "id": "7detYrRUMoPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need fully working Neural network\n",
        "\n",
        "# --------- Import the necessary package here -----------\n",
        "\n",
        "\n",
        "class NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        # ----- Add two Linear layers here ---------\n",
        "        # First layers: in = 2, out = 2 \n",
        "        # Second layer: in = 2, out = 1\n",
        "    )\n",
        "  \n",
        "  def forward(self, X):\n",
        "    # --------- Add your code here ----------\n",
        "    # Uncomment\n",
        "    # outputs = ...\n",
        "    # return outputs\n",
        "    ...\n",
        "\n",
        "# Uncomment when complete\n",
        "\n",
        "model = NN().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "0BkQ3MX6g4xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c1b02d-58be-4025-e4c5-16ba74ab8593"
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN(\n",
            "  (layer_stack): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (1): Linear(in_features=2, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor([[1,2],[3,4]], dtype=torch.float).to(device)\n",
        "labels = torch.tensor([[0],[1]]).to(device)"
      ],
      "metadata": {
        "id": "NifIs-FQtEgJ"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Forward Propagation**"
      ],
      "metadata": {
        "id": "YIZuzO7IfDG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Pass\n",
        "prediction = model(data)\n",
        "print(f\"The shape of the output tensor: {prediction.shape}\")"
      ],
      "metadata": {
        "id": "SY9pGveWfMoO",
        "outputId": "d8873788-71a2-40e3-ab18-de7917e62820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the output tensor: torch.Size([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Prediction Errors - Loss**\n",
        "\n",
        "In practice, most of the cases, we use predefined Loss functions\n",
        "\n",
        "- MSELoss\n",
        "- CrossEntropyLoss\n",
        "- etc https://pytorch.org/docs/stable/nn.html\n"
      ],
      "metadata": {
        "id": "EDQKn2jFfR66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the loss, ie the error of the model given the prediction and the corresponding correct label\n",
        "loss = (prediction - labels).sum()\n",
        "print(f\"Loss = {loss}\")"
      ],
      "metadata": {
        "id": "2USRwy7DfcpQ",
        "outputId": "b6d2a009-0ef0-458c-f45a-49da8681be54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = 3.1825716495513916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Backward Propagation - Autograd** "
      ],
      "metadata": {
        "id": "Gp9xfYrjfgTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagate the error through the network\n",
        "# By calling backward on the error tensor, the Autograd will be triggered\n",
        "# And the gradients for each model parameter are calculated and stored in the '.grad' attribute.\n",
        "# In practice, we set all gradients to zero before calculating -- optimizer.zero_grad()\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "421g2F6ufkjw"
      },
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Optimizers**\n",
        "Optimization Algorithms \n",
        "\n",
        "- Gradient Descent\n",
        "- Stochastic Gradient Descent (SGD)\n",
        "- Adam\n",
        "\n",
        "Find more on:\n",
        "https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "\n",
        "NB: Learning Rate (lr): step size at each iteration of the oprimization algorithm. (Hyperparameter) in French Pas d'apprentissage\n",
        "\n"
      ],
      "metadata": {
        "id": "CQjTZfJCf1Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer: register model parameters\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "print(\"--------------------PARAMETERS----------------------\")\n",
        "print(f\"Parameters before update: {list(model.parameters())}\")\n",
        "# Then finally initiate the gradient descent algorithm ( here the SGD) and updates all models parameters\n",
        "# Set all gradients to zero before calculation\n",
        "optimizer.zero_grad()\n",
        "optimizer.step()\n",
        "print(\"----------------------------------------------------\")\n",
        "print(f\"Parameters after update: {list(model.parameters())}\")"
      ],
      "metadata": {
        "id": "18QLtIligEeq",
        "outputId": "31da4f2d-28ca-4091-84b8-ad82d59b4c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------PARAMETERS----------------------\n",
            "Parameters before update: [Parameter containing:\n",
            "tensor([[-0.4430,  0.1912],\n",
            "        [ 0.4852,  0.6433]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0799,  0.2721], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.5616,  0.3768]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([0.6757], device='cuda:0', requires_grad=True)]\n",
            "----------------------------------------------------\n",
            "Parameters after update: [Parameter containing:\n",
            "tensor([[-0.4430,  0.1912],\n",
            "        [ 0.4852,  0.6433]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0799,  0.2721], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([[-0.5616,  0.3768]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([0.6757], device='cuda:0', requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Frozen Parameters**\n",
        "\n",
        "In some cases, we don't need to update all parameters of the model, this is called **finetuning** in deep learning. To do so, we need to set the gradients to false for any parameters (Tensors) that are not required updates."
      ],
      "metadata": {
        "id": "K9DC_K7hymDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name)\n",
        "  param.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDNSSIESd67e",
        "outputId": "fffbc2c3-d50f-4f14-e3e8-f25da3ffe5a5"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer_stack.0.weight\n",
            "layer_stack.0.bias\n",
            "layer_stack.1.weight\n",
            "layer_stack.1.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_parameter(\"layer_stack.0.weight\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFu2J7oYfr82",
        "outputId": "03df02e7-43d3-40e0-b455-235940563250"
      },
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.4430,  0.1912],\n",
              "        [ 0.4852,  0.6433]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6- Deep Learning Project Workflow ( With Pytorch )\n",
        "\n",
        "- Get the data\n",
        "- Split the data into train and test\n",
        "- Build a model\n",
        "- Train the model the fit the data\n",
        "- Evaluate the model\n",
        "- Save and reload\n",
        "- Make predictions\n",
        "\n",
        "\n",
        "In this section, we are going to build a simple linear regression model. \n",
        "\n",
        "y = ax + b"
      ],
      "metadata": {
        "id": "TjxPH3WBScQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create synthetic data\n",
        "\n",
        "noise = torch.rand(1, dtype=float)\n",
        "a = 0.7\n",
        "b = 0.3\n",
        "\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.03\n",
        "noise =  torch.rand(34).uniform_(0, 0.2)\n",
        "print(noise.shape)\n",
        "X = torch.arange(start, end, step)\n",
        "X.add_(noise)\n",
        "X.unsqueeze_(dim=1)\n",
        "y = a*X + b \n",
        "\n",
        "print(f\"X.shape = {X.shape}\")\n",
        "print(f\"y.shape= {y.shape}\")"
      ],
      "metadata": {
        "id": "GvQyFnH5vYzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5aeb646-10cb-4066-fcc2-d938482b14b5"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([34])\n",
            "X.shape = torch.Size([34, 1])\n",
            "y.shape= torch.Size([34, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the data** into train and test"
      ],
      "metadata": {
        "id": "5u3bXZ4YYZkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 80% of data used for training set, 20% for testing \n",
        "train_split = int(0.8 * len(X)) \n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKzBXpR3YjZT",
        "outputId": "90416e5a-58f7-4069-d788-521213276a96"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27, 27, 7, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data(pred=None):\n",
        "  if pred is not None:\n",
        "     plt.scatter(X_test.cpu().numpy(), pred.cpu().numpy(), c=\"r\", s=4, label=\"Predictions\")\n",
        "  plt.scatter(X_train.cpu().numpy(), y_train.cpu().numpy(), c=\"b\", s=4, label=\"Training data\")\n",
        "  plt.scatter(X_test.cpu().numpy(), y_test.cpu().numpy(), c=\"g\", s=4, label=\"Test data\")\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "plot_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "q3fSTyKHVRAD",
        "outputId": "71b20a00-ee91-42a6-a003-4ba0fab761bd"
      },
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a Model"
      ],
      "metadata": {
        "id": "8sIQcH9Xbzo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LinearRegressionModel(nn.Module): \n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "    self.layer = nn.Linear(1,1)\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)"
      ],
      "metadata": {
        "id": "RKrgPqeMVUN7"
      },
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegressionModel()\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiNu6S4FVWuH",
        "outputId": "640d8a52-d0fc-4c93-b557-003fb2c73051"
      },
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel(\n",
              "  (layer): Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with model\n",
        "with torch.inference_mode():\n",
        "    X_test = X_test.to(device)\n",
        "    pred = model(X_test)"
      ],
      "metadata": {
        "id": "tt1CoApOdbUc"
      },
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sjxXxEI6d9h7",
        "outputId": "a0f8bc4b-fc9d-4bfe-a544-b50379c58862"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAd20lEQVR4nO3de5QU5bnv8e/jDDeFjdySKIhAQpQZhB6mRZClQFRkSwR1xx0QFhg1DOwxZJkTlSQroCZxaeKJHFxEIMbtJSdRN4knJEq8ICxYUSKDjjiDFwiQMIQt40BQw3XG5/zRxexm6LnRt+mp32etXt1V9VbNU7T2r+utqrfN3RERkfA6LdsFiIhIdikIRERCTkEgIhJyCgIRkZBTEIiIhFx+tgs4Fb179/YBAwZkuwwRkZyyadOmD929T8P5ORkEAwYMoKysLNtliIjkFDP7a6L56hoSEQk5BYGISMgpCEREQi4nzxEkcuzYMaqqqjh8+HC2SxGgc+fO9OvXjw4dOmS7FBFpRrsJgqqqKrp168aAAQMws2yXE2ruTk1NDVVVVQwcODDb5YhIM9pN19Dhw4fp1auXQqANMDN69eqlozORHNFuggBQCLQhei9Ecke7CgIRkfaq9LlS8u/Jp/S50pRvW0GQQnl5eUQiEYYOHcr111/PwYMHT3lbN954IytWrADglltuYcuWLY22Xbt2La+++mr99NKlS3niiSdO+W+LSNuzbNMy6ryOZZuWpXzbCoIU6tKlC+Xl5VRUVNCxY0eWLl16wvLa2tpT2u4jjzxCQUFBo8sbBsGcOXOYOXPmKf0tEWmbSopLyLM8SopLUr5tBUGaXHLJJWzbto21a9dyySWXMHnyZAoKCqirq+P222/nwgsvZNiwYSxbFkt3d+fWW2/lvPPO4/LLL2fv3r312xo3blz9kBp//OMfGTFiBMOHD+eyyy5j586dLF26lAcffJBIJML69eu56667eOCBBwAoLy9n1KhRDBs2jGuvvZb9+/fXb/POO+9k5MiRfPGLX2T9+vUAVFZWMnLkSCKRCMOGDWPr1q2Z/GcTkUYsmbSE2gW1LJm0JOXbbjeXj7YltbW1rFq1iokTJwLwxhtvUFFRwcCBA1m+fDndu3dn48aNHDlyhDFjxjBhwgTefPNN3nvvPbZs2cIHH3xAQUEBN9100wnbra6u5utf/zrr1q1j4MCB7Nu3j549ezJnzhy6du3Kt7/9bQBWr15dv87MmTN56KGHGDt2LAsWLODuu+9m0aJF9XW+/vrrPP/889x99928/PLLLF26lG9+85tMnz6do0ePUldXl6F/NRHJlnAfEZSWQn5+7DkFDh06RCQSIRqN0r9/f26++WYARo4cWX89/YsvvsgTTzxBJBLhoosuoqamhq1bt7Ju3TqmTZtGXl4eZ599Nl/60pdO2v6GDRu49NJL67fVs2fPJus5cOAA//jHPxg7diwAs2bNYt26dfXLr7vuOgCKi4vZuXMnAKNHj+bee+/l/vvv569//StdunRJ7h9FRNq8cAfBsmVQVxd7ToHj5wjKy8t56KGH6NixIwBnnHFGfRt356GHHqpvt2PHDiZMmJCSv99anTp1AmInuY+fv7jhhhtYuXIlXbp04aqrruKVV17JSm0iuSydV/ikQ7iDoKQE8vJizxly5ZVX8vDDD3Ps2DEA3n//ff75z39y6aWX8vTTT1NXV8eePXtYs2bNSeuOGjWKdevWsWPHDgD27dsHQLdu3fj4449Pat+9e3d69OhR3///5JNP1h8dNGb79u0MGjSIefPmMWXKFDZv3pzU/oqEUTqv8EmHcAfBkiVQWxt7zpBbbrmFgoICRowYwdChQykpKaG2tpZrr72WwYMHU1BQwMyZMxk9evRJ6/bp04fly5dz3XXXMXz4cL761a8CcPXVV/Pss8/WnyyO9/jjj3P77bczbNgwysvLWbBgQZP1PfPMMwwdOpRIJEJFRYWuPhJpRqJv/+m8wicdzN2zXUOrRaNRb/jDNO+88w5DhgzJUkWSiN4TaW9Knytl2aZllBSX1F+9k39PPnVeR57lUbvg1C4RzxQz2+Tu0YbzU3JEYGaPmtleM6toZLmZ2WIz22Zmm81sRNyyWWa2NXjMSkU9IiKpEv+NP1GXT659+08kVV1DjwETm1j+r8Dg4DEbeBjAzHoCC4GLgJHAQjPrkaKaRESSFv/hn+hDP53X92dKSoLA3dcB+5poMgV4wmM2AGea2VnAlcBL7r7P3fcDL9F0oIiIZFT8h397+NBPJFM3lPUFdsVNVwXzGpt/EjObTexogv79+6enShGRBpZMWtLuPvgbypmrhtx9ubtH3T3ap0+fbJcjItJuZCoIdgPnxE33C+Y1Nl9ERDIkU0GwEpgZXD00Cjjg7nuAF4AJZtYjOEk8IZiXc2pqaohEIkQiET73uc/Rt2/f+umjR482uW5ZWRnz5s1r9m9cfPHFqSr3BPGD2jVm0aJFSQ2rLSJtV0rOEZjZr4FxQG8zqyJ2JVAHAHdfCjwPXAVsAw4CXwuW7TOzHwAbg03d4+5NnXRus3r16kV5eTkAd9111wmDwEFsgLf8/MT/3NFolGj0pEt7TxI/1HSmLVq0iBkzZnD66adnrQYRSY9UXTU0zd3PcvcO7t7P3X/h7kuDECC4WqjU3T/v7he4e1ncuo+6+xeCx3+mop624sYbb2TOnDlcdNFF3HHHHbz++uuMHj2aoqIiLr74Yt577z0g9nsCX/7yl4FYiNx0002MGzeOQYMGsXjx4vrtde3atb79uHHj+MpXvsL555/P9OnTOX5j4PPPP8/5559PcXEx8+bNq99uvEOHDjF16lSGDBnCtddey6FDh+qXzZ07l2g0SmFhIQsXLgRg8eLF/P3vf2f8+PGMHz++0XYikps0DHWaVVVV8eqrr5KXl8dHH33E+vXryc/P5+WXX+a73/0uv/nNb05a591332XNmjV8/PHHnHfeecydO5cOHTqc0ObNN9+ksrKSs88+mzFjxvCnP/2JaDRKSUlJ/TDV06ZNS1jTww8/zOmnn84777zD5s2bGTGi/v4+fvSjH9GzZ0/q6uq47LLL2Lx5M/PmzeOnP/0pa9asoXfv3o22GzZsWAr/5UQkU3LmqqF0SPEo1Aldf/315OXlAbFhoa+//nqGDh3KbbfdRmVlZcJ1Jk2aRKdOnejduzef+cxn+OCDD05qM3LkSPr168dpp51GJBJh586dvPvuuwwaNKh+mOrGgmDdunXMmDEDgGHDhp3wAf7MM88wYsQIioqKqKysbPQnMlvaTiRZuTaSZy4KdRCkeBTqhOKHoP7+97/P+PHjqaio4Pe//z2HDx9OuM7x4aHhxCGiW9umtXbs2MEDDzzA6tWr2bx5M5MmTUpYY0vbiZyq+C9puTaSZy4KdRBkehTqAwcO0Ldv7H65xx57LOXbP++889i+fXv9j8w8/fTTCdtdeuml/OpXvwKgoqKifqjpjz76iDPOOIPu3bvzwQcfsGrVqvp14oe6bqqdyKloeHQe/yWtPYzl09aFOggyPQr1HXfcwXe+8x2KiopS8g2+oS5duvCzn/2MiRMnUlxcTLdu3ejevftJ7ebOncsnn3zCkCFDWLBgAcXFxQAMHz6coqIizj//fG644QbGjBlTv87s2bOZOHEi48ePb7KdSHMSdck2PDqP/5LWXod1aEs0DHU788knn9C1a1fcndLSUgYPHsxtt92WlVr0nkgi+fmxD/28vNgXMQi6gJYFH/z6vE+btA5DLW3Hz3/+cyKRCIWFhRw4cICSDP76mkhLJOqSzcJvREkcHRFI2ug9EWlbdEQgIimlyzrbDwWBiJwSXdbZfigIROSU6LLO9kNDTIjIKQnDD7aEhY4IUiSZYaghNpBcS0cXHTBgAB9++GGTbe69994WbUtEREGQIseHoS4vL2fOnDncdttt9dMdO3Zsdv3WBEFLKAhEpKUUBGm0adMmxo4dS3FxMVdeeSV79uwBYsM6FxQUMGzYMKZOncrOnTtZunQpDz74IJFIhPXr15+wnZqaGiZMmEBhYSG33HIL8Zf8XnPNNRQXF1NYWMjy5csBmD9/PocOHSISiTB9+vRG20l4lJbCaaeBWXoHWZQc5e459yguLvaGtmzZctK8bFm4cKH/+Mc/9tGjR/vevXvd3f2pp57yr33ta+7uftZZZ/nhw4fd3X3//v316/zkJz9JuL1vfOMbfvfdd7u7+x/+8AcHvLq62t3da2pq3N394MGDXlhY6B9++KG7u59xxhknbKOxdunUlt6TsMvLc4fYIy8v29VItgBlnuAzNdRHBOm8DvrIkSNUVFRwxRVXEIlE+OEPf0hVVRUQG/p5+vTp/PKXv2z0V8vixQ8bPWnSJHr06FG/bPHixQwfPpxRo0axa9cutm7dmnAbLW0nuam5IdVLSmJHA8dfi8QLdRCk8zpod6ewsLD+PMHbb7/Niy++CMBzzz1HaWkpb7zxBhdeeOEpD0C3du1aXn75ZV577TXeeustioqKEg4H3dJ2kruaG1J9yRL49NPYMYGGcZCGQh0E6bwOulOnTlRXV/Paa68BcOzYMSorK/n000/ZtWsX48eP5/777+fAgQN88sknJwzz3FD8sNGrVq1i//79QGxY6x49enD66afz7rvvsmHDhvp1OnTowLFjx5ptJ7kn0bf/TA+pLu1LqIMgncPbnnbaaaxYsYI777yT4cOHE4lEePXVV6mrq2PGjBlccMEFFBUVMW/ePM4880yuvvpqnn322YQnixcuXMi6desoLCzkt7/9Lf379wdg4sSJ1NbWMmTIEObPn8+oUaPq15k9e3Z9F1RT7ST3JPr2r0HbJBkpGXTOzCYC/wfIAx5x9/saLH8QGB9Mng58xt3PDJbVAW8Hy/7m7pOb+3sadC436D1JTmNDM2vIZjlVjQ06l3QQmFke8D5wBVAFbASmuXvCH7E1s28ARe5+UzD9ibt3bc3fVBDkBr0nyUk0br9IMtI5+uhIYJu7b3f3o8BTwJQm2k8Dfp2CvyvSrqnfXzIlFUHQF9gVN10VzDuJmZ0LDAReiZvd2czKzGyDmV3T2B8xs9lBu7Lq6uqEbVLRzSWpofeicc1d6nmc+v0lUzJ9sngqsMLd6+LmnRscqtwALDKzzyda0d2Xu3vU3aN9+vQ5aXnnzp2pqanRB1Ab4O7U1NTQuXPnbJfSJjV3qadIpqVi9NHdwDlx0/2CeYlMBU74HuTuu4Pn7Wa2FigC/tLaIvr160dVVRWNHS1IZnXu3Jl+/fplu4w2qaTkf072irQFqThZnE/sZPFlxAJgI3CDu1c2aHc+8EdgYHCrM2bWAzjo7kfMrDfwGjClsRPNxyU6WSySLaWl8PDDsZu1/uM/1JUjbVfaTha7ey1wK/AC8A7wjLtXmtk9ZhZ/KehU4Ck/MXmGAGVm9hawBrivuRAQaWuWLYuFwPHXIrkmJT9M4+7PA883mLegwfRdCdZ7FbggFTWIZEtJyf8cEai7R3JRqO8sFmlMS6/sAY3jI7lPQSASiP/w15U9EiYKApFA/Ie/buaSMFEQSKgc/9Z/wQVNj+Cpm7kkTFIy6Fym6fJROVXHx+85TuP4SJikc6whkTalqRO9x7/1Dx2qrh+R43REIO2ORu0USUxHBBIaOtEr0jo6IhARCQkdEYiISEIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIJC0ac0IniKSPQoCSRuN4CmSGxQEkja6sUskN+iGMhGRkNANZSIikpCCQEQk5FISBGY20czeM7NtZjY/wfIbzazazMqDxy1xy2aZ2dbgMSsV9UjrXHABmMWeRSR88pPdgJnlAUuAK4AqYKOZrXT3LQ2aPu3utzZYtyewEIgCDmwK1t2fbF3SchUVJz6LSLik4ohgJLDN3be7+1HgKWBKC9e9EnjJ3fcFH/4vARNTUJO0wtChJz6LSLikIgj6ArvipquCeQ39m5ltNrMVZnZOK9fFzGabWZmZlVVXV6eg7HBoyU1db78N7rFnEQmfTJ0s/j0wwN2HEfvW/3hrN+Duy9096u7RPn36pLzA9ko3dYlIc1IRBLuBc+Km+wXz6rl7jbsfCSYfAYpbuq60TGPf/HVTl4g0J+kbyswsH3gfuIzYh/hG4AZ3r4xrc5a77wleXwvc6e6jgpPFm4ARQdM3gGJ339fU39QNZSfT7/SKSHPSdkOZu9cCtwIvAO8Az7h7pZndY2aTg2bzzKzSzN4C5gE3BuvuA35ALDw2Avc0FwKSmL75i8ip0hATIiIhoSEmcoSGbhaRTFMQZFnDD35d5SMimaYgyLKGH/zq6xeRTFMQZFnDD/4lS2JX/SxZkt26RCQ8dLJYRCQkdLI4AzSKp4jkIgVBCmkUTxHJRQqCFNIoniKSi5L+PQL5Hxq9U0RykY4I4uhmLhEJIwVBHN3MJSJhpCCIo5u5RCSMQhUEzXX96GYuEQmjUAWBun5ERE4WqiBQ14+IyMk0xISISEhoiAkREUlIQSAiEnIKAhGRkFMQiIiEnIJARCTkUhIEZjbRzN4zs21mNj/B8m+Z2RYz22xmq83s3LhldWZWHjxWpqIeERFpuaRHHzWzPGAJcAVQBWw0s5XuviWu2ZtA1N0Pmtlc4MfAV4Nlh9w9kmwdIiJyalJxRDAS2Obu2939KPAUMCW+gbuvcfeDweQGoF8K/q6IiKRAKoKgL7ArbroqmNeYm4FVcdOdzazMzDaY2TWNrWRms4N2ZdXV1clVLCIi9TL6wzRmNgOIAmPjZp/r7rvNbBDwipm97e5/abiuuy8HlkPszuKMFCwiEgKpOCLYDZwTN90vmHcCM7sc+B4w2d2PHJ/v7ruD5+3AWqAoBTWJiEgLpSIINgKDzWygmXUEpgInXP1jZkXAMmIhsDdufg8z6xS87g2MAeJPMouISJol3TXk7rVmdivwApAHPOrulWZ2D1Dm7iuBnwBdgf8yM4C/uftkYAiwzMw+JRZK9zW42khERNJMo4+KiISERh8VEZGEFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIpSQIzGyimb1nZtvMbH6C5Z3M7Olg+Z/NbEDcsu8E898zsytTUY+IiLRc0kFgZnnAEuBfgQJgmpkVNGh2M7Df3b8APAjcH6xbAEwFCoGJwM+C7YmISIak4ohgJLDN3be7+1HgKWBKgzZTgMeD1yuAy8zMgvlPufsRd98BbAu2JyIiGZKKIOgL7IqbrgrmJWzj7rXAAaBXC9cFwMxmm1mZmZVVV1enoGwREYEcOlns7svdPeru0T59+mS7HBGRdiMVQbAbOCduul8wL2EbM8sHugM1LVxXRETSKBVBsBEYbGYDzawjsZO/Kxu0WQnMCl5/BXjF3T2YPzW4qmggMBh4PQU1iYhIC+UnuwF3rzWzW4EXgDzgUXevNLN7gDJ3Xwn8AnjSzLYB+4iFBUG7Z4AtQC1Q6u51ydYkIiItZ7Ev5rklGo16WVlZtssQEckpZrbJ3aMN5+fMyWIREUkPBYGISMgpCEREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJOQWBiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJySQWBmfU0s5fMbGvw3CNBm4iZvWZmlWa22cy+GrfsMTPbYWblwSOSTD0iItJ6yR4RzAdWu/tgYHUw3dBBYKa7FwITgUVmdmbc8tvdPRI8ypOsR0REWinZIJgCPB68fhy4pmEDd3/f3bcGr/8O7AX6JPl3RUQkRZINgs+6+57g9X8Dn22qsZmNBDoCf4mb/aOgy+hBM+uUZD0iItJK+c01MLOXgc8lWPS9+Al3dzPzJrZzFvAkMMvdPw1mf4dYgHQElgN3Avc0sv5sYDZA//79mytbRERaqNkgcPfLG1tmZh+Y2Vnuvif4oN/bSLt/AZ4DvufuG+K2ffxo4oiZ/Sfw7SbqWE4sLIhGo40GjoiItE6yXUMrgVnB61nA7xo2MLOOwLPAE+6+osGys4JnI3Z+oSLJekREpJWSDYL7gCvMbCtweTCNmUXN7JGgzb8DlwI3JrhM9P+a2dvA20Bv4IdJ1iMiIq1k7rnXyxKNRr2srCzbZYiI5BQz2+Tu0YbzdWexiEjIKQhEREJOQSAiEnIKAhGRkFMQiIiEnIJARCTkFAQiIiGnIBARCTkFgYhIyCkIRERCTkEgIhJyCgIRkZBTEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQk5BICIScgoCEZGQUxCIiIRcUkFgZj3N7CUz2xo892ikXZ2ZlQePlXHzB5rZn81sm5k9bWYdk6lHRERaL9kjgvnAancfDKwOphM55O6R4DE5bv79wIPu/gVgP3BzkvWIiEgrJRsEU4DHg9ePA9e0dEUzM+BLwIpTWV9Esqy0FPLzY8+S05INgs+6+57g9X8Dn22kXWczKzOzDWZ2/MO+F/APd68NpquAvo39ITObHWyjrLq6OsmyRSRpy5ZBXV3sWXJas0FgZi+bWUWCx5T4du7ugDeymXPdPQrcACwys8+3tlB3X+7uUXeP9unTp7Wri0iqlZRAXl7sWXJas0Hg7pe7+9AEj98BH5jZWQDB895GtrE7eN4OrAWKgBrgTDPLD5r1A3YnvUcikhlLlkBtbew5ndQFlXbJdg2tBGYFr2cBv2vYwMx6mFmn4HVvYAywJTiCWAN8pan1RSTk1AWVdskGwX3AFWa2Fbg8mMbMomb2SNBmCFBmZm8R++C/z923BMvuBL5lZtuInTP4RZL1iEh7oy6otLPYF/PcEo1GvaysLNtliEgYlJbGjkaOB9Hx1+nuEksDM9sUnK89ge4sFhFpSnzXVKJuqnZwDkNBICLSlPiuqUTdVO0gHNQ1JCKSjPiuo+PdRfn5sXDIy4tdWdVGqGtIRCQdEl1Gm44T3Gk8ytARgYhILkjBUYaOCEREclkaL6PVEYGISEjoiEBERBJSEIiIhJyCQEQk5BQEIiIhpyAQEQk5BYGISMgpCEREQi4n7yMws2rgr9muIw16Ax9mu4g0au/7B+1/H9v7/kH73sdz3f2k3/rNySBor8ysLNHNHu1Fe98/aP/72N73D8Kxjw2pa0hEJOQUBCIiIacgaFuWZ7uANGvv+wftfx/b+/5BOPbxBDpHICIScjoiEBEJOQWBiEjIKQgyzMwmmtl7ZrbNzOYnWP4tM9tiZpvNbLWZnZuNOpPR3D7Gtfs3M3Mzy7lL9Vqyj2b278F7WWlmv8p0jclowX+n/c1sjZm9Gfy3elU26jxVZvaome01s4pGlpuZLQ72f7OZjch0jRnl7npk6AHkAX8BBgEdgbeAggZtxgOnB6/nAk9nu+5U72PQrhuwDtgARLNddxrex8HAm0CPYPoz2a47xfu3HJgbvC4Adma77lbu46XACKCikeVXAasAA0YBf852zel86Iggs0YC29x9u7sfBZ4CpsQ3cPc17n4wmNwA9Mtwjclqdh8DPwDuBw5nsrgUack+fh1Y4u77Adx9b4ZrTEZL9s+Bfwledwf+nsH6kubu64B9TTSZAjzhMRuAM83srMxUl3kKgszqC+yKm64K5jXmZmLfSnJJs/sYHGaf4+7PZbKwFGrJ+/hF4Itm9icz22BmEzNWXfJasn93ATPMrAp4HvhGZkrLmNb+v5rT8rNdgCRmZjOAKDA227WkkpmdBvwUuDHLpaRbPrHuoXHEjurWmdkF7v6PrFaVOtOAx9z9f5vZaOBJMxvq7p9muzBpPR0RZNZu4Jy46X7BvBOY2eXA94DJ7n4kQ7WlSnP72A0YCqw1s53E+l9X5tgJ45a8j1XASnc/5u47gPeJBUMuaMn+3Qw8A+DurwGdiQ3W1l606P/V9kJBkFkbgcFmNtDMOgJTgZXxDcysCFhGLARyqV/5uCb30d0PuHtvdx/g7gOInQeZ7O5l2Sn3lDT7PgL/j9jRAGbWm1hX0fZMFpmEluzf34DLAMxsCLEgqM5olem1EpgZXD00Cjjg7nuyXVS6qGsog9y91sxuBV4gdmXGo+5eaWb3AGXuvhL4CdAV+C8zA/ibu0/OWtGt1MJ9zGkt3McXgAlmtgWoA25395rsVd1yLdy//wX83MxuI3bi+EYPLrfJBWb2a2JB3Ts4z7EQ6ADg7kuJnfe4CtgGHAS+lp1KM0NDTIiIhJy6hkREQk5BICIScgoCEZGQUxCIiIScgkBEJOQUBCIiIacgEBEJuf8POxwNsb29oJUAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model\n",
        "\n",
        "- Define a loss function"
      ],
      "metadata": {
        "id": "bdKNv_FRergr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "UYNgavVud1je"
      },
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "train_dataloader = DataLoader(TensorDataset(X_train,y_train), batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(TensorDataset(X_test,y_test), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "nmH_Zh_Zvk7t"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader.dataset"
      ],
      "metadata": {
        "id": "5R6VgdFayLf2",
        "outputId": "389d2f61-404b-406d-d807-3de8b9d9a9c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 405,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7f71733c6eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 405
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "  model.train()\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    # Set the data to load into the device\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute the prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    # Set the gradients to all Tensor to zero\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Train Loss: {loss}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xRVS27qLu1eb"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  # Disable gradient calculation\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "  \n",
        "  test_loss /= num_batches\n",
        "\n",
        "  print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "fBN25SwPwBlF"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n ------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done!\")\n",
        "     "
      ],
      "metadata": {
        "id": "6bFR1Sb7wFpw",
        "outputId": "51a92320-1e76-4c54-e6b0-2b5b40adcaae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 408,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            " ------------------------\n",
            "Train Loss: 0.5036067962646484\n",
            "Train Loss: 0.6866831183433533\n",
            "Train Loss: 1.0994762182235718\n",
            "Train Loss: 1.1851738691329956\n",
            "Train Loss: 1.4795528650283813\n",
            "Train Loss: 1.6426172256469727\n",
            "Test Loss: 1.9140554666519165\n",
            "Epoch 2\n",
            " ------------------------\n",
            "Train Loss: 0.33302196860313416\n",
            "Train Loss: 0.4746071994304657\n",
            "Train Loss: 0.8002805113792419\n",
            "Train Loss: 0.8718787431716919\n",
            "Train Loss: 1.1119377613067627\n",
            "Train Loss: 1.2488518953323364\n",
            "Test Loss: 1.4768688678741455\n",
            "Epoch 3\n",
            " ------------------------\n",
            "Train Loss: 0.21529816091060638\n",
            "Train Loss: 0.32444825768470764\n",
            "Train Loss: 0.5821626782417297\n",
            "Train Loss: 0.6421028971672058\n",
            "Train Loss: 0.8390837907791138\n",
            "Train Loss: 0.9546675682067871\n",
            "Test Loss: 1.1475909948349\n",
            "Epoch 4\n",
            " ------------------------\n",
            "Train Loss: 0.1351386159658432\n",
            "Train Loss: 0.21880225837230682\n",
            "Train Loss: 0.4232160151004791\n",
            "Train Loss: 0.4734865725040436\n",
            "Train Loss: 0.6361093521118164\n",
            "Train Loss: 0.7342143058776855\n",
            "Test Loss: 0.8986218869686127\n",
            "Epoch 5\n",
            " ------------------------\n",
            "Train Loss: 0.0815253034234047\n",
            "Train Loss: 0.14506123960018158\n",
            "Train Loss: 0.30744311213493347\n",
            "Train Loss: 0.34967041015625\n",
            "Train Loss: 0.4847364127635956\n",
            "Train Loss: 0.5684559345245361\n",
            "Test Loss: 0.7095730304718018\n",
            "Epoch 6\n",
            " ------------------------\n",
            "Train Loss: 0.04654289036989212\n",
            "Train Loss: 0.09410542249679565\n",
            "Train Loss: 0.22316323220729828\n",
            "Train Loss: 0.2586829662322998\n",
            "Train Loss: 0.37152665853500366\n",
            "Train Loss: 0.44335874915122986\n",
            "Test Loss: 0.5653593689203262\n",
            "Epoch 7\n",
            " ------------------------\n",
            "Train Loss: 0.024522915482521057\n",
            "Train Loss: 0.05935018137097359\n",
            "Train Loss: 0.161848783493042\n",
            "Train Loss: 0.19176216423511505\n",
            "Train Loss: 0.2865912914276123\n",
            "Train Loss: 0.34856289625167847\n",
            "Test Loss: 0.4547994136810303\n",
            "Epoch 8\n",
            " ------------------------\n",
            "Train Loss: 0.01142146810889244\n",
            "Train Loss: 0.03605141490697861\n",
            "Train Loss: 0.11727529019117355\n",
            "Train Loss: 0.14249317348003387\n",
            "Train Loss: 0.2226455956697464\n",
            "Train Loss: 0.2764091491699219\n",
            "Test Loss: 0.36958831548690796\n",
            "Epoch 9\n",
            " ------------------------\n",
            "Train Loss: 0.004367028828710318\n",
            "Train Loss: 0.0207999125123024\n",
            "Train Loss: 0.08490012586116791\n",
            "Train Loss: 0.1061784029006958\n",
            "Train Loss: 0.1743166297674179\n",
            "Train Loss: 0.22122517228126526\n",
            "Test Loss: 0.30354316532611847\n",
            "Epoch 10\n",
            " ------------------------\n",
            "Train Loss: 0.0013322075828909874\n",
            "Train Loss: 0.011153294704854488\n",
            "Train Loss: 0.061409033834934235\n",
            "Train Loss: 0.07937667518854141\n",
            "Train Loss: 0.13763611018657684\n",
            "Train Loss: 0.17880213260650635\n",
            "Test Loss: 0.25204939395189285\n",
            "Epoch 11\n",
            " ------------------------\n",
            "Train Loss: 0.0008958338876254857\n",
            "Train Loss: 0.005368317477405071\n",
            "Train Loss: 0.044384416192770004\n",
            "Train Loss: 0.05956612154841423\n",
            "Train Loss: 0.10966844856739044\n",
            "Train Loss: 0.14600998163223267\n",
            "Test Loss: 0.2116527259349823\n",
            "Epoch 12\n",
            " ------------------------\n",
            "Train Loss: 0.002070641377940774\n",
            "Train Loss: 0.002206463133916259\n",
            "Train Loss: 0.03206346556544304\n",
            "Train Loss: 0.04489795118570328\n",
            "Train Loss: 0.08823829144239426\n",
            "Train Loss: 0.12051552534103394\n",
            "Test Loss: 0.17975938320159912\n",
            "Epoch 13\n",
            " ------------------------\n",
            "Train Loss: 0.004178733564913273\n",
            "Train Loss: 0.0007926832186058164\n",
            "Train Loss: 0.023161200806498528\n",
            "Train Loss: 0.03401600196957588\n",
            "Train Loss: 0.07173025608062744\n",
            "Train Loss: 0.10057468712329865\n",
            "Test Loss: 0.15441474318504333\n",
            "Epoch 14\n",
            " ------------------------\n",
            "Train Loss: 0.006761730648577213\n",
            "Train Loss: 0.00051304348744452\n",
            "Train Loss: 0.016741415485739708\n",
            "Train Loss: 0.02592497505247593\n",
            "Train Loss: 0.05894216522574425\n",
            "Train Loss: 0.0848798081278801\n",
            "Test Loss: 0.13414039090275764\n",
            "Epoch 15\n",
            " ------------------------\n",
            "Train Loss: 0.009516085498034954\n",
            "Train Loss: 0.0009405766613781452\n",
            "Train Loss: 0.012122302316129208\n",
            "Train Loss: 0.019893888384103775\n",
            "Train Loss: 0.04897692799568176\n",
            "Train Loss: 0.07244710624217987\n",
            "Test Loss: 0.11781321838498116\n",
            "Epoch 16\n",
            " ------------------------\n",
            "Train Loss: 0.012246680445969105\n",
            "Train Loss: 0.0017816824838519096\n",
            "Train Loss: 0.008807646110653877\n",
            "Train Loss: 0.015385478734970093\n",
            "Train Loss: 0.04116334393620491\n",
            "Train Loss: 0.06253379583358765\n",
            "Test Loss: 0.10457632318139076\n",
            "Epoch 17\n",
            " ------------------------\n",
            "Train Loss: 0.014833606779575348\n",
            "Train Loss: 0.002837452804669738\n",
            "Train Loss: 0.006436534225940704\n",
            "Train Loss: 0.012004503048956394\n",
            "Train Loss: 0.03499756380915642\n",
            "Train Loss: 0.05457671731710434\n",
            "Test Loss: 0.0937727615237236\n",
            "Epoch 18\n",
            " ------------------------\n",
            "Train Loss: 0.017208503559231758\n",
            "Train Loss: 0.003975823055952787\n",
            "Train Loss: 0.004746691789478064\n",
            "Train Loss: 0.009459922090172768\n",
            "Train Loss: 0.030100062489509583\n",
            "Train Loss: 0.04814712703227997\n",
            "Test Loss: 0.08489635959267616\n",
            "Epoch 19\n",
            " ------------------------\n",
            "Train Loss: 0.01933775655925274\n",
            "Train Loss: 0.0051115695387125015\n",
            "Train Loss: 0.00354769523255527\n",
            "Train Loss: 0.007537171244621277\n",
            "Train Loss: 0.02618393674492836\n",
            "Train Loss: 0.042917147278785706\n",
            "Test Loss: 0.07755512371659279\n",
            "Epoch 20\n",
            " ------------------------\n",
            "Train Loss: 0.02121068350970745\n",
            "Train Loss: 0.006191982422024012\n",
            "Train Loss: 0.002701457589864731\n",
            "Train Loss: 0.006077855825424194\n",
            "Train Loss: 0.023031312972307205\n",
            "Train Loss: 0.038634613156318665\n",
            "Test Loss: 0.07144363597035408\n",
            "Epoch 21\n",
            " ------------------------\n",
            "Train Loss: 0.022831294685602188\n",
            "Train Loss: 0.007186712231487036\n",
            "Train Loss: 0.0021079524885863066\n",
            "Train Loss: 0.004964868072420359\n",
            "Train Loss: 0.020476084202528\n",
            "Train Loss: 0.03510475158691406\n",
            "Test Loss: 0.06632276810705662\n",
            "Epoch 22\n",
            " ------------------------\n",
            "Train Loss: 0.024212554097175598\n",
            "Train Loss: 0.00808048527687788\n",
            "Train Loss: 0.0016948579577729106\n",
            "Train Loss: 0.0041114771738648415\n",
            "Train Loss: 0.01839093305170536\n",
            "Train Loss: 0.03217625245451927\n",
            "Test Loss: 0.062004175037145615\n",
            "Epoch 23\n",
            " ------------------------\n",
            "Train Loss: 0.025372380390763283\n",
            "Train Loss: 0.00886799581348896\n",
            "Train Loss: 0.0014099780237302184\n",
            "Train Loss: 0.0034533366560935974\n",
            "Train Loss: 0.016677869483828545\n",
            "Train Loss: 0.029730942100286484\n",
            "Test Loss: 0.05833868868649006\n",
            "Epoch 24\n",
            " ------------------------\n",
            "Train Loss: 0.02633115090429783\n",
            "Train Loss: 0.009550386108458042\n",
            "Train Loss: 0.0012157087912783027\n",
            "Train Loss: 0.0029425739776343107\n",
            "Train Loss: 0.015260972082614899\n",
            "Train Loss: 0.027675939723849297\n",
            "Test Loss: 0.055207496508955956\n",
            "Epoch 25\n",
            " ------------------------\n",
            "Train Loss: 0.027109822258353233\n",
            "Train Loss: 0.010132688097655773\n",
            "Train Loss: 0.0010850519174709916\n",
            "Train Loss: 0.0025435250718146563\n",
            "Train Loss: 0.014081254601478577\n",
            "Train Loss: 0.025937963277101517\n",
            "Test Loss: 0.052515408024191856\n",
            "Epoch 26\n",
            " ------------------------\n",
            "Train Loss: 0.027728987857699394\n",
            "Train Loss: 0.010622198693454266\n",
            "Train Loss: 0.000998670351691544\n",
            "Train Loss: 0.0022295136004686356\n",
            "Train Loss: 0.013092443346977234\n",
            "Train Loss: 0.024458665400743484\n",
            "Test Loss: 0.050185779109597206\n",
            "Epoch 27\n",
            " ------------------------\n",
            "Train Loss: 0.028208047151565552\n",
            "Train Loss: 0.011027284897863865\n",
            "Train Loss: 0.0009427730110473931\n",
            "Train Loss: 0.001980553148314357\n",
            "Train Loss: 0.01225818321108818\n",
            "Train Loss: 0.023191537708044052\n",
            "Test Loss: 0.048156630247831345\n",
            "Epoch 28\n",
            " ------------------------\n",
            "Train Loss: 0.02856495976448059\n",
            "Train Loss: 0.011356666684150696\n",
            "Train Loss: 0.000907572393771261\n",
            "Train Loss: 0.0017815874889492989\n",
            "Train Loss: 0.01154966838657856\n",
            "Train Loss: 0.022099163383245468\n",
            "Test Loss: 0.046377478167414665\n",
            "Epoch 29\n",
            " ------------------------\n",
            "Train Loss: 0.02881605736911297\n",
            "Train Loss: 0.011618941091001034\n",
            "Train Loss: 0.0008861582027748227\n",
            "Train Loss: 0.0016212642658501863\n",
            "Train Loss: 0.010943982750177383\n",
            "Train Loss: 0.021151378750801086\n",
            "Test Loss: 0.044807156547904015\n",
            "Epoch 30\n",
            " ------------------------\n",
            "Train Loss: 0.02897590957581997\n",
            "Train Loss: 0.01182219386100769\n",
            "Train Loss: 0.0008736873278394341\n",
            "Train Loss: 0.001490970840677619\n",
            "Train Loss: 0.010422772727906704\n",
            "Train Loss: 0.02032366767525673\n",
            "Test Loss: 0.043411847203969955\n",
            "Epoch 31\n",
            " ------------------------\n",
            "Train Loss: 0.029057487845420837\n",
            "Train Loss: 0.011973964050412178\n",
            "Train Loss: 0.0008667988586239517\n",
            "Train Loss: 0.0013841504696756601\n",
            "Train Loss: 0.009971290826797485\n",
            "Train Loss: 0.019596092402935028\n",
            "Test Loss: 0.042163739912211895\n",
            "Epoch 32\n",
            " ------------------------\n",
            "Train Loss: 0.029072219505906105\n",
            "Train Loss: 0.012081099674105644\n",
            "Train Loss: 0.0008631864911876619\n",
            "Train Loss: 0.001295786234550178\n",
            "Train Loss: 0.009577578864991665\n",
            "Train Loss: 0.018952269107103348\n",
            "Test Loss: 0.04103977046906948\n",
            "Epoch 33\n",
            " ------------------------\n",
            "Train Loss: 0.029030103236436844\n",
            "Train Loss: 0.01214973721653223\n",
            "Train Loss: 0.0008612919482402503\n",
            "Train Loss: 0.001222017570398748\n",
            "Train Loss: 0.009231938049197197\n",
            "Train Loss: 0.018378756940364838\n",
            "Test Loss: 0.04002086725085974\n",
            "Epoch 34\n",
            " ------------------------\n",
            "Train Loss: 0.028939807787537575\n",
            "Train Loss: 0.012185336090624332\n",
            "Train Loss: 0.0008600805886089802\n",
            "Train Loss: 0.0011598690180107951\n",
            "Train Loss: 0.008926446549594402\n",
            "Train Loss: 0.017864475026726723\n",
            "Test Loss: 0.039091154001653194\n",
            "Epoch 35\n",
            " ------------------------\n",
            "Train Loss: 0.028808901086449623\n",
            "Train Loss: 0.012192712165415287\n",
            "Train Loss: 0.0008588958880864084\n",
            "Train Loss: 0.0011070234468206763\n",
            "Train Loss: 0.008654596284031868\n",
            "Train Loss: 0.01740020327270031\n",
            "Test Loss: 0.03823735937476158\n",
            "Epoch 36\n",
            " ------------------------\n",
            "Train Loss: 0.02864389680325985\n",
            "Train Loss: 0.012176061980426311\n",
            "Train Loss: 0.0008573255618102849\n",
            "Train Loss: 0.0010616738582029939\n",
            "Train Loss: 0.008411030285060406\n",
            "Train Loss: 0.01697833091020584\n",
            "Test Loss: 0.0374484034255147\n",
            "Epoch 37\n",
            " ------------------------\n",
            "Train Loss: 0.028450408950448036\n",
            "Train Loss: 0.012139077298343182\n",
            "Train Loss: 0.0008551442879252136\n",
            "Train Loss: 0.001022394746541977\n",
            "Train Loss: 0.008191348984837532\n",
            "Train Loss: 0.01659250818192959\n",
            "Test Loss: 0.036714994348585606\n",
            "Epoch 38\n",
            " ------------------------\n",
            "Train Loss: 0.0282332431524992\n",
            "Train Loss: 0.012084915302693844\n",
            "Train Loss: 0.0008522380376234651\n",
            "Train Loss: 0.000988066429272294\n",
            "Train Loss: 0.007991856895387173\n",
            "Train Loss: 0.016237381845712662\n",
            "Test Loss: 0.03602935280650854\n",
            "Epoch 39\n",
            " ------------------------\n",
            "Train Loss: 0.02799653820693493\n",
            "Train Loss: 0.01201634481549263\n",
            "Train Loss: 0.0008485683356411755\n",
            "Train Loss: 0.0009577968157827854\n",
            "Train Loss: 0.007809521164745092\n",
            "Train Loss: 0.015908516943454742\n",
            "Test Loss: 0.03538485337048769\n",
            "Epoch 40\n",
            " ------------------------\n",
            "Train Loss: 0.027743829414248466\n",
            "Train Loss: 0.011935727670788765\n",
            "Train Loss: 0.0008441564859822392\n",
            "Train Loss: 0.0009308703592978418\n",
            "Train Loss: 0.007641799282282591\n",
            "Train Loss: 0.015602199360728264\n",
            "Test Loss: 0.03477604500949383\n",
            "Epoch 41\n",
            " ------------------------\n",
            "Train Loss: 0.02747810259461403\n",
            "Train Loss: 0.011845086701214314\n",
            "Train Loss: 0.0008390421280637383\n",
            "Train Loss: 0.0009067135979421437\n",
            "Train Loss: 0.007486563175916672\n",
            "Train Loss: 0.015315277501940727\n",
            "Test Loss: 0.03419819753617048\n",
            "Epoch 42\n",
            " ------------------------\n",
            "Train Loss: 0.027201954275369644\n",
            "Train Loss: 0.011746163479983807\n",
            "Train Loss: 0.0008332860888913274\n",
            "Train Loss: 0.0008848680299706757\n",
            "Train Loss: 0.007342025637626648\n",
            "Train Loss: 0.015045108273625374\n",
            "Test Loss: 0.033647339791059494\n",
            "Epoch 43\n",
            " ------------------------\n",
            "Train Loss: 0.02691759541630745\n",
            "Train Loss: 0.011640473268926144\n",
            "Train Loss: 0.000826961942948401\n",
            "Train Loss: 0.0008649496594443917\n",
            "Train Loss: 0.007206681650131941\n",
            "Train Loss: 0.014789428561925888\n",
            "Test Loss: 0.033120171166956425\n",
            "Epoch 44\n",
            " ------------------------\n",
            "Train Loss: 0.026626838371157646\n",
            "Train Loss: 0.011529281735420227\n",
            "Train Loss: 0.0008201384916901588\n",
            "Train Loss: 0.0008466606377623975\n",
            "Train Loss: 0.007079285569489002\n",
            "Train Loss: 0.014546386897563934\n",
            "Test Loss: 0.03261381667107344\n",
            "Epoch 45\n",
            " ------------------------\n",
            "Train Loss: 0.026331311091780663\n",
            "Train Loss: 0.011413662694394588\n",
            "Train Loss: 0.0008128873887471855\n",
            "Train Loss: 0.0008297438616864383\n",
            "Train Loss: 0.006958771497011185\n",
            "Train Loss: 0.014314400032162666\n",
            "Test Loss: 0.03212591074407101\n",
            "Epoch 46\n",
            " ------------------------\n",
            "Train Loss: 0.02603229694068432\n",
            "Train Loss: 0.011294553987681866\n",
            "Train Loss: 0.0008052725461311638\n",
            "Train Loss: 0.0008139945566654205\n",
            "Train Loss: 0.006844244431704283\n",
            "Train Loss: 0.014092091470956802\n",
            "Test Loss: 0.03165438398718834\n",
            "Epoch 47\n",
            " ------------------------\n",
            "Train Loss: 0.025730956345796585\n",
            "Train Loss: 0.011172720231115818\n",
            "Train Loss: 0.0007973546162247658\n",
            "Train Loss: 0.0007992422324605286\n",
            "Train Loss: 0.00673494441434741\n",
            "Train Loss: 0.013878319412469864\n",
            "Test Loss: 0.03119747992604971\n",
            "Epoch 48\n",
            " ------------------------\n",
            "Train Loss: 0.025428248569369316\n",
            "Train Loss: 0.011048857122659683\n",
            "Train Loss: 0.0007891910499893129\n",
            "Train Loss: 0.0007853456772863865\n",
            "Train Loss: 0.006630228366702795\n",
            "Train Loss: 0.013672116212546825\n",
            "Test Loss: 0.03075373638421297\n",
            "Epoch 49\n",
            " ------------------------\n",
            "Train Loss: 0.025124967098236084\n",
            "Train Loss: 0.010923506692051888\n",
            "Train Loss: 0.000780826376285404\n",
            "Train Loss: 0.0007721827714703977\n",
            "Train Loss: 0.006529538426548243\n",
            "Train Loss: 0.01347261667251587\n",
            "Test Loss: 0.0303218150511384\n",
            "Epoch 50\n",
            " ------------------------\n",
            "Train Loss: 0.02482183277606964\n",
            "Train Loss: 0.010797195136547089\n",
            "Train Loss: 0.0007723115268163383\n",
            "Train Loss: 0.0007596557843498886\n",
            "Train Loss: 0.006432423833757639\n",
            "Train Loss: 0.013279156759381294\n",
            "Test Loss: 0.029900703579187393\n",
            "Epoch 51\n",
            " ------------------------\n",
            "Train Loss: 0.02451935224235058\n",
            "Train Loss: 0.010670258663594723\n",
            "Train Loss: 0.0007636760710738599\n",
            "Train Loss: 0.0007476878236047924\n",
            "Train Loss: 0.006338505540043116\n",
            "Train Loss: 0.013091132044792175\n",
            "Test Loss: 0.029489459469914436\n",
            "Epoch 52\n",
            " ------------------------\n",
            "Train Loss: 0.024218030273914337\n",
            "Train Loss: 0.010543080978095531\n",
            "Train Loss: 0.0007549577276222408\n",
            "Train Loss: 0.0007362043834291399\n",
            "Train Loss: 0.006247423123568296\n",
            "Train Loss: 0.01290801726281643\n",
            "Test Loss: 0.029087276197969913\n",
            "Epoch 53\n",
            " ------------------------\n",
            "Train Loss: 0.023918259888887405\n",
            "Train Loss: 0.010415928438305855\n",
            "Train Loss: 0.0007461847853846848\n",
            "Train Loss: 0.0007251488859765232\n",
            "Train Loss: 0.00615891395136714\n",
            "Train Loss: 0.01272936537861824\n",
            "Test Loss: 0.02869347110390663\n",
            "Epoch 54\n",
            " ------------------------\n",
            "Train Loss: 0.02362038940191269\n",
            "Train Loss: 0.010289067402482033\n",
            "Train Loss: 0.0007373805274255574\n",
            "Train Loss: 0.0007144722039811313\n",
            "Train Loss: 0.006072718184441328\n",
            "Train Loss: 0.012554822489619255\n",
            "Test Loss: 0.02830746676772833\n",
            "Epoch 55\n",
            " ------------------------\n",
            "Train Loss: 0.02332468144595623\n",
            "Train Loss: 0.01016267854720354\n",
            "Train Loss: 0.0007285669562406838\n",
            "Train Loss: 0.0007041334174573421\n",
            "Train Loss: 0.005988639779388905\n",
            "Train Loss: 0.012384045869112015\n",
            "Test Loss: 0.027928742580115795\n",
            "Epoch 56\n",
            " ------------------------\n",
            "Train Loss: 0.023031361401081085\n",
            "Train Loss: 0.010036925785243511\n",
            "Train Loss: 0.0007197604863904417\n",
            "Train Loss: 0.0006940981256775558\n",
            "Train Loss: 0.005906486883759499\n",
            "Train Loss: 0.012216763570904732\n",
            "Test Loss: 0.027556855231523514\n",
            "Epoch 57\n",
            " ------------------------\n",
            "Train Loss: 0.022740596905350685\n",
            "Train Loss: 0.009911955334246159\n",
            "Train Loss: 0.0007109783473424613\n",
            "Train Loss: 0.0006843334413133562\n",
            "Train Loss: 0.005826123990118504\n",
            "Train Loss: 0.012052745558321476\n",
            "Test Loss: 0.027191433124244213\n",
            "Epoch 58\n",
            " ------------------------\n",
            "Train Loss: 0.022452563047409058\n",
            "Train Loss: 0.009787891991436481\n",
            "Train Loss: 0.0007022310164757073\n",
            "Train Loss: 0.0006748174200765789\n",
            "Train Loss: 0.005747401621192694\n",
            "Train Loss: 0.011891759932041168\n",
            "Test Loss: 0.026832145638763905\n",
            "Epoch 59\n",
            " ------------------------\n",
            "Train Loss: 0.02216736413538456\n",
            "Train Loss: 0.00966482050716877\n",
            "Train Loss: 0.0006935326382517815\n",
            "Train Loss: 0.0006655256147496402\n",
            "Train Loss: 0.005670219659805298\n",
            "Train Loss: 0.011733658611774445\n",
            "Test Loss: 0.026478690095245838\n",
            "Epoch 60\n",
            " ------------------------\n",
            "Train Loss: 0.021885110065340996\n",
            "Train Loss: 0.009542828425765038\n",
            "Train Loss: 0.0006848896737210453\n",
            "Train Loss: 0.0006564422510564327\n",
            "Train Loss: 0.005594476126134396\n",
            "Train Loss: 0.011578262783586979\n",
            "Test Loss: 0.026130808517336845\n",
            "Epoch 61\n",
            " ------------------------\n",
            "Train Loss: 0.02160586789250374\n",
            "Train Loss: 0.009421974420547485\n",
            "Train Loss: 0.0006763103301636875\n",
            "Train Loss: 0.0006475473055616021\n",
            "Train Loss: 0.005520084407180548\n",
            "Train Loss: 0.011425452306866646\n",
            "Test Loss: 0.025788291357457638\n",
            "Epoch 62\n",
            " ------------------------\n",
            "Train Loss: 0.02132970467209816\n",
            "Train Loss: 0.009302315302193165\n",
            "Train Loss: 0.000667803396936506\n",
            "Train Loss: 0.0006388312322087586\n",
            "Train Loss: 0.005446966737508774\n",
            "Train Loss: 0.01127508096396923\n",
            "Test Loss: 0.02545091975480318\n",
            "Epoch 63\n",
            " ------------------------\n",
            "Train Loss: 0.02105664648115635\n",
            "Train Loss: 0.009183882735669613\n",
            "Train Loss: 0.0006593695725314319\n",
            "Train Loss: 0.0006302801193669438\n",
            "Train Loss: 0.005375064443796873\n",
            "Train Loss: 0.011127076111733913\n",
            "Test Loss: 0.025118554010987282\n",
            "Epoch 64\n",
            " ------------------------\n",
            "Train Loss: 0.020786723122000694\n",
            "Train Loss: 0.00906671304255724\n",
            "Train Loss: 0.0006510165403597057\n",
            "Train Loss: 0.0006218840717338026\n",
            "Train Loss: 0.00530431792140007\n",
            "Train Loss: 0.010981323197484016\n",
            "Test Loss: 0.024790971539914608\n",
            "Epoch 65\n",
            " ------------------------\n",
            "Train Loss: 0.020519964396953583\n",
            "Train Loss: 0.00895083136856556\n",
            "Train Loss: 0.0006427452899515629\n",
            "Train Loss: 0.0006136330193839967\n",
            "Train Loss: 0.005234675016254187\n",
            "Train Loss: 0.010837748646736145\n",
            "Test Loss: 0.0244680754840374\n",
            "Epoch 66\n",
            " ------------------------\n",
            "Train Loss: 0.020256364718079567\n",
            "Train Loss: 0.008836249820888042\n",
            "Train Loss: 0.0006345603032968938\n",
            "Train Loss: 0.0006055226549506187\n",
            "Train Loss: 0.0051661026664078236\n",
            "Train Loss: 0.010696277022361755\n",
            "Test Loss: 0.024149736389517784\n",
            "Epoch 67\n",
            " ------------------------\n",
            "Train Loss: 0.01999593898653984\n",
            "Train Loss: 0.00872299075126648\n",
            "Train Loss: 0.0006264631520025432\n",
            "Train Loss: 0.0005975425592623651\n",
            "Train Loss: 0.0050985501147806644\n",
            "Train Loss: 0.010556857101619244\n",
            "Test Loss: 0.023835839703679085\n",
            "Epoch 68\n",
            " ------------------------\n",
            "Train Loss: 0.01973867602646351\n",
            "Train Loss: 0.008611052297055721\n",
            "Train Loss: 0.0006184540106914937\n",
            "Train Loss: 0.0005896913935430348\n",
            "Train Loss: 0.005031995940953493\n",
            "Train Loss: 0.010419410653412342\n",
            "Test Loss: 0.023526275530457497\n",
            "Epoch 69\n",
            " ------------------------\n",
            "Train Loss: 0.019484562799334526\n",
            "Train Loss: 0.008500438183546066\n",
            "Train Loss: 0.000610535847954452\n",
            "Train Loss: 0.0005819606594741344\n",
            "Train Loss: 0.00496640196070075\n",
            "Train Loss: 0.010283892042934895\n",
            "Test Loss: 0.023220923729240894\n",
            "Epoch 70\n",
            " ------------------------\n",
            "Train Loss: 0.019233591854572296\n",
            "Train Loss: 0.00839115958660841\n",
            "Train Loss: 0.0006027104682289064\n",
            "Train Loss: 0.0005743441870436072\n",
            "Train Loss: 0.004901733249425888\n",
            "Train Loss: 0.010150251910090446\n",
            "Test Loss: 0.022919723764061928\n",
            "Epoch 71\n",
            " ------------------------\n",
            "Train Loss: 0.018985746428370476\n",
            "Train Loss: 0.008283211849629879\n",
            "Train Loss: 0.0005949748447164893\n",
            "Train Loss: 0.0005668418598361313\n",
            "Train Loss: 0.004837978631258011\n",
            "Train Loss: 0.010018464177846909\n",
            "Test Loss: 0.022622589953243732\n",
            "Epoch 72\n",
            " ------------------------\n",
            "Train Loss: 0.018740985542535782\n",
            "Train Loss: 0.008176576346158981\n",
            "Train Loss: 0.0005873311893083155\n",
            "Train Loss: 0.0005594490212388337\n",
            "Train Loss: 0.004775115754455328\n",
            "Train Loss: 0.00988846831023693\n",
            "Test Loss: 0.022329434752464294\n",
            "Epoch 73\n",
            " ------------------------\n",
            "Train Loss: 0.018499299883842468\n",
            "Train Loss: 0.00807125587016344\n",
            "Train Loss: 0.0005797796184197068\n",
            "Train Loss: 0.0005521627026610076\n",
            "Train Loss: 0.004713111091405153\n",
            "Train Loss: 0.009760228917002678\n",
            "Test Loss: 0.02204019483178854\n",
            "Epoch 74\n",
            " ------------------------\n",
            "Train Loss: 0.018260663375258446\n",
            "Train Loss: 0.007967248558998108\n",
            "Train Loss: 0.0005723203648813069\n",
            "Train Loss: 0.000544977025128901\n",
            "Train Loss: 0.00465195020660758\n",
            "Train Loss: 0.009633710607886314\n",
            "Test Loss: 0.021754787303507328\n",
            "Epoch 75\n",
            " ------------------------\n",
            "Train Loss: 0.01802505925297737\n",
            "Train Loss: 0.007864548824727535\n",
            "Train Loss: 0.0005649536033160985\n",
            "Train Loss: 0.0005378932110033929\n",
            "Train Loss: 0.004591619595885277\n",
            "Train Loss: 0.0095088891685009\n",
            "Test Loss: 0.021473166532814503\n",
            "Epoch 76\n",
            " ------------------------\n",
            "Train Loss: 0.017792431637644768\n",
            "Train Loss: 0.0077631208114326\n",
            "Train Loss: 0.0005576764233410358\n",
            "Train Loss: 0.0005309071275405586\n",
            "Train Loss: 0.004532100632786751\n",
            "Train Loss: 0.00938573107123375\n",
            "Test Loss: 0.021195251028984785\n",
            "Epoch 77\n",
            " ------------------------\n",
            "Train Loss: 0.017562761902809143\n",
            "Train Loss: 0.0076629891991615295\n",
            "Train Loss: 0.0005504894070327282\n",
            "Train Loss: 0.0005240161553956568\n",
            "Train Loss: 0.004473377950489521\n",
            "Train Loss: 0.009264197200536728\n",
            "Test Loss: 0.02092097420245409\n",
            "Epoch 78\n",
            " ------------------------\n",
            "Train Loss: 0.017336025834083557\n",
            "Train Loss: 0.007564112544059753\n",
            "Train Loss: 0.0005433925543911755\n",
            "Train Loss: 0.0005172166274860501\n",
            "Train Loss: 0.004415430128574371\n",
            "Train Loss: 0.009144269861280918\n",
            "Test Loss: 0.020650305319577456\n",
            "Epoch 79\n",
            " ------------------------\n",
            "Train Loss: 0.017112186178565025\n",
            "Train Loss: 0.0074664936400949955\n",
            "Train Loss: 0.0005363855161704123\n",
            "Train Loss: 0.0005105116288177669\n",
            "Train Loss: 0.004358259029686451\n",
            "Train Loss: 0.009025927633047104\n",
            "Test Loss: 0.02038318896666169\n",
            "Epoch 80\n",
            " ------------------------\n",
            "Train Loss: 0.01689121499657631\n",
            "Train Loss: 0.007370119448751211\n",
            "Train Loss: 0.0005294664297252893\n",
            "Train Loss: 0.0005038959207013249\n",
            "Train Loss: 0.00430184556171298\n",
            "Train Loss: 0.008909150958061218\n",
            "Test Loss: 0.020119566936045885\n",
            "Epoch 81\n",
            " ------------------------\n",
            "Train Loss: 0.01667306199669838\n",
            "Train Loss: 0.007274971809238195\n",
            "Train Loss: 0.0005226347711868584\n",
            "Train Loss: 0.0004973686300218105\n",
            "Train Loss: 0.004246174823492765\n",
            "Train Loss: 0.008793899789452553\n",
            "Test Loss: 0.019859380088746548\n",
            "Epoch 82\n",
            " ------------------------\n",
            "Train Loss: 0.01645771600306034\n",
            "Train Loss: 0.00718103488907218\n",
            "Train Loss: 0.0005158892017789185\n",
            "Train Loss: 0.0004909289418719709\n",
            "Train Loss: 0.0041912393644452095\n",
            "Train Loss: 0.008680161088705063\n",
            "Test Loss: 0.019602587912231684\n",
            "Epoch 83\n",
            " ------------------------\n",
            "Train Loss: 0.016245124861598015\n",
            "Train Loss: 0.007088297512382269\n",
            "Train Loss: 0.0005092289648018777\n",
            "Train Loss: 0.00048457394586876035\n",
            "Train Loss: 0.004137019626796246\n",
            "Train Loss: 0.008567901328206062\n",
            "Test Loss: 0.019349141977727413\n",
            "Epoch 84\n",
            " ------------------------\n",
            "Train Loss: 0.016035275533795357\n",
            "Train Loss: 0.0069967531599104404\n",
            "Train Loss: 0.0005026545259170234\n",
            "Train Loss: 0.0004783013428095728\n",
            "Train Loss: 0.004083507228642702\n",
            "Train Loss: 0.008457102812826633\n",
            "Test Loss: 0.0190989812836051\n",
            "Epoch 85\n",
            " ------------------------\n",
            "Train Loss: 0.015828130766749382\n",
            "Train Loss: 0.006906390190124512\n",
            "Train Loss: 0.0004961655358783901\n",
            "Train Loss: 0.00047211238415911794\n",
            "Train Loss: 0.004030696582049131\n",
            "Train Loss: 0.008347748778760433\n",
            "Test Loss: 0.018852069042623043\n",
            "Epoch 86\n",
            " ------------------------\n",
            "Train Loss: 0.01562363188713789\n",
            "Train Loss: 0.006817174609750509\n",
            "Train Loss: 0.0004897574544884264\n",
            "Train Loss: 0.0004660047125071287\n",
            "Train Loss: 0.003978569060564041\n",
            "Train Loss: 0.008239815942943096\n",
            "Test Loss: 0.018608352169394493\n",
            "Epoch 87\n",
            " ------------------------\n",
            "Train Loss: 0.015421787276864052\n",
            "Train Loss: 0.006729113403707743\n",
            "Train Loss: 0.00048343228991143405\n",
            "Train Loss: 0.00045997611596249044\n",
            "Train Loss: 0.003927126526832581\n",
            "Train Loss: 0.008133294992148876\n",
            "Test Loss: 0.018367826007306576\n",
            "Epoch 88\n",
            " ------------------------\n",
            "Train Loss: 0.015222533605992794\n",
            "Train Loss: 0.00664218096062541\n",
            "Train Loss: 0.00047718826681375504\n",
            "Train Loss: 0.0004540279624052346\n",
            "Train Loss: 0.0038763543125241995\n",
            "Train Loss: 0.008028150536119938\n",
            "Test Loss: 0.018130403943359852\n",
            "Epoch 89\n",
            " ------------------------\n",
            "Train Loss: 0.01502585131675005\n",
            "Train Loss: 0.006556372623890638\n",
            "Train Loss: 0.0004710244538728148\n",
            "Train Loss: 0.0004481562355067581\n",
            "Train Loss: 0.003826236817985773\n",
            "Train Loss: 0.007924364879727364\n",
            "Test Loss: 0.0178960501216352\n",
            "Epoch 90\n",
            " ------------------------\n",
            "Train Loss: 0.014831713400781155\n",
            "Train Loss: 0.006471672095358372\n",
            "Train Loss: 0.0004649400361813605\n",
            "Train Loss: 0.00044236014946363866\n",
            "Train Loss: 0.0037767679896205664\n",
            "Train Loss: 0.007821929641067982\n",
            "Test Loss: 0.017664736602455378\n",
            "Epoch 91\n",
            " ------------------------\n",
            "Train Loss: 0.014640080742537975\n",
            "Train Loss: 0.00638806214556098\n",
            "Train Loss: 0.000458934431662783\n",
            "Train Loss: 0.00043664081022143364\n",
            "Train Loss: 0.0037279443349689245\n",
            "Train Loss: 0.007720820605754852\n",
            "Test Loss: 0.017436412628740072\n",
            "Epoch 92\n",
            " ------------------------\n",
            "Train Loss: 0.014450917020440102\n",
            "Train Loss: 0.006305530201643705\n",
            "Train Loss: 0.00045300545752979815\n",
            "Train Loss: 0.0004309949872549623\n",
            "Train Loss: 0.003679756075143814\n",
            "Train Loss: 0.007621025666594505\n",
            "Test Loss: 0.017211060505360365\n",
            "Epoch 93\n",
            " ------------------------\n",
            "Train Loss: 0.014264188706874847\n",
            "Train Loss: 0.006224055774509907\n",
            "Train Loss: 0.00044715264812111855\n",
            "Train Loss: 0.0004254234954714775\n",
            "Train Loss: 0.0036321915686130524\n",
            "Train Loss: 0.007522515952587128\n",
            "Test Loss: 0.01698861038312316\n",
            "Epoch 94\n",
            " ------------------------\n",
            "Train Loss: 0.014079871587455273\n",
            "Train Loss: 0.0061436365358531475\n",
            "Train Loss: 0.0004413762071635574\n",
            "Train Loss: 0.00041992441401816905\n",
            "Train Loss: 0.003585244296118617\n",
            "Train Loss: 0.007425298914313316\n",
            "Test Loss: 0.016769055277109146\n",
            "Epoch 95\n",
            " ------------------------\n",
            "Train Loss: 0.013897920027375221\n",
            "Train Loss: 0.006064245942980051\n",
            "Train Loss: 0.00043567214743234217\n",
            "Train Loss: 0.0004144969570916146\n",
            "Train Loss: 0.0035389072727411985\n",
            "Train Loss: 0.007329332642257214\n",
            "Test Loss: 0.016552337910979986\n",
            "Epoch 96\n",
            " ------------------------\n",
            "Train Loss: 0.013718328438699245\n",
            "Train Loss: 0.005985885392874479\n",
            "Train Loss: 0.0004300429136492312\n",
            "Train Loss: 0.00040913894190452993\n",
            "Train Loss: 0.0034931693226099014\n",
            "Train Loss: 0.0072346031665802\n",
            "Test Loss: 0.016338415909558535\n",
            "Epoch 97\n",
            " ------------------------\n",
            "Train Loss: 0.013541054911911488\n",
            "Train Loss: 0.005908529739826918\n",
            "Train Loss: 0.00042448562453500926\n",
            "Train Loss: 0.0004038514744024724\n",
            "Train Loss: 0.00344802625477314\n",
            "Train Loss: 0.0071411156095564365\n",
            "Test Loss: 0.016127276234328747\n",
            "Epoch 98\n",
            " ------------------------\n",
            "Train Loss: 0.01336606778204441\n",
            "Train Loss: 0.005832180380821228\n",
            "Train Loss: 0.00041900062933564186\n",
            "Train Loss: 0.00039863065467216074\n",
            "Train Loss: 0.0034034610725939274\n",
            "Train Loss: 0.007048816420137882\n",
            "Test Loss: 0.015918851364403963\n",
            "Epoch 99\n",
            " ------------------------\n",
            "Train Loss: 0.01319335401058197\n",
            "Train Loss: 0.005756822880357504\n",
            "Train Loss: 0.0004135870549362153\n",
            "Train Loss: 0.0003934784035664052\n",
            "Train Loss: 0.0033594712149351835\n",
            "Train Loss: 0.006957715377211571\n",
            "Test Loss: 0.015713110100477934\n",
            "Epoch 100\n",
            " ------------------------\n",
            "Train Loss: 0.01302286982536316\n",
            "Train Loss: 0.005682434421032667\n",
            "Train Loss: 0.0004082431551069021\n",
            "Train Loss: 0.00038839239277876914\n",
            "Train Loss: 0.003316050861030817\n",
            "Train Loss: 0.00686779385432601\n",
            "Test Loss: 0.015510048251599073\n",
            "Epoch 101\n",
            " ------------------------\n",
            "Train Loss: 0.012854586355388165\n",
            "Train Loss: 0.005609007086604834\n",
            "Train Loss: 0.0004029684641864151\n",
            "Train Loss: 0.000383371690986678\n",
            "Train Loss: 0.003273192560300231\n",
            "Train Loss: 0.006779036484658718\n",
            "Test Loss: 0.015309599228203297\n",
            "Epoch 102\n",
            " ------------------------\n",
            "Train Loss: 0.01268846821039915\n",
            "Train Loss: 0.005536522250622511\n",
            "Train Loss: 0.0003977603919338435\n",
            "Train Loss: 0.0003784177533816546\n",
            "Train Loss: 0.0032308914232999086\n",
            "Train Loss: 0.006691427901387215\n",
            "Test Loss: 0.015111749526113272\n",
            "Epoch 103\n",
            " ------------------------\n",
            "Train Loss: 0.01252449955791235\n",
            "Train Loss: 0.005464982707053423\n",
            "Train Loss: 0.0003926207427866757\n",
            "Train Loss: 0.00037352615618146956\n",
            "Train Loss: 0.0031891309190541506\n",
            "Train Loss: 0.0066049410961568356\n",
            "Test Loss: 0.014916433952748775\n",
            "Epoch 104\n",
            " ------------------------\n",
            "Train Loss: 0.012362661771476269\n",
            "Train Loss: 0.005394361913204193\n",
            "Train Loss: 0.00038754832348786294\n",
            "Train Loss: 0.0003686981799546629\n",
            "Train Loss: 0.003147912910208106\n",
            "Train Loss: 0.0065195802599191666\n",
            "Test Loss: 0.014723669271916151\n",
            "Epoch 105\n",
            " ------------------------\n",
            "Train Loss: 0.012202901765704155\n",
            "Train Loss: 0.005324652884155512\n",
            "Train Loss: 0.00038254025275819004\n",
            "Train Loss: 0.0003639328933786601\n",
            "Train Loss: 0.003107234602794051\n",
            "Train Loss: 0.006435334216803312\n",
            "Test Loss: 0.014533397275954485\n",
            "Epoch 106\n",
            " ------------------------\n",
            "Train Loss: 0.012045199982821941\n",
            "Train Loss: 0.005255843047052622\n",
            "Train Loss: 0.00037759667611680925\n",
            "Train Loss: 0.0003592304128687829\n",
            "Train Loss: 0.003067081794142723\n",
            "Train Loss: 0.006352163385599852\n",
            "Test Loss: 0.014345587696880102\n",
            "Epoch 107\n",
            " ------------------------\n",
            "Train Loss: 0.011889542452991009\n",
            "Train Loss: 0.005187924485653639\n",
            "Train Loss: 0.0003727173316292465\n",
            "Train Loss: 0.00035458753700368106\n",
            "Train Loss: 0.0030274437740445137\n",
            "Train Loss: 0.00627007195726037\n",
            "Test Loss: 0.014160188380628824\n",
            "Epoch 108\n",
            " ------------------------\n",
            "Train Loss: 0.011735904030501842\n",
            "Train Loss: 0.005120886489748955\n",
            "Train Loss: 0.000367901346180588\n",
            "Train Loss: 0.00035000391653738916\n",
            "Train Loss: 0.0029883135575801134\n",
            "Train Loss: 0.006189039908349514\n",
            "Test Loss: 0.01397719606757164\n",
            "Epoch 109\n",
            " ------------------------\n",
            "Train Loss: 0.011584246531128883\n",
            "Train Loss: 0.005054713226854801\n",
            "Train Loss: 0.0003631462750490755\n",
            "Train Loss: 0.00034548138501122594\n",
            "Train Loss: 0.00294969673268497\n",
            "Train Loss: 0.006109056994318962\n",
            "Test Loss: 0.01379656232893467\n",
            "Epoch 110\n",
            " ------------------------\n",
            "Train Loss: 0.011434540152549744\n",
            "Train Loss: 0.004989386536180973\n",
            "Train Loss: 0.00035845351521857083\n",
            "Train Loss: 0.0003410173230804503\n",
            "Train Loss: 0.0029115823563188314\n",
            "Train Loss: 0.0060301171615719795\n",
            "Test Loss: 0.01361828064545989\n",
            "Epoch 111\n",
            " ------------------------\n",
            "Train Loss: 0.011286762543022633\n",
            "Train Loss: 0.004924905952066183\n",
            "Train Loss: 0.00035382117494009435\n",
            "Train Loss: 0.00033661103225313127\n",
            "Train Loss: 0.0028739559929817915\n",
            "Train Loss: 0.005952192470431328\n",
            "Test Loss: 0.013442287221550941\n",
            "Epoch 112\n",
            " ------------------------\n",
            "Train Loss: 0.011140904389321804\n",
            "Train Loss: 0.004861259367316961\n",
            "Train Loss: 0.0003492487012408674\n",
            "Train Loss: 0.00033226070809178054\n",
            "Train Loss: 0.0028368174098432064\n",
            "Train Loss: 0.005875272676348686\n",
            "Test Loss: 0.01326858252286911\n",
            "Epoch 113\n",
            " ------------------------\n",
            "Train Loss: 0.01099692564457655\n",
            "Train Loss: 0.004798435606062412\n",
            "Train Loss: 0.00034473458072170615\n",
            "Train Loss: 0.0003279669035691768\n",
            "Train Loss: 0.0028001584578305483\n",
            "Train Loss: 0.005799348000437021\n",
            "Test Loss: 0.013097106944769621\n",
            "Epoch 114\n",
            " ------------------------\n",
            "Train Loss: 0.010854815132915974\n",
            "Train Loss: 0.004736430011689663\n",
            "Train Loss: 0.0003402804140932858\n",
            "Train Loss: 0.00032372839632444084\n",
            "Train Loss: 0.0027639700565487146\n",
            "Train Loss: 0.005724399816244841\n",
            "Test Loss: 0.012927853036671877\n",
            "Epoch 115\n",
            " ------------------------\n",
            "Train Loss: 0.010714543052017689\n",
            "Train Loss: 0.004675223957747221\n",
            "Train Loss: 0.00033588334918022156\n",
            "Train Loss: 0.0003195442259311676\n",
            "Train Loss: 0.0027282468508929014\n",
            "Train Loss: 0.005650418810546398\n",
            "Test Loss: 0.012760777492076159\n",
            "Epoch 116\n",
            " ------------------------\n",
            "Train Loss: 0.010576087981462479\n",
            "Train Loss: 0.004614808596670628\n",
            "Train Loss: 0.00033154277480207384\n",
            "Train Loss: 0.0003154143050778657\n",
            "Train Loss: 0.002692990703508258\n",
            "Train Loss: 0.005577395670115948\n",
            "Test Loss: 0.01259587425738573\n",
            "Epoch 117\n",
            " ------------------------\n",
            "Train Loss: 0.010439412668347359\n",
            "Train Loss: 0.00455516716465354\n",
            "Train Loss: 0.0003272575559094548\n",
            "Train Loss: 0.0003113388956990093\n",
            "Train Loss: 0.0026581939309835434\n",
            "Train Loss: 0.005505328997969627\n",
            "Test Loss: 0.012433111667633057\n",
            "Epoch 118\n",
            " ------------------------\n",
            "Train Loss: 0.010304494760930538\n",
            "Train Loss: 0.004496298264712095\n",
            "Train Loss: 0.0003230289148632437\n",
            "Train Loss: 0.00030731596052646637\n",
            "Train Loss: 0.0026238402351737022\n",
            "Train Loss: 0.005434179678559303\n",
            "Test Loss: 0.012272424530237913\n",
            "Epoch 119\n",
            " ------------------------\n",
            "Train Loss: 0.010171337984502316\n",
            "Train Loss: 0.004438198637217283\n",
            "Train Loss: 0.0003188549599144608\n",
            "Train Loss: 0.0003033441607840359\n",
            "Train Loss: 0.0025899310130625963\n",
            "Train Loss: 0.005363951902836561\n",
            "Test Loss: 0.012113832402974367\n",
            "Epoch 120\n",
            " ------------------------\n",
            "Train Loss: 0.010039898566901684\n",
            "Train Loss: 0.004380844067782164\n",
            "Train Loss: 0.0003147339157294482\n",
            "Train Loss: 0.00029942349647171795\n",
            "Train Loss: 0.0025564597453922033\n",
            "Train Loss: 0.005294636357575655\n",
            "Test Loss: 0.011957280337810516\n",
            "Epoch 121\n",
            " ------------------------\n",
            "Train Loss: 0.0099101597443223\n",
            "Train Loss: 0.004324234556406736\n",
            "Train Loss: 0.0003106664808001369\n",
            "Train Loss: 0.0002955546078737825\n",
            "Train Loss: 0.0025234250351786613\n",
            "Train Loss: 0.005226215347647667\n",
            "Test Loss: 0.011802767869085073\n",
            "Epoch 122\n",
            " ------------------------\n",
            "Train Loss: 0.009782085195183754\n",
            "Train Loss: 0.004268350545316935\n",
            "Train Loss: 0.0003066523640882224\n",
            "Train Loss: 0.00029173545772209764\n",
            "Train Loss: 0.0024908159393817186\n",
            "Train Loss: 0.005158680956810713\n",
            "Test Loss: 0.011650243308395147\n",
            "Epoch 123\n",
            " ------------------------\n",
            "Train Loss: 0.009655669331550598\n",
            "Train Loss: 0.004213189240545034\n",
            "Train Loss: 0.0003026887134183198\n",
            "Train Loss: 0.00028796502738259733\n",
            "Train Loss: 0.002458628499880433\n",
            "Train Loss: 0.005092015489935875\n",
            "Test Loss: 0.011499695479869843\n",
            "Epoch 124\n",
            " ------------------------\n",
            "Train Loss: 0.009530891664326191\n",
            "Train Loss: 0.004158743657171726\n",
            "Train Loss: 0.0002987768966704607\n",
            "Train Loss: 0.0002842443936970085\n",
            "Train Loss: 0.002426855731755495\n",
            "Train Loss: 0.0050262100994586945\n",
            "Test Loss: 0.011351082473993301\n",
            "Epoch 125\n",
            " ------------------------\n",
            "Train Loss: 0.009407724253833294\n",
            "Train Loss: 0.004105001222342253\n",
            "Train Loss: 0.000294916913844645\n",
            "Train Loss: 0.00028057029703631997\n",
            "Train Loss: 0.002395490650087595\n",
            "Train Loss: 0.004961248487234116\n",
            "Test Loss: 0.011204375885426998\n",
            "Epoch 126\n",
            " ------------------------\n",
            "Train Loss: 0.009286154992878437\n",
            "Train Loss: 0.004051955882459879\n",
            "Train Loss: 0.00029110570903867483\n",
            "Train Loss: 0.0002769438724499196\n",
            "Train Loss: 0.0023645327892154455\n",
            "Train Loss: 0.0048971399664878845\n",
            "Test Loss: 0.011059583630412817\n",
            "Epoch 127\n",
            " ------------------------\n",
            "Train Loss: 0.009166150353848934\n",
            "Train Loss: 0.003999588545411825\n",
            "Train Loss: 0.0002873433695640415\n",
            "Train Loss: 0.00027336503262631595\n",
            "Train Loss: 0.002333974465727806\n",
            "Train Loss: 0.004833849612623453\n",
            "Test Loss: 0.010916666127741337\n",
            "Epoch 128\n",
            " ------------------------\n",
            "Train Loss: 0.009047692641615868\n",
            "Train Loss: 0.00394790293648839\n",
            "Train Loss: 0.0002836303028743714\n",
            "Train Loss: 0.0002698327589314431\n",
            "Train Loss: 0.002303812885656953\n",
            "Train Loss: 0.004771387204527855\n",
            "Test Loss: 0.010775590781122446\n",
            "Epoch 129\n",
            " ------------------------\n",
            "Train Loss: 0.008930769748985767\n",
            "Train Loss: 0.0038968869484961033\n",
            "Train Loss: 0.0002799647336360067\n",
            "Train Loss: 0.0002663466730155051\n",
            "Train Loss: 0.0022740438580513\n",
            "Train Loss: 0.0047097280621528625\n",
            "Test Loss: 0.010636339196935296\n",
            "Epoch 130\n",
            " ------------------------\n",
            "Train Loss: 0.008815362118184566\n",
            "Train Loss: 0.0038465261459350586\n",
            "Train Loss: 0.0002763461961876601\n",
            "Train Loss: 0.00026290383539162576\n",
            "Train Loss: 0.0022446587681770325\n",
            "Train Loss: 0.004648864269256592\n",
            "Test Loss: 0.010498895309865475\n",
            "Epoch 131\n",
            " ------------------------\n",
            "Train Loss: 0.008701439015567303\n",
            "Train Loss: 0.0037968165706843138\n",
            "Train Loss: 0.00027277605840936303\n",
            "Train Loss: 0.00025950674898922443\n",
            "Train Loss: 0.002215649699792266\n",
            "Train Loss: 0.004588783718645573\n",
            "Test Loss: 0.01036321371793747\n",
            "Epoch 132\n",
            " ------------------------\n",
            "Train Loss: 0.008588995784521103\n",
            "Train Loss: 0.0037477524019777775\n",
            "Train Loss: 0.00026925033307634294\n",
            "Train Loss: 0.00025615256163291633\n",
            "Train Loss: 0.002187015488743782\n",
            "Train Loss: 0.004529479891061783\n",
            "Test Loss: 0.010229285806417465\n",
            "Epoch 133\n",
            " ------------------------\n",
            "Train Loss: 0.00847800262272358\n",
            "Train Loss: 0.0036993236280977726\n",
            "Train Loss: 0.00026577175594866276\n",
            "Train Loss: 0.00025284182629548013\n",
            "Train Loss: 0.0021587484516203403\n",
            "Train Loss: 0.0044709439389407635\n",
            "Test Loss: 0.010097092017531395\n",
            "Epoch 134\n",
            " ------------------------\n",
            "Train Loss: 0.008368444629013538\n",
            "Train Loss: 0.0036515151150524616\n",
            "Train Loss: 0.00026233712560497224\n",
            "Train Loss: 0.0002495744847692549\n",
            "Train Loss: 0.0021308527793735266\n",
            "Train Loss: 0.004413169343024492\n",
            "Test Loss: 0.00996660441160202\n",
            "Epoch 135\n",
            " ------------------------\n",
            "Train Loss: 0.00826029758900404\n",
            "Train Loss: 0.0036043294239789248\n",
            "Train Loss: 0.00025894693681038916\n",
            "Train Loss: 0.0002463485288899392\n",
            "Train Loss: 0.002103314269334078\n",
            "Train Loss: 0.0043561323545873165\n",
            "Test Loss: 0.009837804129347205\n",
            "Epoch 136\n",
            " ------------------------\n",
            "Train Loss: 0.00815355684608221\n",
            "Train Loss: 0.0035577528178691864\n",
            "Train Loss: 0.00025560084031894803\n",
            "Train Loss: 0.0002431652828818187\n",
            "Train Loss: 0.002076133619993925\n",
            "Train Loss: 0.004299841821193695\n",
            "Test Loss: 0.00971067207865417\n",
            "Epoch 137\n",
            " ------------------------\n",
            "Train Loss: 0.00804818607866764\n",
            "Train Loss: 0.0035117759834975004\n",
            "Train Loss: 0.00025229790480807424\n",
            "Train Loss: 0.0002400221856078133\n",
            "Train Loss: 0.0020492977928370237\n",
            "Train Loss: 0.004244259092956781\n",
            "Test Loss: 0.00958516076207161\n",
            "Epoch 138\n",
            " ------------------------\n",
            "Train Loss: 0.007944191806018353\n",
            "Train Loss: 0.003466399386525154\n",
            "Train Loss: 0.0002490380429662764\n",
            "Train Loss: 0.00023692012473475188\n",
            "Train Loss: 0.0020228142384439707\n",
            "Train Loss: 0.004189416766166687\n",
            "Test Loss: 0.009461297420784831\n",
            "Epoch 139\n",
            " ------------------------\n",
            "Train Loss: 0.007841520011425018\n",
            "Train Loss: 0.0034215988125652075\n",
            "Train Loss: 0.00024581915931776166\n",
            "Train Loss: 0.00023385910026263446\n",
            "Train Loss: 0.0019966771360486746\n",
            "Train Loss: 0.004135283175855875\n",
            "Test Loss: 0.009339034324511886\n",
            "Epoch 140\n",
            " ------------------------\n",
            "Train Loss: 0.0077401804737746716\n",
            "Train Loss: 0.0033773796167224646\n",
            "Train Loss: 0.00024264199601020664\n",
            "Train Loss: 0.00023083764244802296\n",
            "Train Loss: 0.0019708797335624695\n",
            "Train Loss: 0.004081848077476025\n",
            "Test Loss: 0.009218362858518958\n",
            "Epoch 141\n",
            " ------------------------\n",
            "Train Loss: 0.007640144322067499\n",
            "Train Loss: 0.003333726665005088\n",
            "Train Loss: 0.00023950553440954536\n",
            "Train Loss: 0.00022785486362408847\n",
            "Train Loss: 0.0019454117864370346\n",
            "Train Loss: 0.004029105417430401\n",
            "Test Loss: 0.009099240880459547\n",
            "Epoch 142\n",
            " ------------------------\n",
            "Train Loss: 0.007541407831013203\n",
            "Train Loss: 0.0032906432170420885\n",
            "Train Loss: 0.00023641060397494584\n",
            "Train Loss: 0.00022491048730444163\n",
            "Train Loss: 0.001920271315611899\n",
            "Train Loss: 0.003977037034928799\n",
            "Test Loss: 0.008981640683487058\n",
            "Epoch 143\n",
            " ------------------------\n",
            "Train Loss: 0.007443958427757025\n",
            "Train Loss: 0.0032481250818818808\n",
            "Train Loss: 0.0002333555166842416\n",
            "Train Loss: 0.00022200420789886266\n",
            "Train Loss: 0.0018954564584419131\n",
            "Train Loss: 0.003925635479390621\n",
            "Test Loss: 0.008865572744980454\n",
            "Epoch 144\n",
            " ------------------------\n",
            "Train Loss: 0.007347756065428257\n",
            "Train Loss: 0.003206145018339157\n",
            "Train Loss: 0.00023034043260850012\n",
            "Train Loss: 0.0002191346138715744\n",
            "Train Loss: 0.0018709568539634347\n",
            "Train Loss: 0.0038749021477997303\n",
            "Test Loss: 0.00875099515542388\n",
            "Epoch 145\n",
            " ------------------------\n",
            "Train Loss: 0.007252802141010761\n",
            "Train Loss: 0.0031647179275751114\n",
            "Train Loss: 0.00022736373648513108\n",
            "Train Loss: 0.00021630176343023777\n",
            "Train Loss: 0.0018467766931280494\n",
            "Train Loss: 0.003824823535978794\n",
            "Test Loss: 0.00863790838047862\n",
            "Epoch 146\n",
            " ------------------------\n",
            "Train Loss: 0.007159076631069183\n",
            "Train Loss: 0.003123820060864091\n",
            "Train Loss: 0.00022442525369115174\n",
            "Train Loss: 0.00021350682072807103\n",
            "Train Loss: 0.0018229106208309531\n",
            "Train Loss: 0.003775400575250387\n",
            "Test Loss: 0.008526289137080312\n",
            "Epoch 147\n",
            " ------------------------\n",
            "Train Loss: 0.007066559046506882\n",
            "Train Loss: 0.0030834467615932226\n",
            "Train Loss: 0.00022152478049974889\n",
            "Train Loss: 0.00021074786491226405\n",
            "Train Loss: 0.0017993560759350657\n",
            "Train Loss: 0.003726608119904995\n",
            "Test Loss: 0.008416091091930866\n",
            "Epoch 148\n",
            " ------------------------\n",
            "Train Loss: 0.006975238211452961\n",
            "Train Loss: 0.003043604549020529\n",
            "Train Loss: 0.00021866231691092253\n",
            "Train Loss: 0.00020802362996619195\n",
            "Train Loss: 0.0017760973423719406\n",
            "Train Loss: 0.0036784405820071697\n",
            "Test Loss: 0.00830731657333672\n",
            "Epoch 149\n",
            " ------------------------\n",
            "Train Loss: 0.006885101553052664\n",
            "Train Loss: 0.0030042713042348623\n",
            "Train Loss: 0.00021583694615401328\n",
            "Train Loss: 0.00020533535280264914\n",
            "Train Loss: 0.0017531432677060366\n",
            "Train Loss: 0.003630906343460083\n",
            "Test Loss: 0.008199970936402678\n",
            "Epoch 150\n",
            " ------------------------\n",
            "Train Loss: 0.006796122994273901\n",
            "Train Loss: 0.002965447260066867\n",
            "Train Loss: 0.00021304748952388763\n",
            "Train Loss: 0.00020268182561267167\n",
            "Train Loss: 0.0017304891953244805\n",
            "Train Loss: 0.003583984449505806\n",
            "Test Loss: 0.00809399876743555\n",
            "Epoch 151\n",
            " ------------------------\n",
            "Train Loss: 0.006708297412842512\n",
            "Train Loss: 0.0029271235689520836\n",
            "Train Loss: 0.00021029426716268063\n",
            "Train Loss: 0.0002000632230192423\n",
            "Train Loss: 0.0017081272089853883\n",
            "Train Loss: 0.003537672571837902\n",
            "Test Loss: 0.007989408681169152\n",
            "Epoch 152\n",
            " ------------------------\n",
            "Train Loss: 0.006621600594371557\n",
            "Train Loss: 0.002889295108616352\n",
            "Train Loss: 0.00020757620222866535\n",
            "Train Loss: 0.00019747721671592444\n",
            "Train Loss: 0.0016860526520758867\n",
            "Train Loss: 0.0034919511526823044\n",
            "Test Loss: 0.007886153645813465\n",
            "Epoch 153\n",
            " ------------------------\n",
            "Train Loss: 0.006536032538861036\n",
            "Train Loss: 0.0028519590850919485\n",
            "Train Loss: 0.00020489403686951846\n",
            "Train Loss: 0.000194925582036376\n",
            "Train Loss: 0.0016642630798742175\n",
            "Train Loss: 0.0034468243829905987\n",
            "Test Loss: 0.0077842348255217075\n",
            "Epoch 154\n",
            " ------------------------\n",
            "Train Loss: 0.0064515708945691586\n",
            "Train Loss: 0.0028151050209999084\n",
            "Train Loss: 0.0002022461558226496\n",
            "Train Loss: 0.00019240622350480407\n",
            "Train Loss: 0.0016427554655820131\n",
            "Train Loss: 0.003402283415198326\n",
            "Test Loss: 0.007683648029342294\n",
            "Epoch 155\n",
            " ------------------------\n",
            "Train Loss: 0.006368193309754133\n",
            "Train Loss: 0.002778722671791911\n",
            "Train Loss: 0.00019963228260166943\n",
            "Train Loss: 0.00018991994147654623\n",
            "Train Loss: 0.0016215270152315497\n",
            "Train Loss: 0.0033583189360797405\n",
            "Test Loss: 0.007584353676065803\n",
            "Epoch 156\n",
            " ------------------------\n",
            "Train Loss: 0.0062858969904482365\n",
            "Train Loss: 0.0027428101748228073\n",
            "Train Loss: 0.0001970524899661541\n",
            "Train Loss: 0.00018746592104434967\n",
            "Train Loss: 0.0016005743527784944\n",
            "Train Loss: 0.0033149183727800846\n",
            "Test Loss: 0.007486346177756786\n",
            "Epoch 157\n",
            " ------------------------\n",
            "Train Loss: 0.006204660981893539\n",
            "Train Loss: 0.0027073663659393787\n",
            "Train Loss: 0.00019450593390502036\n",
            "Train Loss: 0.0001850435946835205\n",
            "Train Loss: 0.001579892705194652\n",
            "Train Loss: 0.0032720831222832203\n",
            "Test Loss: 0.007389606907963753\n",
            "Epoch 158\n",
            " ------------------------\n",
            "Train Loss: 0.006124480627477169\n",
            "Train Loss: 0.002672378672286868\n",
            "Train Loss: 0.00019199277448933572\n",
            "Train Loss: 0.00018265236576553434\n",
            "Train Loss: 0.0015594748547300696\n",
            "Train Loss: 0.0032297978177666664\n",
            "Test Loss: 0.007294106297194958\n",
            "Epoch 159\n",
            " ------------------------\n",
            "Train Loss: 0.00604533776640892\n",
            "Train Loss: 0.0026378431357443333\n",
            "Train Loss: 0.00018951074162032455\n",
            "Train Loss: 0.00018029149214271456\n",
            "Train Loss: 0.0015393230132758617\n",
            "Train Loss: 0.0031880633905529976\n",
            "Test Loss: 0.007199847372248769\n",
            "Epoch 160\n",
            " ------------------------\n",
            "Train Loss: 0.005967208184301853\n",
            "Train Loss: 0.002603751141577959\n",
            "Train Loss: 0.00018706219270825386\n",
            "Train Loss: 0.0001779621234163642\n",
            "Train Loss: 0.001519427984021604\n",
            "Train Loss: 0.0031468612141907215\n",
            "Test Loss: 0.007106800097972155\n",
            "Epoch 161\n",
            " ------------------------\n",
            "Train Loss: 0.005890095140784979\n",
            "Train Loss: 0.0025701045524328947\n",
            "Train Loss: 0.0001846446975832805\n",
            "Train Loss: 0.00017566220776643604\n",
            "Train Loss: 0.001499796169809997\n",
            "Train Loss: 0.0031061931513249874\n",
            "Test Loss: 0.00701496796682477\n",
            "Epoch 162\n",
            " ------------------------\n",
            "Train Loss: 0.005813970696181059\n",
            "Train Loss: 0.0025368884671479464\n",
            "Train Loss: 0.00018225792155135423\n",
            "Train Loss: 0.00017339336045552045\n",
            "Train Loss: 0.001480416627600789\n",
            "Train Loss: 0.0030660568736493587\n",
            "Test Loss: 0.006924312561750412\n",
            "Epoch 163\n",
            " ------------------------\n",
            "Train Loss: 0.005738838575780392\n",
            "Train Loss: 0.0025041033513844013\n",
            "Train Loss: 0.00017990265041589737\n",
            "Train Loss: 0.000171152176335454\n",
            "Train Loss: 0.0014612823724746704\n",
            "Train Loss: 0.003026427701115608\n",
            "Test Loss: 0.00683482363820076\n",
            "Epoch 164\n",
            " ------------------------\n",
            "Train Loss: 0.005664675030857325\n",
            "Train Loss: 0.002471745712682605\n",
            "Train Loss: 0.0001775776909198612\n",
            "Train Loss: 0.00016893938300199807\n",
            "Train Loss: 0.0014423953834921122\n",
            "Train Loss: 0.0029873233288526535\n",
            "Test Loss: 0.006746499799191952\n",
            "Epoch 165\n",
            " ------------------------\n",
            "Train Loss: 0.0055914707481861115\n",
            "Train Loss: 0.0024398041423410177\n",
            "Train Loss: 0.00017528330499771982\n",
            "Train Loss: 0.0001667566248215735\n",
            "Train Loss: 0.0014237555442377925\n",
            "Train Loss: 0.002948709763586521\n",
            "Test Loss: 0.00665930169634521\n",
            "Epoch 166\n",
            " ------------------------\n",
            "Train Loss: 0.005519214551895857\n",
            "Train Loss: 0.002408274682238698\n",
            "Train Loss: 0.00017301832849625498\n",
            "Train Loss: 0.00016460100596304983\n",
            "Train Loss: 0.0014053558697924018\n",
            "Train Loss: 0.002910604700446129\n",
            "Test Loss: 0.006573245627805591\n",
            "Epoch 167\n",
            " ------------------------\n",
            "Train Loss: 0.005447890609502792\n",
            "Train Loss: 0.002377151744440198\n",
            "Train Loss: 0.00017078261589631438\n",
            "Train Loss: 0.00016247433086391538\n",
            "Train Loss: 0.001387193100526929\n",
            "Train Loss: 0.002872990444302559\n",
            "Test Loss: 0.00648829503916204\n",
            "Epoch 168\n",
            " ------------------------\n",
            "Train Loss: 0.005377489607781172\n",
            "Train Loss: 0.0023464348632842302\n",
            "Train Loss: 0.00016857612354215235\n",
            "Train Loss: 0.0001603734417585656\n",
            "Train Loss: 0.001369259669445455\n",
            "Train Loss: 0.0028358493000268936\n",
            "Test Loss: 0.006404438754543662\n",
            "Epoch 169\n",
            " ------------------------\n",
            "Train Loss: 0.005307999439537525\n",
            "Train Loss: 0.0023161147255450487\n",
            "Train Loss: 0.00016639787645544857\n",
            "Train Loss: 0.00015830107440706342\n",
            "Train Loss: 0.0013515673344954848\n",
            "Train Loss: 0.0027992024552077055\n",
            "Test Loss: 0.006321670254692435\n",
            "Epoch 170\n",
            " ------------------------\n",
            "Train Loss: 0.005239405669271946\n",
            "Train Loss: 0.002286182716488838\n",
            "Train Loss: 0.0001642471324885264\n",
            "Train Loss: 0.0001562554680276662\n",
            "Train Loss: 0.0013341039884835482\n",
            "Train Loss: 0.0027630352415144444\n",
            "Test Loss: 0.006239989306777716\n",
            "Epoch 171\n",
            " ------------------------\n",
            "Train Loss: 0.0051716952584683895\n",
            "Train Loss: 0.0022566388361155987\n",
            "Train Loss: 0.00016212480841204524\n",
            "Train Loss: 0.00015423620061483234\n",
            "Train Loss: 0.0013168643927201629\n",
            "Train Loss: 0.0027273334562778473\n",
            "Test Loss: 0.006159348180517554\n",
            "Epoch 172\n",
            " ------------------------\n",
            "Train Loss: 0.005104859825223684\n",
            "Train Loss: 0.0022274740040302277\n",
            "Train Loss: 0.00016002934717107564\n",
            "Train Loss: 0.00015224267554003745\n",
            "Train Loss: 0.0012998429592698812\n",
            "Train Loss: 0.0026920803356915712\n",
            "Test Loss: 0.006079739425331354\n",
            "Epoch 173\n",
            " ------------------------\n",
            "Train Loss: 0.005038892384618521\n",
            "Train Loss: 0.0021986912470310926\n",
            "Train Loss: 0.0001579610543558374\n",
            "Train Loss: 0.00015027546032797545\n",
            "Train Loss: 0.0012830455088987947\n",
            "Train Loss: 0.0026572933420538902\n",
            "Test Loss: 0.006001170491799712\n",
            "Epoch 174\n",
            " ------------------------\n",
            "Train Loss: 0.004973777569830418\n",
            "Train Loss: 0.002170279622077942\n",
            "Train Loss: 0.00015592049749102443\n",
            "Train Loss: 0.00014833235763944685\n",
            "Train Loss: 0.0012664610985666513\n",
            "Train Loss: 0.0026229466311633587\n",
            "Test Loss: 0.005923619028180838\n",
            "Epoch 175\n",
            " ------------------------\n",
            "Train Loss: 0.00490950420498848\n",
            "Train Loss: 0.0021422363352030516\n",
            "Train Loss: 0.0001539058139314875\n",
            "Train Loss: 0.0001464153901906684\n",
            "Train Loss: 0.0012500948505476117\n",
            "Train Loss: 0.0025890504475682974\n",
            "Test Loss: 0.005847068270668387\n",
            "Epoch 176\n",
            " ------------------------\n",
            "Train Loss: 0.004846059251576662\n",
            "Train Loss: 0.0021145502105355263\n",
            "Train Loss: 0.0001519162906333804\n",
            "Train Loss: 0.00014452391769737005\n",
            "Train Loss: 0.0012339424574747682\n",
            "Train Loss: 0.0025555980391800404\n",
            "Test Loss: 0.005771512631326914\n",
            "Epoch 177\n",
            " ------------------------\n",
            "Train Loss: 0.004783427342772484\n",
            "Train Loss: 0.0020872200839221478\n",
            "Train Loss: 0.00014995304809417576\n",
            "Train Loss: 0.0001426564995199442\n",
            "Train Loss: 0.0012179943732917309\n",
            "Train Loss: 0.0025225665885955095\n",
            "Test Loss: 0.005696920212358236\n",
            "Epoch 178\n",
            " ------------------------\n",
            "Train Loss: 0.004721614997833967\n",
            "Train Loss: 0.002060249447822571\n",
            "Train Loss: 0.0001480152568547055\n",
            "Train Loss: 0.00014081269910093397\n",
            "Train Loss: 0.0012022542068734765\n",
            "Train Loss: 0.0024899733252823353\n",
            "Test Loss: 0.005623305449262261\n",
            "Epoch 179\n",
            " ------------------------\n",
            "Train Loss: 0.0046605924144387245\n",
            "Train Loss: 0.0020336215384304523\n",
            "Train Loss: 0.00014610204380005598\n",
            "Train Loss: 0.00013899386976845562\n",
            "Train Loss: 0.0011867234716191888\n",
            "Train Loss: 0.002457801951095462\n",
            "Test Loss: 0.005550643661990762\n",
            "Epoch 180\n",
            " ------------------------\n",
            "Train Loss: 0.004600361455231905\n",
            "Train Loss: 0.002007339848205447\n",
            "Train Loss: 0.00014421391824726015\n",
            "Train Loss: 0.00013719739217776805\n",
            "Train Loss: 0.0011713841231539845\n",
            "Train Loss: 0.0024260333739221096\n",
            "Test Loss: 0.005478901555761695\n",
            "Epoch 181\n",
            " ------------------------\n",
            "Train Loss: 0.004540913738310337\n",
            "Train Loss: 0.001981399953365326\n",
            "Train Loss: 0.00014235007984098047\n",
            "Train Loss: 0.00013542476517613977\n",
            "Train Loss: 0.001156249432824552\n",
            "Train Loss: 0.002394687617197633\n",
            "Test Loss: 0.005408116150647402\n",
            "Epoch 182\n",
            " ------------------------\n",
            "Train Loss: 0.004482225049287081\n",
            "Train Loss: 0.001955790910869837\n",
            "Train Loss: 0.0001405102084390819\n",
            "Train Loss: 0.00013367562496569008\n",
            "Train Loss: 0.001141312881372869\n",
            "Train Loss: 0.002363745588809252\n",
            "Test Loss: 0.005338230519555509\n",
            "Epoch 183\n",
            " ------------------------\n",
            "Train Loss: 0.0044242991134524345\n",
            "Train Loss: 0.0019305167952552438\n",
            "Train Loss: 0.00013869428948964924\n",
            "Train Loss: 0.0001319478906225413\n",
            "Train Loss: 0.0011265649227425456\n",
            "Train Loss: 0.0023332037962973118\n",
            "Test Loss: 0.005269254557788372\n",
            "Epoch 184\n",
            " ------------------------\n",
            "Train Loss: 0.004367122892290354\n",
            "Train Loss: 0.0019055644515901804\n",
            "Train Loss: 0.00013690140622202307\n",
            "Train Loss: 0.0001302436285186559\n",
            "Train Loss: 0.001112008118070662\n",
            "Train Loss: 0.0023030517622828484\n",
            "Test Loss: 0.00520116207189858\n",
            "Epoch 185\n",
            " ------------------------\n",
            "Train Loss: 0.00431068753823638\n",
            "Train Loss: 0.0018809400498867035\n",
            "Train Loss: 0.000135132679133676\n",
            "Train Loss: 0.00012856048124376684\n",
            "Train Loss: 0.0010976380435749888\n",
            "Train Loss: 0.0022732922807335854\n",
            "Test Loss: 0.0051339499186724424\n",
            "Epoch 186\n",
            " ------------------------\n",
            "Train Loss: 0.00425498653203249\n",
            "Train Loss: 0.0018566347425803542\n",
            "Train Loss: 0.00013338674034457654\n",
            "Train Loss: 0.00012689859431702644\n",
            "Train Loss: 0.0010834512067958713\n",
            "Train Loss: 0.002243913011625409\n",
            "Test Loss: 0.0050676020327955484\n",
            "Epoch 187\n",
            " ------------------------\n",
            "Train Loss: 0.004199996590614319\n",
            "Train Loss: 0.0018326407298445702\n",
            "Train Loss: 0.00013166280405130237\n",
            "Train Loss: 0.00012525872443802655\n",
            "Train Loss: 0.0010694500524550676\n",
            "Train Loss: 0.0022149155847728252\n",
            "Test Loss: 0.005002109915949404\n",
            "Epoch 188\n",
            " ------------------------\n",
            "Train Loss: 0.004145719110965729\n",
            "Train Loss: 0.0018089547520503402\n",
            "Train Loss: 0.0001299607683904469\n",
            "Train Loss: 0.0001236406242242083\n",
            "Train Loss: 0.001055632601492107\n",
            "Train Loss: 0.0021862960420548916\n",
            "Test Loss: 0.0049374684458598495\n",
            "Epoch 189\n",
            " ------------------------\n",
            "Train Loss: 0.0040921433828771114\n",
            "Train Loss: 0.0017855808837339282\n",
            "Train Loss: 0.00012828173930756748\n",
            "Train Loss: 0.00012204216181999072\n",
            "Train Loss: 0.0010419869795441628\n",
            "Train Loss: 0.0021580401808023453\n",
            "Test Loss: 0.004873666330240667\n",
            "Epoch 190\n",
            " ------------------------\n",
            "Train Loss: 0.004039260558784008\n",
            "Train Loss: 0.0017625027103349566\n",
            "Train Loss: 0.00012662373774219304\n",
            "Train Loss: 0.00012046571646351367\n",
            "Train Loss: 0.0010285265743732452\n",
            "Train Loss: 0.002130152191966772\n",
            "Test Loss: 0.004810686223208904\n",
            "Epoch 191\n",
            " ------------------------\n",
            "Train Loss: 0.003987060394138098\n",
            "Train Loss: 0.001739728613756597\n",
            "Train Loss: 0.0001249873748747632\n",
            "Train Loss: 0.00011890846508322284\n",
            "Train Loss: 0.0010152301983907819\n",
            "Train Loss: 0.0021026208996772766\n",
            "Test Loss: 0.00474850763566792\n",
            "Epoch 192\n",
            " ------------------------\n",
            "Train Loss: 0.0039355396293103695\n",
            "Train Loss: 0.001717250095680356\n",
            "Train Loss: 0.00012337298539932817\n",
            "Train Loss: 0.00011737095337593928\n",
            "Train Loss: 0.0010021061170846224\n",
            "Train Loss: 0.002075439551845193\n",
            "Test Loss: 0.004687134642153978\n",
            "Epoch 193\n",
            " ------------------------\n",
            "Train Loss: 0.003884682897478342\n",
            "Train Loss: 0.0016950573772192001\n",
            "Train Loss: 0.00012177849566796795\n",
            "Train Loss: 0.00011585408356040716\n",
            "Train Loss: 0.0009891544468700886\n",
            "Train Loss: 0.0020486186258494854\n",
            "Test Loss: 0.0046265581622719765\n",
            "Epoch 194\n",
            " ------------------------\n",
            "Train Loss: 0.0038344853091984987\n",
            "Train Loss: 0.001673154765740037\n",
            "Train Loss: 0.00012020504800602794\n",
            "Train Loss: 0.0001143563786172308\n",
            "Train Loss: 0.0009763714042492211\n",
            "Train Loss: 0.0020221411250531673\n",
            "Test Loss: 0.004566769115626812\n",
            "Epoch 195\n",
            " ------------------------\n",
            "Train Loss: 0.0037849314976483583\n",
            "Train Loss: 0.0016515329480171204\n",
            "Train Loss: 0.00011865139094879851\n",
            "Train Loss: 0.00011287823144812137\n",
            "Train Loss: 0.0009637494804337621\n",
            "Train Loss: 0.001996006350964308\n",
            "Test Loss: 0.0045077422400936484\n",
            "Epoch 196\n",
            " ------------------------\n",
            "Train Loss: 0.0037360209971666336\n",
            "Train Loss: 0.0016301911091431975\n",
            "Train Loss: 0.00011711871775332838\n",
            "Train Loss: 0.0001114196129492484\n",
            "Train Loss: 0.0009512953693047166\n",
            "Train Loss: 0.0019702110439538956\n",
            "Test Loss: 0.004449490457773209\n",
            "Epoch 197\n",
            " ------------------------\n",
            "Train Loss: 0.003687737276777625\n",
            "Train Loss: 0.0016091231955215335\n",
            "Train Loss: 0.0001156046855612658\n",
            "Train Loss: 0.0001099797009374015\n",
            "Train Loss: 0.0009390038321726024\n",
            "Train Loss: 0.0019447568338364363\n",
            "Test Loss: 0.004392001894302666\n",
            "Epoch 198\n",
            " ------------------------\n",
            "Train Loss: 0.0036400810349732637\n",
            "Train Loss: 0.0015883267624303699\n",
            "Train Loss: 0.00011411082232370973\n",
            "Train Loss: 0.00010855880100280046\n",
            "Train Loss: 0.0009268703288398683\n",
            "Train Loss: 0.0019196225330233574\n",
            "Test Loss: 0.004335235338658094\n",
            "Epoch 199\n",
            " ------------------------\n",
            "Train Loss: 0.003593042492866516\n",
            "Train Loss: 0.0015678039053454995\n",
            "Train Loss: 0.00011263634223723784\n",
            "Train Loss: 0.0001071555889211595\n",
            "Train Loss: 0.0009148944518528879\n",
            "Train Loss: 0.0018948176875710487\n",
            "Test Loss: 0.004279220476746559\n",
            "Epoch 200\n",
            " ------------------------\n",
            "Train Loss: 0.0035466072149574757\n",
            "Train Loss: 0.001547542866319418\n",
            "Train Loss: 0.000111180663225241\n",
            "Train Loss: 0.00010577114153420553\n",
            "Train Loss: 0.0009030697983689606\n",
            "Train Loss: 0.0018703314708545804\n",
            "Test Loss: 0.004223920870572329\n",
            "Epoch 201\n",
            " ------------------------\n",
            "Train Loss: 0.0035007770638912916\n",
            "Train Loss: 0.0015275421319529414\n",
            "Train Loss: 0.00010974385077133775\n",
            "Train Loss: 0.00010440440382808447\n",
            "Train Loss: 0.0008913995698094368\n",
            "Train Loss: 0.001846160739660263\n",
            "Test Loss: 0.004169329535216093\n",
            "Epoch 202\n",
            " ------------------------\n",
            "Train Loss: 0.003455539932474494\n",
            "Train Loss: 0.0015078046126291156\n",
            "Train Loss: 0.00010832583211595193\n",
            "Train Loss: 0.00010305434989277273\n",
            "Train Loss: 0.0008798781200312078\n",
            "Train Loss: 0.0018223000224679708\n",
            "Test Loss: 0.0041154519421979785\n",
            "Epoch 203\n",
            " ------------------------\n",
            "Train Loss: 0.003410883015021682\n",
            "Train Loss: 0.0014883192488923669\n",
            "Train Loss: 0.00010692587966332212\n",
            "Train Loss: 0.00010172300972044468\n",
            "Train Loss: 0.000868508534040302\n",
            "Train Loss: 0.0017987510655075312\n",
            "Test Loss: 0.00406227249186486\n",
            "Epoch 204\n",
            " ------------------------\n",
            "Train Loss: 0.003366804216057062\n",
            "Train Loss: 0.0014690852258354425\n",
            "Train Loss: 0.00010554401524132118\n",
            "Train Loss: 0.0001004087898763828\n",
            "Train Loss: 0.0008572848746553063\n",
            "Train Loss: 0.001775506418198347\n",
            "Test Loss: 0.004009770229458809\n",
            "Epoch 205\n",
            " ------------------------\n",
            "Train Loss: 0.00332329492084682\n",
            "Train Loss: 0.0014501017285510898\n",
            "Train Loss: 0.00010418012971058488\n",
            "Train Loss: 9.911058441502973e-05\n",
            "Train Loss: 0.0008462029509246349\n",
            "Train Loss: 0.00175255979411304\n",
            "Test Loss: 0.003957953187637031\n",
            "Epoch 206\n",
            " ------------------------\n",
            "Train Loss: 0.0032803534995764494\n",
            "Train Loss: 0.0014313621213659644\n",
            "Train Loss: 0.00010283403389621526\n",
            "Train Loss: 9.782992128748447e-05\n",
            "Train Loss: 0.0008352703298442066\n",
            "Train Loss: 0.0017299116589128971\n",
            "Test Loss: 0.003906799829564989\n",
            "Epoch 207\n",
            " ------------------------\n",
            "Train Loss: 0.003237961558625102\n",
            "Train Loss: 0.0014128664042800665\n",
            "Train Loss: 0.00010150508023798466\n",
            "Train Loss: 9.656586189521477e-05\n",
            "Train Loss: 0.0008244732744060457\n",
            "Train Loss: 0.0017075558425858617\n",
            "Test Loss: 0.0038563169073313475\n",
            "Epoch 208\n",
            " ------------------------\n",
            "Train Loss: 0.003196118865162134\n",
            "Train Loss: 0.0013946059625595808\n",
            "Train Loss: 0.00010019319597631693\n",
            "Train Loss: 9.531808609608561e-05\n",
            "Train Loss: 0.0008138216217048466\n",
            "Train Loss: 0.0016854924615472555\n",
            "Test Loss: 0.003806487307883799\n",
            "Epoch 209\n",
            " ------------------------\n",
            "Train Loss: 0.0031548140104860067\n",
            "Train Loss: 0.0013765853364020586\n",
            "Train Loss: 9.889851935440674e-05\n",
            "Train Loss: 9.408644837094471e-05\n",
            "Train Loss: 0.0008033066987991333\n",
            "Train Loss: 0.0016637135995551944\n",
            "Test Loss: 0.0037572936853393912\n",
            "Epoch 210\n",
            " ------------------------\n",
            "Train Loss: 0.0031140425708144903\n",
            "Train Loss: 0.001358792302198708\n",
            "Train Loss: 9.762031550053507e-05\n",
            "Train Loss: 9.287067223340273e-05\n",
            "Train Loss: 0.0007929246057756245\n",
            "Train Loss: 0.0016422116896137595\n",
            "Test Loss: 0.0037087448872625828\n",
            "Epoch 211\n",
            " ------------------------\n",
            "Train Loss: 0.0030738015193492174\n",
            "Train Loss: 0.0013412327971309423\n",
            "Train Loss: 9.63584243436344e-05\n",
            "Train Loss: 9.167109965346754e-05\n",
            "Train Loss: 0.0007826830260455608\n",
            "Train Loss: 0.001620996044948697\n",
            "Test Loss: 0.003660822636447847\n",
            "Epoch 212\n",
            " ------------------------\n",
            "Train Loss: 0.003034078050404787\n",
            "Train Loss: 0.0013239006511867046\n",
            "Train Loss: 9.511328971711919e-05\n",
            "Train Loss: 9.04863336472772e-05\n",
            "Train Loss: 0.0007725674659013748\n",
            "Train Loss: 0.0016000471077859402\n",
            "Test Loss: 0.00361351715400815\n",
            "Epoch 213\n",
            " ------------------------\n",
            "Train Loss: 0.0029948691371828318\n",
            "Train Loss: 0.0013067927211523056\n",
            "Train Loss: 9.388419130118564e-05\n",
            "Train Loss: 8.931718912208453e-05\n",
            "Train Loss: 0.0007625857251696289\n",
            "Train Loss: 0.0015793744241818786\n",
            "Test Loss: 0.003566827392205596\n",
            "Epoch 214\n",
            " ------------------------\n",
            "Train Loss: 0.0029561666306108236\n",
            "Train Loss: 0.001289905165322125\n",
            "Train Loss: 9.267080895369872e-05\n",
            "Train Loss: 8.816331683192402e-05\n",
            "Train Loss: 0.0007527326815761626\n",
            "Train Loss: 0.0015589678660035133\n",
            "Test Loss: 0.0035207426408305764\n",
            "Epoch 215\n",
            " ------------------------\n",
            "Train Loss: 0.0029179637786000967\n",
            "Train Loss: 0.0012732342584058642\n",
            "Train Loss: 9.147300443146378e-05\n",
            "Train Loss: 8.702412742422894e-05\n",
            "Train Loss: 0.0007430050172843039\n",
            "Train Loss: 0.001538820331916213\n",
            "Test Loss: 0.003475235542282462\n",
            "Epoch 216\n",
            " ------------------------\n",
            "Train Loss: 0.002880258485674858\n",
            "Train Loss: 0.0012567833764478564\n",
            "Train Loss: 9.029138163896278e-05\n",
            "Train Loss: 8.589933713665232e-05\n",
            "Train Loss: 0.0007334011024795473\n",
            "Train Loss: 0.0015189338009804487\n",
            "Test Loss: 0.0034303327556699514\n",
            "Epoch 217\n",
            " ------------------------\n",
            "Train Loss: 0.002843038411810994\n",
            "Train Loss: 0.001240540063008666\n",
            "Train Loss: 8.912412886274979e-05\n",
            "Train Loss: 8.478989911964163e-05\n",
            "Train Loss: 0.0007239297847263515\n",
            "Train Loss: 0.0014993131626397371\n",
            "Test Loss: 0.003386008436791599\n",
            "Epoch 218\n",
            " ------------------------\n",
            "Train Loss: 0.002806298201903701\n",
            "Train Loss: 0.0012245088582858443\n",
            "Train Loss: 8.797251939540729e-05\n",
            "Train Loss: 8.369388524442911e-05\n",
            "Train Loss: 0.0007145737181417644\n",
            "Train Loss: 0.001479934435337782\n",
            "Test Loss: 0.003342253272421658\n",
            "Epoch 219\n",
            " ------------------------\n",
            "Train Loss: 0.0027700322680175304\n",
            "Train Loss: 0.0012086827773600817\n",
            "Train Loss: 8.683529449626803e-05\n",
            "Train Loss: 8.261300536105409e-05\n",
            "Train Loss: 0.0007053408189676702\n",
            "Train Loss: 0.0014608119381591678\n",
            "Test Loss: 0.0032990617910400033\n",
            "Epoch 220\n",
            " ------------------------\n",
            "Train Loss: 0.002734235255047679\n",
            "Train Loss: 0.0011930655455216765\n",
            "Train Loss: 8.571356011088938e-05\n",
            "Train Loss: 8.154472016030923e-05\n",
            "Train Loss: 0.0006962238694541156\n",
            "Train Loss: 0.0014419336803257465\n",
            "Test Loss: 0.003256427007727325\n",
            "Epoch 221\n",
            " ------------------------\n",
            "Train Loss: 0.0026989036705344915\n",
            "Train Loss: 0.0011776486644521356\n",
            "Train Loss: 8.460590470349416e-05\n",
            "Train Loss: 8.049123425735161e-05\n",
            "Train Loss: 0.000687225314322859\n",
            "Train Loss: 0.0014232974499464035\n",
            "Test Loss: 0.003214347059838474\n",
            "Epoch 222\n",
            " ------------------------\n",
            "Train Loss: 0.0026640284340828657\n",
            "Train Loss: 0.0011624310864135623\n",
            "Train Loss: 8.351246651727706e-05\n",
            "Train Loss: 7.945065590320155e-05\n",
            "Train Loss: 0.000678344804327935\n",
            "Train Loss: 0.0014049054589122534\n",
            "Test Loss: 0.003172800876200199\n",
            "Epoch 223\n",
            " ------------------------\n",
            "Train Loss: 0.002629602327942848\n",
            "Train Loss: 0.0011474097846075892\n",
            "Train Loss: 8.243347838288173e-05\n",
            "Train Loss: 7.842346531106159e-05\n",
            "Train Loss: 0.0006695787888020277\n",
            "Train Loss: 0.00138674839399755\n",
            "Test Loss: 0.0031318027758970857\n",
            "Epoch 224\n",
            " ------------------------\n",
            "Train Loss: 0.002595622092485428\n",
            "Train Loss: 0.0011325835948809981\n",
            "Train Loss: 8.136862743413076e-05\n",
            "Train Loss: 7.741019362583756e-05\n",
            "Train Loss: 0.0006609244155697525\n",
            "Train Loss: 0.0013688263716176152\n",
            "Test Loss: 0.003091324004344642\n",
            "Epoch 225\n",
            " ------------------------\n",
            "Train Loss: 0.0025620802771300077\n",
            "Train Loss: 0.0011179473949596286\n",
            "Train Loss: 8.031684410525486e-05\n",
            "Train Loss: 7.640993862878531e-05\n",
            "Train Loss: 0.0006523827323690057\n",
            "Train Loss: 0.0013511371798813343\n",
            "Test Loss: 0.0030513793462887406\n",
            "Epoch 226\n",
            " ------------------------\n",
            "Train Loss: 0.002528972690925002\n",
            "Train Loss: 0.0011035019997507334\n",
            "Train Loss: 7.927935803309083e-05\n",
            "Train Loss: 7.542219100287184e-05\n",
            "Train Loss: 0.000643949955701828\n",
            "Train Loss: 0.0013336739502847195\n",
            "Test Loss: 0.0030119414441287518\n",
            "Epoch 227\n",
            " ------------------------\n",
            "Train Loss: 0.002496295375749469\n",
            "Train Loss: 0.0010892444988712668\n",
            "Train Loss: 7.825496140867472e-05\n",
            "Train Loss: 7.444694347213954e-05\n",
            "Train Loss: 0.000635626376606524\n",
            "Train Loss: 0.0013164367992430925\n",
            "Test Loss: 0.0029730129754170775\n",
            "Epoch 228\n",
            " ------------------------\n",
            "Train Loss: 0.0024640343617647886\n",
            "Train Loss: 0.0010751675581559539\n",
            "Train Loss: 7.724375609541312e-05\n",
            "Train Loss: 7.348509825533256e-05\n",
            "Train Loss: 0.0006274123443290591\n",
            "Train Loss: 0.0012994236312806606\n",
            "Test Loss: 0.00293459533713758\n",
            "Epoch 229\n",
            " ------------------------\n",
            "Train Loss: 0.0024321917444467545\n",
            "Train Loss: 0.0010612721089273691\n",
            "Train Loss: 7.624545105500147e-05\n",
            "Train Loss: 7.253572402987629e-05\n",
            "Train Loss: 0.0006193059380166233\n",
            "Train Loss: 0.0012826323509216309\n",
            "Test Loss: 0.0028966701356694102\n",
            "Epoch 230\n",
            " ------------------------\n",
            "Train Loss: 0.0024007619358599186\n",
            "Train Loss: 0.0010475581511855125\n",
            "Train Loss: 7.526014087488875e-05\n",
            "Train Loss: 7.159826782299206e-05\n",
            "Train Loss: 0.0006113036652095616\n",
            "Train Loss: 0.0012660559732466936\n",
            "Test Loss: 0.0028592379530891776\n",
            "Epoch 231\n",
            " ------------------------\n",
            "Train Loss: 0.0023697358556091785\n",
            "Train Loss: 0.0010340193985030055\n",
            "Train Loss: 7.428706157952547e-05\n",
            "Train Loss: 7.067323895171285e-05\n",
            "Train Loss: 0.0006034054094925523\n",
            "Train Loss: 0.0012497014831751585\n",
            "Test Loss: 0.0028222965775057673\n",
            "Epoch 232\n",
            " ------------------------\n",
            "Train Loss: 0.0023391067516058683\n",
            "Train Loss: 0.0010206556180492043\n",
            "Train Loss: 7.332731183851138e-05\n",
            "Train Loss: 6.975974247325212e-05\n",
            "Train Loss: 0.0005956075037829578\n",
            "Train Loss: 0.001233549672178924\n",
            "Test Loss: 0.0027858222601935267\n",
            "Epoch 233\n",
            " ------------------------\n",
            "Train Loss: 0.00230887858197093\n",
            "Train Loss: 0.0010074651800096035\n",
            "Train Loss: 7.237938552862033e-05\n",
            "Train Loss: 6.885817128932104e-05\n",
            "Train Loss: 0.0005879075615666807\n",
            "Train Loss: 0.0012176047312095761\n",
            "Test Loss: 0.002749818377196789\n",
            "Epoch 234\n",
            " ------------------------\n",
            "Train Loss: 0.002279043197631836\n",
            "Train Loss: 0.0009944465709850192\n",
            "Train Loss: 7.144411210902035e-05\n",
            "Train Loss: 6.79685253999196e-05\n",
            "Train Loss: 0.0005803114618174732\n",
            "Train Loss: 0.0012018689885735512\n",
            "Test Loss: 0.002714278409257531\n",
            "Epoch 235\n",
            " ------------------------\n",
            "Train Loss: 0.0022495908197015524\n",
            "Train Loss: 0.0009815938537940383\n",
            "Train Loss: 7.052107685012743e-05\n",
            "Train Loss: 6.709009176120162e-05\n",
            "Train Loss: 0.0005728122778236866\n",
            "Train Loss: 0.0011863363906741142\n",
            "Test Loss: 0.00267920148326084\n",
            "Epoch 236\n",
            " ------------------------\n",
            "Train Loss: 0.002220519119873643\n",
            "Train Loss: 0.0009689099970273674\n",
            "Train Loss: 6.960967584745958e-05\n",
            "Train Loss: 6.622308865189552e-05\n",
            "Train Loss: 0.0005654087290167809\n",
            "Train Loss: 0.0011710086837410927\n",
            "Test Loss: 0.0026445872499607503\n",
            "Epoch 237\n",
            " ------------------------\n",
            "Train Loss: 0.002191821811720729\n",
            "Train Loss: 0.0009563869098201394\n",
            "Train Loss: 6.871006189612672e-05\n",
            "Train Loss: 6.536731962114573e-05\n",
            "Train Loss: 0.0005581025034189224\n",
            "Train Loss: 0.0011558779515326023\n",
            "Test Loss: 0.002610404568258673\n",
            "Epoch 238\n",
            " ------------------------\n",
            "Train Loss: 0.0021634974982589483\n",
            "Train Loss: 0.0009440275025554001\n",
            "Train Loss: 6.782214040867984e-05\n",
            "Train Loss: 6.452252273447812e-05\n",
            "Train Loss: 0.0005508921458385885\n",
            "Train Loss: 0.0011409403523430228\n",
            "Test Loss: 0.0025766814942471683\n",
            "Epoch 239\n",
            " ------------------------\n",
            "Train Loss: 0.0021355398930609226\n",
            "Train Loss: 0.0009318277589045465\n",
            "Train Loss: 6.694538024021313e-05\n",
            "Train Loss: 6.368904723785818e-05\n",
            "Train Loss: 0.0005437745130620897\n",
            "Train Loss: 0.0011261978652328253\n",
            "Test Loss: 0.0025433817063458264\n",
            "Epoch 240\n",
            " ------------------------\n",
            "Train Loss: 0.0021079403813928366\n",
            "Train Loss: 0.0009197862818837166\n",
            "Train Loss: 6.608018156839535e-05\n",
            "Train Loss: 6.286667485255748e-05\n",
            "Train Loss: 0.0005367492558434606\n",
            "Train Loss: 0.0011116487439721823\n",
            "Test Loss: 0.0025105212698690593\n",
            "Epoch 241\n",
            " ------------------------\n",
            "Train Loss: 0.00208069640211761\n",
            "Train Loss: 0.0009078970178961754\n",
            "Train Loss: 6.522630428662524e-05\n",
            "Train Loss: 6.205392855918035e-05\n",
            "Train Loss: 0.0005298126488924026\n",
            "Train Loss: 0.0010972812306135893\n",
            "Test Loss: 0.0024780764360912144\n",
            "Epoch 242\n",
            " ------------------------\n",
            "Train Loss: 0.002053809119388461\n",
            "Train Loss: 0.0008961671264842153\n",
            "Train Loss: 6.438332638936117e-05\n",
            "Train Loss: 6.125208892626688e-05\n",
            "Train Loss: 0.0005229636444710195\n",
            "Train Loss: 0.0010830992832779884\n",
            "Test Loss: 0.0024460467393510044\n",
            "Epoch 243\n",
            " ------------------------\n",
            "Train Loss: 0.002027269685640931\n",
            "Train Loss: 0.0008845854899846017\n",
            "Train Loss: 6.355130608426407e-05\n",
            "Train Loss: 6.046026828698814e-05\n",
            "Train Loss: 0.0005162068409845233\n",
            "Train Loss: 0.0010691051138564944\n",
            "Test Loss: 0.002414439048152417\n",
            "Epoch 244\n",
            " ------------------------\n",
            "Train Loss: 0.002001070184633136\n",
            "Train Loss: 0.0008731535053811967\n",
            "Train Loss: 6.273008330026641e-05\n",
            "Train Loss: 5.967912875348702e-05\n",
            "Train Loss: 0.0005095368251204491\n",
            "Train Loss: 0.0010552871972322464\n",
            "Test Loss: 0.002383236715104431\n",
            "Epoch 245\n",
            " ------------------------\n",
            "Train Loss: 0.001975215272977948\n",
            "Train Loss: 0.0008618712308816612\n",
            "Train Loss: 6.191958527779207e-05\n",
            "Train Loss: 5.890754619031213e-05\n",
            "Train Loss: 0.0005029498715884984\n",
            "Train Loss: 0.0010416476288810372\n",
            "Test Loss: 0.00235243869246915\n",
            "Epoch 246\n",
            " ------------------------\n",
            "Train Loss: 0.0019496886525303125\n",
            "Train Loss: 0.0008507341262884438\n",
            "Train Loss: 6.111937545938417e-05\n",
            "Train Loss: 5.814685209770687e-05\n",
            "Train Loss: 0.0004964513937011361\n",
            "Train Loss: 0.0010281886206939816\n",
            "Test Loss: 0.002322039508726448\n",
            "Epoch 247\n",
            " ------------------------\n",
            "Train Loss: 0.0019244890427216887\n",
            "Train Loss: 0.0008397381752729416\n",
            "Train Loss: 6.032931196386926e-05\n",
            "Train Loss: 5.7395092881051823e-05\n",
            "Train Loss: 0.0004900355124846101\n",
            "Train Loss: 0.0010149007430300117\n",
            "Test Loss: 0.0022920272313058376\n",
            "Epoch 248\n",
            " ------------------------\n",
            "Train Loss: 0.0018996220314875245\n",
            "Train Loss: 0.0008288890239782631\n",
            "Train Loss: 5.954978405497968e-05\n",
            "Train Loss: 5.665330900228582e-05\n",
            "Train Loss: 0.00048370269360020757\n",
            "Train Loss: 0.0010017860913649201\n",
            "Test Loss: 0.0022624139674007893\n",
            "Epoch 249\n",
            " ------------------------\n",
            "Train Loss: 0.0018750712042674422\n",
            "Train Loss: 0.0008181738085113466\n",
            "Train Loss: 5.8780013205250725e-05\n",
            "Train Loss: 5.592148590949364e-05\n",
            "Train Loss: 0.00047745369374752045\n",
            "Train Loss: 0.0009888429194688797\n",
            "Test Loss: 0.002233180799521506\n",
            "Epoch 250\n",
            " ------------------------\n",
            "Train Loss: 0.0018508393550291657\n",
            "Train Loss: 0.0008076006779447198\n",
            "Train Loss: 5.8020403230329975e-05\n",
            "Train Loss: 5.519932528841309e-05\n",
            "Train Loss: 0.000471285282401368\n",
            "Train Loss: 0.0009760658722370863\n",
            "Test Loss: 0.002204323245678097\n",
            "Epoch 251\n",
            " ------------------------\n",
            "Train Loss: 0.0018269196152687073\n",
            "Train Loss: 0.0007971635204739869\n",
            "Train Loss: 5.7270815887022763e-05\n",
            "Train Loss: 5.4485783039126545e-05\n",
            "Train Loss: 0.0004651940835174173\n",
            "Train Loss: 0.0009634514572098851\n",
            "Test Loss: 0.0021758360089734197\n",
            "Epoch 252\n",
            " ------------------------\n",
            "Train Loss: 0.0018033130327239633\n",
            "Train Loss: 0.0007868640241213143\n",
            "Train Loss: 5.653073822031729e-05\n",
            "Train Loss: 5.3781237511429936e-05\n",
            "Train Loss: 0.00045918067917227745\n",
            "Train Loss: 0.000950996414758265\n",
            "Test Loss: 0.002147710241843015\n",
            "Epoch 253\n",
            " ------------------------\n",
            "Train Loss: 0.0017800119239836931\n",
            "Train Loss: 0.0007766967755742371\n",
            "Train Loss: 5.580034485319629e-05\n",
            "Train Loss: 5.308629624778405e-05\n",
            "Train Loss: 0.0004532465245574713\n",
            "Train Loss: 0.0009387083118781447\n",
            "Test Loss: 0.0021199569455347955\n",
            "Epoch 254\n",
            " ------------------------\n",
            "Train Loss: 0.0017570089548826218\n",
            "Train Loss: 0.0007666607270948589\n",
            "Train Loss: 5.5079650337575004e-05\n",
            "Train Loss: 5.239995880401693e-05\n",
            "Train Loss: 0.0004473882436286658\n",
            "Train Loss: 0.0009265744592994452\n",
            "Test Loss: 0.0020925566786900163\n",
            "Epoch 255\n",
            " ------------------------\n",
            "Train Loss: 0.001734306337311864\n",
            "Train Loss: 0.0007567543652839959\n",
            "Train Loss: 5.436784340417944e-05\n",
            "Train Loss: 5.172237433725968e-05\n",
            "Train Loss: 0.00044160374091006815\n",
            "Train Loss: 0.0009145989315584302\n",
            "Test Loss: 0.002065513690467924\n",
            "Epoch 256\n",
            " ------------------------\n",
            "Train Loss: 0.0017118951072916389\n",
            "Train Loss: 0.0007469755946658552\n",
            "Train Loss: 5.3665306040784344e-05\n",
            "Train Loss: 5.105409582029097e-05\n",
            "Train Loss: 0.00043589883716776967\n",
            "Train Loss: 0.0009027835913002491\n",
            "Test Loss: 0.00203882745699957\n",
            "Epoch 257\n",
            " ------------------------\n",
            "Train Loss: 0.001689773052930832\n",
            "Train Loss: 0.0007373220869340003\n",
            "Train Loss: 5.297164534567855e-05\n",
            "Train Loss: 5.039447205490433e-05\n",
            "Train Loss: 0.00043026558705605567\n",
            "Train Loss: 0.0008911128388717771\n",
            "Test Loss: 0.002012473880313337\n",
            "Epoch 258\n",
            " ------------------------\n",
            "Train Loss: 0.0016679380787536502\n",
            "Train Loss: 0.0007277951226569712\n",
            "Train Loss: 5.2287210564827546e-05\n",
            "Train Loss: 4.974303374183364e-05\n",
            "Train Loss: 0.00042470518383197486\n",
            "Train Loss: 0.0008795958128757775\n",
            "Test Loss: 0.0019864681526087224\n",
            "Epoch 259\n",
            " ------------------------\n",
            "Train Loss: 0.001646379823796451\n",
            "Train Loss: 0.0007183882989920676\n",
            "Train Loss: 5.1611423259601e-05\n",
            "Train Loss: 4.910019561066292e-05\n",
            "Train Loss: 0.00041921655065380037\n",
            "Train Loss: 0.0008682309417054057\n",
            "Test Loss: 0.0019607965950854123\n",
            "Epoch 260\n",
            " ------------------------\n",
            "Train Loss: 0.0016251016641035676\n",
            "Train Loss: 0.000709103187546134\n",
            "Train Loss: 5.0944108807016164e-05\n",
            "Train Loss: 4.846583033213392e-05\n",
            "Train Loss: 0.0004137983196415007\n",
            "Train Loss: 0.0008570098434574902\n",
            "Test Loss: 0.0019354577525518835\n",
            "Epoch 261\n",
            " ------------------------\n",
            "Train Loss: 0.001604100107215345\n",
            "Train Loss: 0.0006999389152042568\n",
            "Train Loss: 5.028561645303853e-05\n",
            "Train Loss: 4.78397159895394e-05\n",
            "Train Loss: 0.00040845313924364746\n",
            "Train Loss: 0.0008459381060674787\n",
            "Test Loss: 0.001910450286231935\n",
            "Epoch 262\n",
            " ------------------------\n",
            "Train Loss: 0.0015833675861358643\n",
            "Train Loss: 0.0006908926297910511\n",
            "Train Loss: 4.963576793670654e-05\n",
            "Train Loss: 4.7221637942129746e-05\n",
            "Train Loss: 0.000403175363317132\n",
            "Train Loss: 0.0008350071730092168\n",
            "Test Loss: 0.0018857594695873559\n",
            "Epoch 263\n",
            " ------------------------\n",
            "Train Loss: 0.0015629035187885165\n",
            "Train Loss: 0.0006819621194154024\n",
            "Train Loss: 4.899417399428785e-05\n",
            "Train Loss: 4.6611461584689096e-05\n",
            "Train Loss: 0.0003979635948780924\n",
            "Train Loss: 0.000824214075691998\n",
            "Test Loss: 0.0018613908323459327\n",
            "Epoch 264\n",
            " ------------------------\n",
            "Train Loss: 0.001542706391774118\n",
            "Train Loss: 0.000673150469083339\n",
            "Train Loss: 4.836139487451874e-05\n",
            "Train Loss: 4.6008917706785724e-05\n",
            "Train Loss: 0.00039282027864828706\n",
            "Train Loss: 0.0008135626558214426\n",
            "Test Loss: 0.0018373331986367702\n",
            "Epoch 265\n",
            " ------------------------\n",
            "Train Loss: 0.0015227722469717264\n",
            "Train Loss: 0.0006644518580287695\n",
            "Train Loss: 4.773637556354515e-05\n",
            "Train Loss: 4.5414264604914933e-05\n",
            "Train Loss: 0.00038774279528297484\n",
            "Train Loss: 0.0008030464523471892\n",
            "Test Loss: 0.0018135858117602766\n",
            "Epoch 266\n",
            " ------------------------\n",
            "Train Loss: 0.001503095030784607\n",
            "Train Loss: 0.0006558668101206422\n",
            "Train Loss: 4.7119414375629276e-05\n",
            "Train Loss: 4.482728036236949e-05\n",
            "Train Loss: 0.00038273390964604914\n",
            "Train Loss: 0.0007926708203740418\n",
            "Test Loss: 0.0017901514074765146\n",
            "Epoch 267\n",
            " ------------------------\n",
            "Train Loss: 0.001483671716414392\n",
            "Train Loss: 0.0006473910761997104\n",
            "Train Loss: 4.651059498428367e-05\n",
            "Train Loss: 4.4248135964153334e-05\n",
            "Train Loss: 0.0003777880338020623\n",
            "Train Loss: 0.0007824293570593\n",
            "Test Loss: 0.0017670245724730194\n",
            "Epoch 268\n",
            " ------------------------\n",
            "Train Loss: 0.0014644970651715994\n",
            "Train Loss: 0.0006390234339050949\n",
            "Train Loss: 4.590939715853892e-05\n",
            "Train Loss: 4.367678047856316e-05\n",
            "Train Loss: 0.00037290898035280406\n",
            "Train Loss: 0.0007723240996710956\n",
            "Test Loss: 0.0017441961681470275\n",
            "Epoch 269\n",
            " ------------------------\n",
            "Train Loss: 0.0014455713098868728\n",
            "Train Loss: 0.0006307649309746921\n",
            "Train Loss: 4.531590093392879e-05\n",
            "Train Loss: 4.3112406274303794e-05\n",
            "Train Loss: 0.0003680900845210999\n",
            "Train Loss: 0.000762342126108706\n",
            "Test Loss: 0.0017216566484421492\n",
            "Epoch 270\n",
            " ------------------------\n",
            "Train Loss: 0.0014268903760239482\n",
            "Train Loss: 0.0006226133555173874\n",
            "Train Loss: 4.47303282271605e-05\n",
            "Train Loss: 4.255538442521356e-05\n",
            "Train Loss: 0.0003633323940448463\n",
            "Train Loss: 0.0007524856482632458\n",
            "Test Loss: 0.001699400891084224\n",
            "Epoch 271\n",
            " ------------------------\n",
            "Train Loss: 0.0014084536815062165\n",
            "Train Loss: 0.0006145699298940599\n",
            "Train Loss: 4.415276271174662e-05\n",
            "Train Loss: 4.2004870920209214e-05\n",
            "Train Loss: 0.0003586368402466178\n",
            "Train Loss: 0.000742764794267714\n",
            "Test Loss: 0.001677441003266722\n",
            "Epoch 272\n",
            " ------------------------\n",
            "Train Loss: 0.0013902507489547133\n",
            "Train Loss: 0.0006066257483325899\n",
            "Train Loss: 4.3581716454355046e-05\n",
            "Train Loss: 4.1462615627096966e-05\n",
            "Train Loss: 0.0003540045290719718\n",
            "Train Loss: 0.0007331668166443706\n",
            "Test Loss: 0.0016557647031731904\n",
            "Epoch 273\n",
            " ------------------------\n",
            "Train Loss: 0.0013722839066758752\n",
            "Train Loss: 0.0005987865733914077\n",
            "Train Loss: 4.3018673750339076e-05\n",
            "Train Loss: 4.092688322998583e-05\n",
            "Train Loss: 0.000349428883055225\n",
            "Train Loss: 0.0007236938690766692\n",
            "Test Loss: 0.0016343749011866748\n",
            "Epoch 274\n",
            " ------------------------\n",
            "Train Loss: 0.0013545486144721508\n",
            "Train Loss: 0.0005910471663810313\n",
            "Train Loss: 4.246246680850163e-05\n",
            "Train Loss: 4.0398073906544596e-05\n",
            "Train Loss: 0.0003449138894211501\n",
            "Train Loss: 0.0007143431575968862\n",
            "Test Loss: 0.001613255764823407\n",
            "Epoch 275\n",
            " ------------------------\n",
            "Train Loss: 0.001337045687250793\n",
            "Train Loss: 0.0005834100884385407\n",
            "Train Loss: 4.1913877794286236e-05\n",
            "Train Loss: 3.98761585529428e-05\n",
            "Train Loss: 0.00034045797656290233\n",
            "Train Loss: 0.0007051101420074701\n",
            "Test Loss: 0.00159240432549268\n",
            "Epoch 276\n",
            " ------------------------\n",
            "Train Loss: 0.0013197696534916759\n",
            "Train Loss: 0.0005758725455962121\n",
            "Train Loss: 4.137238647672348e-05\n",
            "Train Loss: 3.9360464143101126e-05\n",
            "Train Loss: 0.0003360552655067295\n",
            "Train Loss: 0.0006959938327781856\n",
            "Test Loss: 0.0015718189533799887\n",
            "Epoch 277\n",
            " ------------------------\n",
            "Train Loss: 0.0013027189997956157\n",
            "Train Loss: 0.0005684339557774365\n",
            "Train Loss: 4.0838065615389496e-05\n",
            "Train Loss: 3.885138357873075e-05\n",
            "Train Loss: 0.00033171186805702746\n",
            "Train Loss: 0.0006870009237900376\n",
            "Test Loss: 0.0015515103586949408\n",
            "Epoch 278\n",
            " ------------------------\n",
            "Train Loss: 0.0012858848785981536\n",
            "Train Loss: 0.0005610883818008006\n",
            "Train Loss: 4.0310285839950666e-05\n",
            "Train Loss: 3.834928065771237e-05\n",
            "Train Loss: 0.00032742484472692013\n",
            "Train Loss: 0.0006781239062547684\n",
            "Test Loss: 0.0015314625925384462\n",
            "Epoch 279\n",
            " ------------------------\n",
            "Train Loss: 0.0012692686868831515\n",
            "Train Loss: 0.00055383809376508\n",
            "Train Loss: 3.978944005211815e-05\n",
            "Train Loss: 3.785383523791097e-05\n",
            "Train Loss: 0.00032319454476237297\n",
            "Train Loss: 0.0006693617906421423\n",
            "Test Loss: 0.001511669543106109\n",
            "Epoch 280\n",
            " ------------------------\n",
            "Train Loss: 0.001252868096344173\n",
            "Train Loss: 0.0005466816946864128\n",
            "Train Loss: 3.927558645955287e-05\n",
            "Train Loss: 3.7364377931226045e-05\n",
            "Train Loss: 0.00031901613692753017\n",
            "Train Loss: 0.0006607102695852518\n",
            "Test Loss: 0.0014921339461579919\n",
            "Epoch 281\n",
            " ------------------------\n",
            "Train Loss: 0.0012366792652755976\n",
            "Train Loss: 0.0005396170308813453\n",
            "Train Loss: 3.8767902879044414e-05\n",
            "Train Loss: 3.6881869164062664e-05\n",
            "Train Loss: 0.0003148964897263795\n",
            "Train Loss: 0.0006521743489429355\n",
            "Test Loss: 0.001472857256885618\n",
            "Epoch 282\n",
            " ------------------------\n",
            "Train Loss: 0.0012206941610202193\n",
            "Train Loss: 0.0005326411919668317\n",
            "Train Loss: 3.8266476622084156e-05\n",
            "Train Loss: 3.640565773821436e-05\n",
            "Train Loss: 0.000310826872009784\n",
            "Train Loss: 0.0006437469273805618\n",
            "Test Loss: 0.0014538256218656898\n",
            "Epoch 283\n",
            " ------------------------\n",
            "Train Loss: 0.0012049170909449458\n",
            "Train Loss: 0.0005257563898339868\n",
            "Train Loss: 3.777195524889976e-05\n",
            "Train Loss: 3.5935037885792553e-05\n",
            "Train Loss: 0.0003068105725105852\n",
            "Train Loss: 0.0006354254437610507\n",
            "Test Loss: 0.0014350319979712367\n",
            "Epoch 284\n",
            " ------------------------\n",
            "Train Loss: 0.0011893481714650989\n",
            "Train Loss: 0.0005189639632590115\n",
            "Train Loss: 3.728420415427536e-05\n",
            "Train Loss: 3.547030428308062e-05\n",
            "Train Loss: 0.00030284313834272325\n",
            "Train Loss: 0.0006272117607295513\n",
            "Test Loss: 0.0014164858148433268\n",
            "Epoch 285\n",
            " ------------------------\n",
            "Train Loss: 0.0011739761102944613\n",
            "Train Loss: 0.0005122567526996136\n",
            "Train Loss: 3.680205190903507e-05\n",
            "Train Loss: 3.501215542200953e-05\n",
            "Train Loss: 0.00029893169994466007\n",
            "Train Loss: 0.0006191080319695175\n",
            "Test Loss: 0.0013981802621856332\n",
            "Epoch 286\n",
            " ------------------------\n",
            "Train Loss: 0.0011588043998926878\n",
            "Train Loss: 0.0005056382506154478\n",
            "Train Loss: 3.63267281500157e-05\n",
            "Train Loss: 3.455937257967889e-05\n",
            "Train Loss: 0.0002950666821561754\n",
            "Train Loss: 0.0006111055845394731\n",
            "Test Loss: 0.0013801116147078574\n",
            "Epoch 287\n",
            " ------------------------\n",
            "Train Loss: 0.0011438311776146293\n",
            "Train Loss: 0.0004991051973775029\n",
            "Train Loss: 3.585732338251546e-05\n",
            "Train Loss: 3.411277793929912e-05\n",
            "Train Loss: 0.000291254575131461\n",
            "Train Loss: 0.0006032093660905957\n",
            "Test Loss: 0.0013622725382447243\n",
            "Epoch 288\n",
            " ------------------------\n",
            "Train Loss: 0.001129048760049045\n",
            "Train Loss: 0.0004926543333567679\n",
            "Train Loss: 3.5393721191212535e-05\n",
            "Train Loss: 3.3671782148303464e-05\n",
            "Train Loss: 0.0002874885394703597\n",
            "Train Loss: 0.0005954110529273748\n",
            "Test Loss: 0.0013446673983708024\n",
            "Epoch 289\n",
            " ------------------------\n",
            "Train Loss: 0.001114460057578981\n",
            "Train Loss: 0.0004862881323788315\n",
            "Train Loss: 3.493652184261009e-05\n",
            "Train Loss: 3.3236741728615016e-05\n",
            "Train Loss: 0.00028377430862747133\n",
            "Train Loss: 0.0005877183866687119\n",
            "Test Loss: 0.0013272956712171435\n",
            "Epoch 290\n",
            " ------------------------\n",
            "Train Loss: 0.0011000585509464145\n",
            "Train Loss: 0.00048000356764532626\n",
            "Train Loss: 3.4484917705412954e-05\n",
            "Train Loss: 3.280729288235307e-05\n",
            "Train Loss: 0.00028010798268951476\n",
            "Train Loss: 0.0005801260704174638\n",
            "Test Loss: 0.0013101431832183152\n",
            "Epoch 291\n",
            " ------------------------\n",
            "Train Loss: 0.0010858412133529782\n",
            "Train Loss: 0.0004738002025987953\n",
            "Train Loss: 3.4039196179946885e-05\n",
            "Train Loss: 3.2383464713348076e-05\n",
            "Train Loss: 0.00027648755349218845\n",
            "Train Loss: 0.0005726257804781199\n",
            "Test Loss: 0.0012932097015436739\n",
            "Epoch 292\n",
            " ------------------------\n",
            "Train Loss: 0.0010718120029196143\n",
            "Train Loss: 0.0004676796088460833\n",
            "Train Loss: 3.359974652994424e-05\n",
            "Train Loss: 3.1964675144990906e-05\n",
            "Train Loss: 0.0002729140978772193\n",
            "Train Loss: 0.0005652282852679491\n",
            "Test Loss: 0.0012765015999320894\n",
            "Epoch 293\n",
            " ------------------------\n",
            "Train Loss: 0.0010579618392512202\n",
            "Train Loss: 0.0004616345395334065\n",
            "Train Loss: 3.316548827569932e-05\n",
            "Train Loss: 3.1551968277199194e-05\n",
            "Train Loss: 0.0002693893329706043\n",
            "Train Loss: 0.0005579253775067627\n",
            "Test Loss: 0.0012600113986991346\n",
            "Epoch 294\n",
            " ------------------------\n",
            "Train Loss: 0.0010442885104566813\n",
            "Train Loss: 0.00045566848712041974\n",
            "Train Loss: 3.273665060987696e-05\n",
            "Train Loss: 3.1144500098889694e-05\n",
            "Train Loss: 0.0002659084275364876\n",
            "Train Loss: 0.0005507174646481872\n",
            "Test Loss: 0.001243726583197713\n",
            "Epoch 295\n",
            " ------------------------\n",
            "Train Loss: 0.001030795625410974\n",
            "Train Loss: 0.0004497819172684103\n",
            "Train Loss: 3.231368464184925e-05\n",
            "Train Loss: 3.074185224249959e-05\n",
            "Train Loss: 0.00026247231289744377\n",
            "Train Loss: 0.0005436008796095848\n",
            "Test Loss: 0.0012276571069378406\n",
            "Epoch 296\n",
            " ------------------------\n",
            "Train Loss: 0.0010174763156101108\n",
            "Train Loss: 0.00044396944576874375\n",
            "Train Loss: 3.1896164728095755e-05\n",
            "Train Loss: 3.034438668692019e-05\n",
            "Train Loss: 0.00025907918461598456\n",
            "Train Loss: 0.0005365734687075019\n",
            "Test Loss: 0.0012117887963540852\n",
            "Epoch 297\n",
            " ------------------------\n",
            "Train Loss: 0.0010043269721791148\n",
            "Train Loss: 0.00043823206215165555\n",
            "Train Loss: 3.148398536723107e-05\n",
            "Train Loss: 2.9952245313324966e-05\n",
            "Train Loss: 0.00025573238963261247\n",
            "Train Loss: 0.0005296425661072135\n",
            "Test Loss: 0.001196132943732664\n",
            "Epoch 298\n",
            " ------------------------\n",
            "Train Loss: 0.0009913499234244227\n",
            "Train Loss: 0.0004325689806137234\n",
            "Train Loss: 3.1077022867975757e-05\n",
            "Train Loss: 2.956539719889406e-05\n",
            "Train Loss: 0.00025242890114896\n",
            "Train Loss: 0.0005227989749982953\n",
            "Test Loss: 0.0011806775873992592\n",
            "Epoch 299\n",
            " ------------------------\n",
            "Train Loss: 0.0009785372531041503\n",
            "Train Loss: 0.00042697807657532394\n",
            "Train Loss: 3.0675499147037044e-05\n",
            "Train Loss: 2.9183534934418276e-05\n",
            "Train Loss: 0.00024916609982028604\n",
            "Train Loss: 0.0005160431610420346\n",
            "Test Loss: 0.0011654210684355348\n",
            "Epoch 300\n",
            " ------------------------\n",
            "Train Loss: 0.0009658882627263665\n",
            "Train Loss: 0.0004214579821564257\n",
            "Train Loss: 3.0278808480943553e-05\n",
            "Train Loss: 2.8806460250052623e-05\n",
            "Train Loss: 0.00024594669230282307\n",
            "Train Loss: 0.0005093744257465005\n",
            "Test Loss: 0.0011503613786771894\n",
            "Epoch 301\n",
            " ------------------------\n",
            "Train Loss: 0.0009534051641821861\n",
            "Train Loss: 0.0004160120151937008\n",
            "Train Loss: 2.9887745768064633e-05\n",
            "Train Loss: 2.8434256819309667e-05\n",
            "Train Loss: 0.00024276830663438886\n",
            "Train Loss: 0.0005027904990129173\n",
            "Test Loss: 0.00113549068919383\n",
            "Epoch 302\n",
            " ------------------------\n",
            "Train Loss: 0.000941086036618799\n",
            "Train Loss: 0.0004106371779926121\n",
            "Train Loss: 2.9501417884603143e-05\n",
            "Train Loss: 2.806676275213249e-05\n",
            "Train Loss: 0.00023963162675499916\n",
            "Train Loss: 0.0004962945240549743\n",
            "Test Loss: 0.0011208175565116107\n",
            "Epoch 303\n",
            " ------------------------\n",
            "Train Loss: 0.00092892482643947\n",
            "Train Loss: 0.0004053300654049963\n",
            "Train Loss: 2.9120108592906035e-05\n",
            "Train Loss: 2.7703896193997934e-05\n",
            "Train Loss: 0.00023653345124330372\n",
            "Train Loss: 0.0004898790502920747\n",
            "Test Loss: 0.0011063327547162771\n",
            "Epoch 304\n",
            " ------------------------\n",
            "Train Loss: 0.0009169219993054867\n",
            "Train Loss: 0.00040009315125644207\n",
            "Train Loss: 2.8743952498189174e-05\n",
            "Train Loss: 2.7345906346454285e-05\n",
            "Train Loss: 0.0002334779128432274\n",
            "Train Loss: 0.00048354995669797063\n",
            "Test Loss: 0.0010920358472503722\n",
            "Epoch 305\n",
            " ------------------------\n",
            "Train Loss: 0.000905073422472924\n",
            "Train Loss: 0.0003949230595026165\n",
            "Train Loss: 2.837248393916525e-05\n",
            "Train Loss: 2.6992414859705605e-05\n",
            "Train Loss: 0.0002304598456248641\n",
            "Train Loss: 0.0004772984830196947\n",
            "Test Loss: 0.0010779195872601122\n",
            "Epoch 306\n",
            " ------------------------\n",
            "Train Loss: 0.0008933778735809028\n",
            "Train Loss: 0.00038981952820904553\n",
            "Train Loss: 2.8006053980789147e-05\n",
            "Train Loss: 2.664323073986452e-05\n",
            "Train Loss: 0.00022747933689970523\n",
            "Train Loss: 0.00047112908214330673\n",
            "Test Loss: 0.0010639908141456544\n",
            "Epoch 307\n",
            " ------------------------\n",
            "Train Loss: 0.0008818337810225785\n",
            "Train Loss: 0.0003847820626106113\n",
            "Train Loss: 2.7643825887935236e-05\n",
            "Train Loss: 2.6299243472749367e-05\n",
            "Train Loss: 0.0002245426003355533\n",
            "Train Loss: 0.00046504478086717427\n",
            "Test Loss: 0.0010502453078515828\n",
            "Epoch 308\n",
            " ------------------------\n",
            "Train Loss: 0.0008704362553544343\n",
            "Train Loss: 0.0003798088582698256\n",
            "Train Loss: 2.728669460339006e-05\n",
            "Train Loss: 2.595947626105044e-05\n",
            "Train Loss: 0.00022164153051562607\n",
            "Train Loss: 0.00045903580030426383\n",
            "Test Loss: 0.001036676054354757\n",
            "Epoch 309\n",
            " ------------------------\n",
            "Train Loss: 0.0008591892546974123\n",
            "Train Loss: 0.0003749004390556365\n",
            "Train Loss: 2.693399801501073e-05\n",
            "Train Loss: 2.5624214686104096e-05\n",
            "Train Loss: 0.0002187781356042251\n",
            "Train Loss: 0.0004531053127720952\n",
            "Test Loss: 0.0010232808126602322\n",
            "Epoch 310\n",
            " ------------------------\n",
            "Train Loss: 0.0008480854448862374\n",
            "Train Loss: 0.0003700565139297396\n",
            "Train Loss: 2.6585805244394578e-05\n",
            "Train Loss: 2.5293225917266682e-05\n",
            "Train Loss: 0.0002159513533115387\n",
            "Train Loss: 0.00044724991312250495\n",
            "Test Loss: 0.0010100608342327178\n",
            "Epoch 311\n",
            " ------------------------\n",
            "Train Loss: 0.0008371245348826051\n",
            "Train Loss: 0.00036527341580949724\n",
            "Train Loss: 2.6242316380375996e-05\n",
            "Train Loss: 2.496638444426935e-05\n",
            "Train Loss: 0.00021316067432053387\n",
            "Train Loss: 0.00044147143489681184\n",
            "Test Loss: 0.0009970101236831397\n",
            "Epoch 312\n",
            " ------------------------\n",
            "Train Loss: 0.0008263072231784463\n",
            "Train Loss: 0.00036055329837836325\n",
            "Train Loss: 2.5903033019858412e-05\n",
            "Train Loss: 2.4644006771268323e-05\n",
            "Train Loss: 0.0002104067971231416\n",
            "Train Loss: 0.0004357666475698352\n",
            "Test Loss: 0.0009841207938734442\n",
            "Epoch 313\n",
            " ------------------------\n",
            "Train Loss: 0.0008156312978826463\n",
            "Train Loss: 0.0003558954631444067\n",
            "Train Loss: 2.556856816227082e-05\n",
            "Train Loss: 2.432518158457242e-05\n",
            "Train Loss: 0.0002076849777949974\n",
            "Train Loss: 0.0004301323206163943\n",
            "Test Loss: 0.000971400790149346\n",
            "Epoch 314\n",
            " ------------------------\n",
            "Train Loss: 0.0008050922187976539\n",
            "Train Loss: 0.0003512966213747859\n",
            "Train Loss: 2.523826333344914e-05\n",
            "Train Loss: 2.4010445486055687e-05\n",
            "Train Loss: 0.00020499991660472006\n",
            "Train Loss: 0.0004245715099386871\n",
            "Test Loss: 0.0009588417306076735\n",
            "Epoch 315\n",
            " ------------------------\n",
            "Train Loss: 0.000794689345639199\n",
            "Train Loss: 0.0003467584028840065\n",
            "Train Loss: 2.491209306754172e-05\n",
            "Train Loss: 2.3700229576206766e-05\n",
            "Train Loss: 0.0002023516281042248\n",
            "Train Loss: 0.00041908599087037146\n",
            "Test Loss: 0.0009464559843763709\n",
            "Epoch 316\n",
            " ------------------------\n",
            "Train Loss: 0.0007844189531169832\n",
            "Train Loss: 0.00034227693686261773\n",
            "Train Loss: 2.459029565216042e-05\n",
            "Train Loss: 2.3394026356982067e-05\n",
            "Train Loss: 0.00019973581947851926\n",
            "Train Loss: 0.0004136700590606779\n",
            "Test Loss: 0.0009342257690150291\n",
            "Epoch 317\n",
            " ------------------------\n",
            "Train Loss: 0.0007742807501927018\n",
            "Train Loss: 0.000337852950906381\n",
            "Train Loss: 2.427256367809605e-05\n",
            "Train Loss: 2.3091613911674358e-05\n",
            "Train Loss: 0.00019715535745490342\n",
            "Train Loss: 0.0004083230742253363\n",
            "Test Loss: 0.0009221468644682318\n",
            "Epoch 318\n",
            " ------------------------\n",
            "Train Loss: 0.0007642754353582859\n",
            "Train Loss: 0.0003334868815727532\n",
            "Train Loss: 2.395887531747576e-05\n",
            "Train Loss: 2.279301952512469e-05\n",
            "Train Loss: 0.00019460644398350269\n",
            "Train Loss: 0.00040304564754478633\n",
            "Test Loss: 0.0009102332987822592\n",
            "Epoch 319\n",
            " ------------------------\n",
            "Train Loss: 0.0007543982937932014\n",
            "Train Loss: 0.0003291770408395678\n",
            "Train Loss: 2.3649232389288954e-05\n",
            "Train Loss: 2.249882891192101e-05\n",
            "Train Loss: 0.00019209271704312414\n",
            "Train Loss: 0.00039783818647265434\n",
            "Test Loss: 0.0008984722080640495\n",
            "Epoch 320\n",
            " ------------------------\n",
            "Train Loss: 0.0007446491508744657\n",
            "Train Loss: 0.00032492238096892834\n",
            "Train Loss: 2.334347846044693e-05\n",
            "Train Loss: 2.2207801521290094e-05\n",
            "Train Loss: 0.00018961045134346932\n",
            "Train Loss: 0.00039269766421057284\n",
            "Test Loss: 0.000886859925230965\n",
            "Epoch 321\n",
            " ------------------------\n",
            "Train Loss: 0.0007350278319790959\n",
            "Train Loss: 0.00032072505564428866\n",
            "Train Loss: 2.304189001733903e-05\n",
            "Train Loss: 2.1920937797403894e-05\n",
            "Train Loss: 0.00018716062186285853\n",
            "Train Loss: 0.00038762460462749004\n",
            "Test Loss: 0.0008754020964261144\n",
            "Epoch 322\n",
            " ------------------------\n",
            "Train Loss: 0.000725527701433748\n",
            "Train Loss: 0.0003165790985804051\n",
            "Train Loss: 2.274395046697464e-05\n",
            "Train Loss: 2.1637895770254545e-05\n",
            "Train Loss: 0.0001847429375629872\n",
            "Train Loss: 0.0003826172323897481\n",
            "Test Loss: 0.0008640901942271739\n",
            "Epoch 323\n",
            " ------------------------\n",
            "Train Loss: 0.0007161499233916402\n",
            "Train Loss: 0.0003124874201603234\n",
            "Train Loss: 2.2450096366810612e-05\n",
            "Train Loss: 2.1358368030632846e-05\n",
            "Train Loss: 0.00018235652532894164\n",
            "Train Loss: 0.00037767254980280995\n",
            "Test Loss: 0.0008529247716069221\n",
            "Epoch 324\n",
            " ------------------------\n",
            "Train Loss: 0.0007068963604979217\n",
            "Train Loss: 0.00030844935099594295\n",
            "Train Loss: 2.2159967556945048e-05\n",
            "Train Loss: 2.1082383682369255e-05\n",
            "Train Loss: 0.00017999914416577667\n",
            "Train Loss: 0.00037279108073562384\n",
            "Test Loss: 0.0008419044024776667\n",
            "Epoch 325\n",
            " ------------------------\n",
            "Train Loss: 0.0006977591547183692\n",
            "Train Loss: 0.00030446151504293084\n",
            "Train Loss: 2.187339669035282e-05\n",
            "Train Loss: 2.081017373711802e-05\n",
            "Train Loss: 0.00017767492681741714\n",
            "Train Loss: 0.0003679779183585197\n",
            "Test Loss: 0.0008310308039654046\n",
            "Epoch 326\n",
            " ------------------------\n",
            "Train Loss: 0.0006887412164360285\n",
            "Train Loss: 0.00030052641523070633\n",
            "Train Loss: 2.1590520191239193e-05\n",
            "Train Loss: 2.0541390767903067e-05\n",
            "Train Loss: 0.00017537889652885497\n",
            "Train Loss: 0.00036322110099717975\n",
            "Test Loss: 0.0008202896569855511\n",
            "Epoch 327\n",
            " ------------------------\n",
            "Train Loss: 0.0006798419053666294\n",
            "Train Loss: 0.0002966437314171344\n",
            "Train Loss: 2.1311609089025296e-05\n",
            "Train Loss: 2.0275905626476742e-05\n",
            "Train Loss: 0.00017311243573203683\n",
            "Train Loss: 0.00035852682776749134\n",
            "Test Loss: 0.0008096889650914818\n",
            "Epoch 328\n",
            " ------------------------\n",
            "Train Loss: 0.0006710579618811607\n",
            "Train Loss: 0.0002928108151536435\n",
            "Train Loss: 2.1036290490883403e-05\n",
            "Train Loss: 2.001381290028803e-05\n",
            "Train Loss: 0.00017087611195165664\n",
            "Train Loss: 0.00035389448748901486\n",
            "Test Loss: 0.0007992273021955043\n",
            "Epoch 329\n",
            " ------------------------\n",
            "Train Loss: 0.0006623856606893241\n",
            "Train Loss: 0.0002890267933253199\n",
            "Train Loss: 2.076455348287709e-05\n",
            "Train Loss: 1.9755021639866754e-05\n",
            "Train Loss: 0.00016866507939994335\n",
            "Train Loss: 0.00034931680420413613\n",
            "Test Loss: 0.0007888897671364248\n",
            "Epoch 330\n",
            " ------------------------\n",
            "Train Loss: 0.0006538278539665043\n",
            "Train Loss: 0.000285294430796057\n",
            "Train Loss: 2.0496370780165307e-05\n",
            "Train Loss: 1.9499280824675225e-05\n",
            "Train Loss: 0.00016648457676637918\n",
            "Train Loss: 0.0003448021016083658\n",
            "Test Loss: 0.0007786940259393305\n",
            "Epoch 331\n",
            " ------------------------\n",
            "Train Loss: 0.00064537936123088\n",
            "Train Loss: 0.000281606859061867\n",
            "Train Loss: 2.0231589587638155e-05\n",
            "Train Loss: 1.9247570890001953e-05\n",
            "Train Loss: 0.00016433352720923722\n",
            "Train Loss: 0.00034034764394164085\n",
            "Test Loss: 0.0007686344324611127\n",
            "Epoch 332\n",
            " ------------------------\n",
            "Train Loss: 0.0006370380287989974\n",
            "Train Loss: 0.00027796716312877834\n",
            "Train Loss: 1.9969984350609593e-05\n",
            "Train Loss: 1.899891321954783e-05\n",
            "Train Loss: 0.0001622111303731799\n",
            "Train Loss: 0.0003359526745043695\n",
            "Test Loss: 0.0007587063009850681\n",
            "Epoch 333\n",
            " ------------------------\n",
            "Train Loss: 0.0006288039148785174\n",
            "Train Loss: 0.00027437368407845497\n",
            "Train Loss: 1.9711726054083556e-05\n",
            "Train Loss: 1.875364250736311e-05\n",
            "Train Loss: 0.00016011556726880372\n",
            "Train Loss: 0.00033161125611513853\n",
            "Test Loss: 0.000748901889892295\n",
            "Epoch 334\n",
            " ------------------------\n",
            "Train Loss: 0.0006206772523000836\n",
            "Train Loss: 0.00027082726592198014\n",
            "Train Loss: 1.9457014786894433e-05\n",
            "Train Loss: 1.8511453163227998e-05\n",
            "Train Loss: 0.00015804730355739594\n",
            "Train Loss: 0.0003273261827416718\n",
            "Test Loss: 0.0007392244879156351\n",
            "Epoch 335\n",
            " ------------------------\n",
            "Train Loss: 0.0006126570515334606\n",
            "Train Loss: 0.00026732831611298025\n",
            "Train Loss: 1.9205528587917797e-05\n",
            "Train Loss: 1.8272048691869713e-05\n",
            "Train Loss: 0.00015600366168655455\n",
            "Train Loss: 0.0003230935544706881\n",
            "Test Loss: 0.0007296658877748996\n",
            "Epoch 336\n",
            " ------------------------\n",
            "Train Loss: 0.0006047403439879417\n",
            "Train Loss: 0.00026387450634501874\n",
            "Train Loss: 1.8957578504341654e-05\n",
            "Train Loss: 1.803548730094917e-05\n",
            "Train Loss: 0.00015398595132865012\n",
            "Train Loss: 0.00031891721300780773\n",
            "Test Loss: 0.0007202357519418001\n",
            "Epoch 337\n",
            " ------------------------\n",
            "Train Loss: 0.000596926489379257\n",
            "Train Loss: 0.00026046435232274234\n",
            "Train Loss: 1.871253152785357e-05\n",
            "Train Loss: 1.780260026862379e-05\n",
            "Train Loss: 0.0001519958459539339\n",
            "Train Loss: 0.0003147945099044591\n",
            "Test Loss: 0.0007109264261089265\n",
            "Epoch 338\n",
            " ------------------------\n",
            "Train Loss: 0.0005892135086469352\n",
            "Train Loss: 0.00025710021145641804\n",
            "Train Loss: 1.847080966399517e-05\n",
            "Train Loss: 1.757228710630443e-05\n",
            "Train Loss: 0.00015003101725596935\n",
            "Train Loss: 0.0003107249503955245\n",
            "Test Loss: 0.0007017398893367499\n",
            "Epoch 339\n",
            " ------------------------\n",
            "Train Loss: 0.0005815990152768791\n",
            "Train Loss: 0.00025377795100212097\n",
            "Train Loss: 1.823234742914792e-05\n",
            "Train Loss: 1.7345066225971095e-05\n",
            "Train Loss: 0.00014809156709816307\n",
            "Train Loss: 0.0003067100187763572\n",
            "Test Loss: 0.0006926672940608114\n",
            "Epoch 340\n",
            " ------------------------\n",
            "Train Loss: 0.0005740844644606113\n",
            "Train Loss: 0.00025049890973605216\n",
            "Train Loss: 1.799682831915561e-05\n",
            "Train Loss: 1.7120652046287432e-05\n",
            "Train Loss: 0.00014617778651881963\n",
            "Train Loss: 0.00030274607706815004\n",
            "Test Loss: 0.0006837164401076734\n",
            "Epoch 341\n",
            " ------------------------\n",
            "Train Loss: 0.0005666652577929199\n",
            "Train Loss: 0.0002472613414283842\n",
            "Train Loss: 1.7764097719918936e-05\n",
            "Train Loss: 1.6899564798222855e-05\n",
            "Train Loss: 0.00014428851136472076\n",
            "Train Loss: 0.0002988327178172767\n",
            "Test Loss: 0.0006748804589733481\n",
            "Epoch 342\n",
            " ------------------------\n",
            "Train Loss: 0.0005593433743342757\n",
            "Train Loss: 0.00024406681768596172\n",
            "Train Loss: 1.753460310283117e-05\n",
            "Train Loss: 1.668119330133777e-05\n",
            "Train Loss: 0.00014242390170693398\n",
            "Train Loss: 0.000294972414849326\n",
            "Test Loss: 0.0006661591614829376\n",
            "Epoch 343\n",
            " ------------------------\n",
            "Train Loss: 0.0005521142738871276\n",
            "Train Loss: 0.00024091159866657108\n",
            "Train Loss: 1.7307771486230195e-05\n",
            "Train Loss: 1.6465817680000328e-05\n",
            "Train Loss: 0.00014058467058930546\n",
            "Train Loss: 0.0002911616466008127\n",
            "Test Loss: 0.0006575505249202251\n",
            "Epoch 344\n",
            " ------------------------\n",
            "Train Loss: 0.0005449781892821193\n",
            "Train Loss: 0.00023779789626132697\n",
            "Train Loss: 1.7084046703530475e-05\n",
            "Train Loss: 1.6253006833721884e-05\n",
            "Train Loss: 0.00013876786397304386\n",
            "Train Loss: 0.00028739788103848696\n",
            "Test Loss: 0.000649056353722699\n",
            "Epoch 345\n",
            " ------------------------\n",
            "Train Loss: 0.0005379351787269115\n",
            "Train Loss: 0.0002347245317650959\n",
            "Train Loss: 1.6863436030689627e-05\n",
            "Train Loss: 1.6042975403252058e-05\n",
            "Train Loss: 0.00013697466056328267\n",
            "Train Loss: 0.0002836837084032595\n",
            "Test Loss: 0.0006406676402548328\n",
            "Epoch 346\n",
            " ------------------------\n",
            "Train Loss: 0.0005309839616529644\n",
            "Train Loss: 0.00023169159248936921\n",
            "Train Loss: 1.6645366486045532e-05\n",
            "Train Loss: 1.5835603335290216e-05\n",
            "Train Loss: 0.00013520340144168586\n",
            "Train Loss: 0.0002800176735036075\n",
            "Test Loss: 0.0006323893467197195\n",
            "Epoch 347\n",
            " ------------------------\n",
            "Train Loss: 0.0005241210456006229\n",
            "Train Loss: 0.00022869757958687842\n",
            "Train Loss: 1.6430385585408658e-05\n",
            "Train Loss: 1.563107616675552e-05\n",
            "Train Loss: 0.0001334576663793996\n",
            "Train Loss: 0.0002764012315310538\n",
            "Test Loss: 0.0006242172967176884\n",
            "Epoch 348\n",
            " ------------------------\n",
            "Train Loss: 0.0005173489334993064\n",
            "Train Loss: 0.0002257420273963362\n",
            "Train Loss: 1.6218129530898295e-05\n",
            "Train Loss: 1.542895734019112e-05\n",
            "Train Loss: 0.00013173242041375488\n",
            "Train Loss: 0.0002728280087467283\n",
            "Test Loss: 0.00061614895821549\n",
            "Epoch 349\n",
            " ------------------------\n",
            "Train Loss: 0.0005106633761897683\n",
            "Train Loss: 0.00022282497957348824\n",
            "Train Loss: 1.6008407328627072e-05\n",
            "Train Loss: 1.5229818927764427e-05\n",
            "Train Loss: 0.00013003045751247555\n",
            "Train Loss: 0.000269304437097162\n",
            "Test Loss: 0.0006081895262468606\n",
            "Epoch 350\n",
            " ------------------------\n",
            "Train Loss: 0.0005040644900873303\n",
            "Train Loss: 0.00021994594135321677\n",
            "Train Loss: 1.5801682820892893e-05\n",
            "Train Loss: 1.5032885130494833e-05\n",
            "Train Loss: 0.00012834917288273573\n",
            "Train Loss: 0.00026582228019833565\n",
            "Test Loss: 0.0006003269663779065\n",
            "Epoch 351\n",
            " ------------------------\n",
            "Train Loss: 0.0004975522751919925\n",
            "Train Loss: 0.000217104607145302\n",
            "Train Loss: 1.55975685629528e-05\n",
            "Train Loss: 1.4838413335382938e-05\n",
            "Train Loss: 0.0001266903564101085\n",
            "Train Loss: 0.00026238590362481773\n",
            "Test Loss: 0.000592568947467953\n",
            "Epoch 352\n",
            " ------------------------\n",
            "Train Loss: 0.0004911224241368473\n",
            "Train Loss: 0.00021429853222798556\n",
            "Train Loss: 1.5395980881294236e-05\n",
            "Train Loss: 1.464673005102668e-05\n",
            "Train Loss: 0.00012505293125286698\n",
            "Train Loss: 0.0002589939977042377\n",
            "Test Loss: 0.0005849105655215681\n",
            "Epoch 353\n",
            " ------------------------\n",
            "Train Loss: 0.0004847750242333859\n",
            "Train Loss: 0.0002115292736561969\n",
            "Train Loss: 1.5196996173472144e-05\n",
            "Train Loss: 1.44572932185838e-05\n",
            "Train Loss: 0.0001234374794876203\n",
            "Train Loss: 0.00025564798852428794\n",
            "Test Loss: 0.0005773516168119386\n",
            "Epoch 354\n",
            " ------------------------\n",
            "Train Loss: 0.00047851010458543897\n",
            "Train Loss: 0.00020879563817288727\n",
            "Train Loss: 1.5000669009168632e-05\n",
            "Train Loss: 1.4270497558754869e-05\n",
            "Train Loss: 0.0001218421311932616\n",
            "Train Loss: 0.0002523445291444659\n",
            "Test Loss: 0.0005698921013390645\n",
            "Epoch 355\n",
            " ------------------------\n",
            "Train Loss: 0.0004723269084934145\n",
            "Train Loss: 0.00020609726198017597\n",
            "Train Loss: 1.480672199249966e-05\n",
            "Train Loss: 1.4086008377489634e-05\n",
            "Train Loss: 0.00012026754120597616\n",
            "Train Loss: 0.00024908327031880617\n",
            "Test Loss: 0.0005625250196317211\n",
            "Epoch 356\n",
            " ------------------------\n",
            "Train Loss: 0.000466223806142807\n",
            "Train Loss: 0.00020343430514913052\n",
            "Train Loss: 1.4615465261158533e-05\n",
            "Train Loss: 1.39041358124814e-05\n",
            "Train Loss: 0.00011871347669512033\n",
            "Train Loss: 0.00024586383369751275\n",
            "Test Loss: 0.0005552552756853402\n",
            "Epoch 357\n",
            " ------------------------\n",
            "Train Loss: 0.00046019721776247025\n",
            "Train Loss: 0.00020080555987078696\n",
            "Train Loss: 1.4426603229367174e-05\n",
            "Train Loss: 1.3724241398449522e-05\n",
            "Train Loss: 0.00011717795132426545\n",
            "Train Loss: 0.00024268575361929834\n",
            "Test Loss: 0.0005480802356032655\n",
            "Epoch 358\n",
            " ------------------------\n",
            "Train Loss: 0.00045425212010741234\n",
            "Train Loss: 0.00019821028399746865\n",
            "Train Loss: 1.4240096788853407e-05\n",
            "Train Loss: 1.3546868103730958e-05\n",
            "Train Loss: 0.00011566399916773662\n",
            "Train Loss: 0.0002395495685050264\n",
            "Test Loss: 0.0005409991572378203\n",
            "Epoch 359\n",
            " ------------------------\n",
            "Train Loss: 0.00044838135363534093\n",
            "Train Loss: 0.00019564871035981923\n",
            "Train Loss: 1.4056039617571514e-05\n",
            "Train Loss: 1.3372106877795886e-05\n",
            "Train Loss: 0.00011417060159146786\n",
            "Train Loss: 0.00023645584587939084\n",
            "Test Loss: 0.0005340093484846875\n",
            "Epoch 360\n",
            " ------------------------\n",
            "Train Loss: 0.0004425859951879829\n",
            "Train Loss: 0.00019312019867356867\n",
            "Train Loss: 1.3874430806026794e-05\n",
            "Train Loss: 1.3199244676798116e-05\n",
            "Train Loss: 0.00011269532842561603\n",
            "Train Loss: 0.00023340218467637897\n",
            "Test Loss: 0.0005271106347208843\n",
            "Epoch 361\n",
            " ------------------------\n",
            "Train Loss: 0.00043686546268872917\n",
            "Train Loss: 0.00019062422506976873\n",
            "Train Loss: 1.369507208437426e-05\n",
            "Train Loss: 1.3028896319156047e-05\n",
            "Train Loss: 0.00011124005686724558\n",
            "Train Loss: 0.00023038650397211313\n",
            "Test Loss: 0.0005203008622629568\n",
            "Epoch 362\n",
            " ------------------------\n",
            "Train Loss: 0.00043122025090269744\n",
            "Train Loss: 0.00018815933435689658\n",
            "Train Loss: 1.3517998922907282e-05\n",
            "Train Loss: 1.286059614358237e-05\n",
            "Train Loss: 0.0001098037464544177\n",
            "Train Loss: 0.00022740932763554156\n",
            "Test Loss: 0.0005135775136295706\n",
            "Epoch 363\n",
            " ------------------------\n",
            "Train Loss: 0.00042564814793877304\n",
            "Train Loss: 0.00018572877161204815\n",
            "Train Loss: 1.334327862423379e-05\n",
            "Train Loss: 1.2694552424363792e-05\n",
            "Train Loss: 0.00010838444723049179\n",
            "Train Loss: 0.000224471150431782\n",
            "Test Loss: 0.0005069418548373505\n",
            "Epoch 364\n",
            " ------------------------\n",
            "Train Loss: 0.0004201481060590595\n",
            "Train Loss: 0.00018332920444663614\n",
            "Train Loss: 1.3170894817449152e-05\n",
            "Train Loss: 1.2530258572951425e-05\n",
            "Train Loss: 0.00010698261030483991\n",
            "Train Loss: 0.00022156894556246698\n",
            "Test Loss: 0.0005003858677810058\n",
            "Epoch 365\n",
            " ------------------------\n",
            "Train Loss: 0.0004147193394601345\n",
            "Train Loss: 0.00018096098210662603\n",
            "Train Loss: 1.3000864782952704e-05\n",
            "Train Loss: 1.2368058378342539e-05\n",
            "Train Loss: 0.00010559864313108847\n",
            "Train Loss: 0.0002187041682191193\n",
            "Test Loss: 0.0004939194477628917\n",
            "Epoch 366\n",
            " ------------------------\n",
            "Train Loss: 0.00040936010191217065\n",
            "Train Loss: 0.00017862266395241022\n",
            "Train Loss: 1.2832780157623347e-05\n",
            "Train Loss: 1.220819558511721e-05\n",
            "Train Loss: 0.00010423408821225166\n",
            "Train Loss: 0.00021587638184428215\n",
            "Test Loss: 0.0004875346494372934\n",
            "Epoch 367\n",
            " ------------------------\n",
            "Train Loss: 0.00040407091728411615\n",
            "Train Loss: 0.00017631443915888667\n",
            "Train Loss: 1.2666995644394774e-05\n",
            "Train Loss: 1.2050517398165539e-05\n",
            "Train Loss: 0.0001028878177748993\n",
            "Train Loss: 0.00021308966097421944\n",
            "Test Loss: 0.0004812374245375395\n",
            "Epoch 368\n",
            " ------------------------\n",
            "Train Loss: 0.0003988480311818421\n",
            "Train Loss: 0.0001740355946822092\n",
            "Train Loss: 1.250329842150677e-05\n",
            "Train Loss: 1.189468184747966e-05\n",
            "Train Loss: 0.0001015581437968649\n",
            "Train Loss: 0.00021033486700616777\n",
            "Test Loss: 0.00047501869266852736\n",
            "Epoch 369\n",
            " ------------------------\n",
            "Train Loss: 0.0003936936263926327\n",
            "Train Loss: 0.00017178581038024276\n",
            "Train Loss: 1.2341595720499754e-05\n",
            "Train Loss: 1.1741306479962077e-05\n",
            "Train Loss: 0.00010024693619925529\n",
            "Train Loss: 0.00020761866471730173\n",
            "Test Loss: 0.0004688822227763012\n",
            "Epoch 370\n",
            " ------------------------\n",
            "Train Loss: 0.00038860589847899973\n",
            "Train Loss: 0.00016956594481598586\n",
            "Train Loss: 1.218215129483724e-05\n",
            "Train Loss: 1.1589466339501087e-05\n",
            "Train Loss: 9.895025141304359e-05\n",
            "Train Loss: 0.00020493287593126297\n",
            "Test Loss: 0.00046281921095214784\n",
            "Epoch 371\n",
            " ------------------------\n",
            "Train Loss: 0.0003835852839984\n",
            "Train Loss: 0.00016737547412049025\n",
            "Train Loss: 1.2024803254462313e-05\n",
            "Train Loss: 1.1439774425525684e-05\n",
            "Train Loss: 9.767177107278258e-05\n",
            "Train Loss: 0.0002022857661359012\n",
            "Test Loss: 0.0004568389558698982\n",
            "Epoch 372\n",
            " ------------------------\n",
            "Train Loss: 0.00037862767931073904\n",
            "Train Loss: 0.00016521204088348895\n",
            "Train Loss: 1.186929875984788e-05\n",
            "Train Loss: 1.1292069757473655e-05\n",
            "Train Loss: 9.641086944611743e-05\n",
            "Train Loss: 0.00019967358093708754\n",
            "Test Loss: 0.00045093876542523503\n",
            "Epoch 373\n",
            " ------------------------\n",
            "Train Loss: 0.0003737353254109621\n",
            "Train Loss: 0.00016307712940033525\n",
            "Train Loss: 1.1715906111930963e-05\n",
            "Train Loss: 1.1145976714033168e-05\n",
            "Train Loss: 9.516427962807938e-05\n",
            "Train Loss: 0.00019709340995177627\n",
            "Test Loss: 0.0004451133281691\n",
            "Epoch 374\n",
            " ------------------------\n",
            "Train Loss: 0.0003689047007355839\n",
            "Train Loss: 0.00016096893523354083\n",
            "Train Loss: 1.1564411579456646e-05\n",
            "Train Loss: 1.1002152859873604e-05\n",
            "Train Loss: 9.393531217938289e-05\n",
            "Train Loss: 0.00019454584980849177\n",
            "Test Loss: 0.0004393598937895149\n",
            "Epoch 375\n",
            " ------------------------\n",
            "Train Loss: 0.0003641366492956877\n",
            "Train Loss: 0.00015888854977674782\n",
            "Train Loss: 1.141497978096595e-05\n",
            "Train Loss: 1.0859912435989827e-05\n",
            "Train Loss: 9.272140596294776e-05\n",
            "Train Loss: 0.00019203301053494215\n",
            "Test Loss: 0.0004336811398388818\n",
            "Epoch 376\n",
            " ------------------------\n",
            "Train Loss: 0.00035943122929893434\n",
            "Train Loss: 0.00015683563833590597\n",
            "Train Loss: 1.1267466106801294e-05\n",
            "Train Loss: 1.0719541933212895e-05\n",
            "Train Loss: 9.152322309091687e-05\n",
            "Train Loss: 0.00018955048290081322\n",
            "Test Loss: 0.0004280775465304032\n",
            "Epoch 377\n",
            " ------------------------\n",
            "Train Loss: 0.000354786025127396\n",
            "Train Loss: 0.0001548090804135427\n",
            "Train Loss: 1.1121845091111027e-05\n",
            "Train Loss: 1.0581027709122282e-05\n",
            "Train Loss: 9.033949027070776e-05\n",
            "Train Loss: 0.0001871012500487268\n",
            "Test Loss: 0.00042254646541550756\n",
            "Epoch 378\n",
            " ------------------------\n",
            "Train Loss: 0.0003502017934806645\n",
            "Train Loss: 0.00015280800289474428\n",
            "Train Loss: 1.0978121281368658e-05\n",
            "Train Loss: 1.0444080544402823e-05\n",
            "Train Loss: 8.917218656279147e-05\n",
            "Train Loss: 0.0001846825034590438\n",
            "Test Loss: 0.00041708348726388067\n",
            "Epoch 379\n",
            " ------------------------\n",
            "Train Loss: 0.0003456763515714556\n",
            "Train Loss: 0.0001508336135884747\n",
            "Train Loss: 1.0836287401616573e-05\n",
            "Train Loss: 1.0309221579518635e-05\n",
            "Train Loss: 8.801978401606902e-05\n",
            "Train Loss: 0.00018229562556371093\n",
            "Test Loss: 0.00041169107134919614\n",
            "Epoch 380\n",
            " ------------------------\n",
            "Train Loss: 0.00034120806958526373\n",
            "Train Loss: 0.00014888413716107607\n",
            "Train Loss: 1.069629070116207e-05\n",
            "Train Loss: 1.0175951501878444e-05\n",
            "Train Loss: 8.688135858392343e-05\n",
            "Train Loss: 0.0001799378514988348\n",
            "Test Loss: 0.00040636927587911487\n",
            "Epoch 381\n",
            " ------------------------\n",
            "Train Loss: 0.0003367988101672381\n",
            "Train Loss: 0.00014696024300064892\n",
            "Train Loss: 1.0558158464846201e-05\n",
            "Train Loss: 1.0044256669061724e-05\n",
            "Train Loss: 8.575805986765772e-05\n",
            "Train Loss: 0.0001776121207512915\n",
            "Test Loss: 0.0004011149285361171\n",
            "Epoch 382\n",
            " ------------------------\n",
            "Train Loss: 0.0003324463905300945\n",
            "Train Loss: 0.0001450613053748384\n",
            "Train Loss: 1.0421628758194856e-05\n",
            "Train Loss: 9.914529073284939e-06\n",
            "Train Loss: 8.464956044917926e-05\n",
            "Train Loss: 0.00017531652702018619\n",
            "Test Loss: 0.00039593380643054843\n",
            "Epoch 383\n",
            " ------------------------\n",
            "Train Loss: 0.00032814982114359736\n",
            "Train Loss: 0.00014318634930532426\n",
            "Train Loss: 1.028697624860797e-05\n",
            "Train Loss: 9.786417649593204e-06\n",
            "Train Loss: 8.355659520020708e-05\n",
            "Train Loss: 0.000173052292666398\n",
            "Test Loss: 0.000390819477615878\n",
            "Epoch 384\n",
            " ------------------------\n",
            "Train Loss: 0.0003239076759200543\n",
            "Train Loss: 0.00014133569493424147\n",
            "Train Loss: 1.0154056326427963e-05\n",
            "Train Loss: 9.659961506258696e-06\n",
            "Train Loss: 8.247637015301734e-05\n",
            "Train Loss: 0.00017081372789107263\n",
            "Test Loss: 0.0003857660776702687\n",
            "Epoch 385\n",
            " ------------------------\n",
            "Train Loss: 0.0003197232144884765\n",
            "Train Loss: 0.00013950974971521646\n",
            "Train Loss: 1.00228526207502e-05\n",
            "Train Loss: 9.535143362882081e-06\n",
            "Train Loss: 8.141068974509835e-05\n",
            "Train Loss: 0.00016860829782672226\n",
            "Test Loss: 0.0003807811444858089\n",
            "Epoch 386\n",
            " ------------------------\n",
            "Train Loss: 0.0003155897429678589\n",
            "Train Loss: 0.0001377060398226604\n",
            "Train Loss: 9.893278729578014e-06\n",
            "Train Loss: 9.411734026798513e-06\n",
            "Train Loss: 8.035756036406383e-05\n",
            "Train Loss: 0.00016642568516544998\n",
            "Test Loss: 0.00037585543759632856\n",
            "Epoch 387\n",
            " ------------------------\n",
            "Train Loss: 0.0003115122381132096\n",
            "Train Loss: 0.00013592657342087477\n",
            "Train Loss: 9.765445611265022e-06\n",
            "Train Loss: 9.290127309213858e-06\n",
            "Train Loss: 7.931916479719803e-05\n",
            "Train Loss: 0.00016427712398581207\n",
            "Test Loss: 0.00037100008921697736\n",
            "Epoch 388\n",
            " ------------------------\n",
            "Train Loss: 0.000307486712699756\n",
            "Train Loss: 0.00013417037553153932\n",
            "Train Loss: 9.639316886023153e-06\n",
            "Train Loss: 9.169858458335511e-06\n",
            "Train Loss: 7.82929128035903e-05\n",
            "Train Loss: 0.00016215091454796493\n",
            "Test Loss: 0.00036620296305045485\n",
            "Epoch 389\n",
            " ------------------------\n",
            "Train Loss: 0.0003035147092305124\n",
            "Train Loss: 0.00013243721332401037\n",
            "Train Loss: 9.514845260127913e-06\n",
            "Train Loss: 9.051416782313026e-06\n",
            "Train Loss: 7.728179480182007e-05\n",
            "Train Loss: 0.00016005663201212883\n",
            "Test Loss: 0.00036147344508208334\n",
            "Epoch 390\n",
            " ------------------------\n",
            "Train Loss: 0.0002995904069393873\n",
            "Train Loss: 0.00013072520960122347\n",
            "Train Loss: 9.39174151426414e-06\n",
            "Train Loss: 8.934427569329273e-06\n",
            "Train Loss: 7.628273306181654e-05\n",
            "Train Loss: 0.00015798720414750278\n",
            "Test Loss: 0.00035679939901456237\n",
            "Epoch 391\n",
            " ------------------------\n",
            "Train Loss: 0.0002957188116852194\n",
            "Train Loss: 0.00012903619790449739\n",
            "Train Loss: 9.270343070966192e-06\n",
            "Train Loss: 8.818989044812042e-06\n",
            "Train Loss: 7.52980267861858e-05\n",
            "Train Loss: 0.00015594690921716392\n",
            "Test Loss: 0.0003521884646033868\n",
            "Epoch 392\n",
            " ------------------------\n",
            "Train Loss: 0.000291897653369233\n",
            "Train Loss: 0.00012736863573081791\n",
            "Train Loss: 9.15057989914203e-06\n",
            "Train Loss: 8.705032996658701e-06\n",
            "Train Loss: 7.432413985952735e-05\n",
            "Train Loss: 0.00015393093053717166\n",
            "Test Loss: 0.0003476375568425283\n",
            "Epoch 393\n",
            " ------------------------\n",
            "Train Loss: 0.00028812416712753475\n",
            "Train Loss: 0.00012572195555549115\n",
            "Train Loss: 9.032431989908218e-06\n",
            "Train Loss: 8.592544872954022e-06\n",
            "Train Loss: 7.336321141337976e-05\n",
            "Train Loss: 0.00015194062143564224\n",
            "Test Loss: 0.00034314354707021266\n",
            "Epoch 394\n",
            " ------------------------\n",
            "Train Loss: 0.000284400099189952\n",
            "Train Loss: 0.00012409665214363486\n",
            "Train Loss: 8.915574653656222e-06\n",
            "Train Loss: 8.481438271701336e-06\n",
            "Train Loss: 7.241529965540394e-05\n",
            "Train Loss: 0.0001499778009019792\n",
            "Test Loss: 0.0003387081524124369\n",
            "Epoch 395\n",
            " ------------------------\n",
            "Train Loss: 0.0002807253913488239\n",
            "Train Loss: 0.0001224936859216541\n",
            "Train Loss: 8.800302566669416e-06\n",
            "Train Loss: 8.371879630431067e-06\n",
            "Train Loss: 7.147966243792325e-05\n",
            "Train Loss: 0.0001480408536735922\n",
            "Test Loss: 0.0003343329226481728\n",
            "Epoch 396\n",
            " ------------------------\n",
            "Train Loss: 0.00027709794812835753\n",
            "Train Loss: 0.00012091032840544358\n",
            "Train Loss: 8.686612090968993e-06\n",
            "Train Loss: 8.26374798634788e-06\n",
            "Train Loss: 7.055672904243693e-05\n",
            "Train Loss: 0.0001461280044168234\n",
            "Test Loss: 0.00033001299016177654\n",
            "Epoch 397\n",
            " ------------------------\n",
            "Train Loss: 0.0002735166053753346\n",
            "Train Loss: 0.0001193475691252388\n",
            "Train Loss: 8.574403182137758e-06\n",
            "Train Loss: 8.157036972988863e-06\n",
            "Train Loss: 6.964457861613482e-05\n",
            "Train Loss: 0.00014423977700062096\n",
            "Test Loss: 0.00032574967917753384\n",
            "Epoch 398\n",
            " ------------------------\n",
            "Train Loss: 0.0002699817414395511\n",
            "Train Loss: 0.0001178051606984809\n",
            "Train Loss: 8.463530321023427e-06\n",
            "Train Loss: 8.05160107120173e-06\n",
            "Train Loss: 6.874446262372658e-05\n",
            "Train Loss: 0.00014237521099857986\n",
            "Test Loss: 0.000321538835123647\n",
            "Epoch 399\n",
            " ------------------------\n",
            "Train Loss: 0.0002664927742443979\n",
            "Train Loss: 0.0001162831686087884\n",
            "Train Loss: 8.354206329386216e-06\n",
            "Train Loss: 7.947601261548698e-06\n",
            "Train Loss: 6.785672303522006e-05\n",
            "Train Loss: 0.00014053547056391835\n",
            "Test Loss: 0.00031738194957142696\n",
            "Epoch 400\n",
            " ------------------------\n",
            "Train Loss: 0.00026304848142899573\n",
            "Train Loss: 0.00011477962834760547\n",
            "Train Loss: 8.246091965702362e-06\n",
            "Train Loss: 7.844844731152989e-06\n",
            "Train Loss: 6.69792789267376e-05\n",
            "Train Loss: 0.00013871965347789228\n",
            "Test Loss: 0.00031328346813097596\n",
            "Epoch 401\n",
            " ------------------------\n",
            "Train Loss: 0.0002596498525235802\n",
            "Train Loss: 0.0001132969991886057\n",
            "Train Loss: 8.13964561530156e-06\n",
            "Train Loss: 7.743497917545028e-06\n",
            "Train Loss: 6.611382559640333e-05\n",
            "Train Loss: 0.000136927526909858\n",
            "Test Loss: 0.00030923572194296867\n",
            "Epoch 402\n",
            " ------------------------\n",
            "Train Loss: 0.00025629406445659697\n",
            "Train Loss: 0.00011183247261215001\n",
            "Train Loss: 8.034402526391204e-06\n",
            "Train Loss: 7.64340984460432e-06\n",
            "Train Loss: 6.52598318993114e-05\n",
            "Train Loss: 0.00013515811588149518\n",
            "Test Loss: 0.00030523916939273477\n",
            "Epoch 403\n",
            " ------------------------\n",
            "Train Loss: 0.00025298225227743387\n",
            "Train Loss: 0.00011038743105018511\n",
            "Train Loss: 7.930657375254668e-06\n",
            "Train Loss: 7.544731488451362e-06\n",
            "Train Loss: 6.441669393097982e-05\n",
            "Train Loss: 0.00013341261364985257\n",
            "Test Loss: 0.0003012960441992618\n",
            "Epoch 404\n",
            " ------------------------\n",
            "Train Loss: 0.00024971188395284116\n",
            "Train Loss: 0.00010896027379203588\n",
            "Train Loss: 7.828027264622506e-06\n",
            "Train Loss: 7.447332791343797e-06\n",
            "Train Loss: 6.358489190461114e-05\n",
            "Train Loss: 0.00013168941950425506\n",
            "Test Loss: 0.00029740379977738485\n",
            "Epoch 405\n",
            " ------------------------\n",
            "Train Loss: 0.00024648389080539346\n",
            "Train Loss: 0.00010755136463558301\n",
            "Train Loss: 7.726735020696651e-06\n",
            "Train Loss: 7.35130242901505e-06\n",
            "Train Loss: 6.276436033658683e-05\n",
            "Train Loss: 0.0001299883151659742\n",
            "Test Loss: 0.0002935625598183833\n",
            "Epoch 406\n",
            " ------------------------\n",
            "Train Loss: 0.00024329843290615827\n",
            "Train Loss: 0.0001061614602804184\n",
            "Train Loss: 7.626840670127422e-06\n",
            "Train Loss: 7.256251592480112e-06\n",
            "Train Loss: 6.195274909259751e-05\n",
            "Train Loss: 0.00012830769992433488\n",
            "Test Loss: 0.00028976660541957244\n",
            "Epoch 407\n",
            " ------------------------\n",
            "Train Loss: 0.00024015447706915438\n",
            "Train Loss: 0.00010478956392034888\n",
            "Train Loss: 7.5282982834323775e-06\n",
            "Train Loss: 7.16248632670613e-06\n",
            "Train Loss: 6.115173891885206e-05\n",
            "Train Loss: 0.00012664940732065588\n",
            "Test Loss: 0.00028602254315046594\n",
            "Epoch 408\n",
            " ------------------------\n",
            "Train Loss: 0.0002370507427258417\n",
            "Train Loss: 0.00010343499161535874\n",
            "Train Loss: 7.431095127685694e-06\n",
            "Train Loss: 7.069877028698102e-06\n",
            "Train Loss: 6.036106060491875e-05\n",
            "Train Loss: 0.0001250119530595839\n",
            "Test Loss: 0.0002823248287313618\n",
            "Epoch 409\n",
            " ------------------------\n",
            "Train Loss: 0.00023398744815494865\n",
            "Train Loss: 0.00010209855827270076\n",
            "Train Loss: 7.33501019567484e-06\n",
            "Train Loss: 6.978241344768321e-06\n",
            "Train Loss: 5.958110705250874e-05\n",
            "Train Loss: 0.00012339637032710016\n",
            "Test Loss: 0.0002786762997857295\n",
            "Epoch 410\n",
            " ------------------------\n",
            "Train Loss: 0.00023096259974408895\n",
            "Train Loss: 0.0001007787577691488\n",
            "Train Loss: 7.240198556246469e-06\n",
            "Train Loss: 6.888377811264945e-06\n",
            "Train Loss: 5.881198376300745e-05\n",
            "Train Loss: 0.00012180251360405236\n",
            "Test Loss: 0.00027507610502652824\n",
            "Epoch 411\n",
            " ------------------------\n",
            "Train Loss: 0.00022797740530222654\n",
            "Train Loss: 9.947654325515032e-05\n",
            "Train Loss: 7.146706138883019e-06\n",
            "Train Loss: 6.799146831326652e-06\n",
            "Train Loss: 5.805079854326323e-05\n",
            "Train Loss: 0.00012022751616314054\n",
            "Test Loss: 0.0002715200243983418\n",
            "Epoch 412\n",
            " ------------------------\n",
            "Train Loss: 0.0002250317920697853\n",
            "Train Loss: 9.819117985898629e-05\n",
            "Train Loss: 7.054298748698784e-06\n",
            "Train Loss: 6.711219612043351e-06\n",
            "Train Loss: 5.7300134358229116e-05\n",
            "Train Loss: 0.00011867315333802253\n",
            "Test Loss: 0.00026800956402439624\n",
            "Epoch 413\n",
            " ------------------------\n",
            "Train Loss: 0.00022212443582247943\n",
            "Train Loss: 9.692290768725798e-05\n",
            "Train Loss: 6.963237865420524e-06\n",
            "Train Loss: 6.624562956858426e-06\n",
            "Train Loss: 5.6559878430562094e-05\n",
            "Train Loss: 0.00011713922140188515\n",
            "Test Loss: 0.0002645478307385929\n",
            "Epoch 414\n",
            " ------------------------\n",
            "Train Loss: 0.00021925412875134498\n",
            "Train Loss: 9.566995868226513e-05\n",
            "Train Loss: 6.873243819427444e-06\n",
            "Train Loss: 6.538965863001067e-06\n",
            "Train Loss: 5.582840458373539e-05\n",
            "Train Loss: 0.000115624898171518\n",
            "Test Loss: 0.00026112703926628456\n",
            "Epoch 415\n",
            " ------------------------\n",
            "Train Loss: 0.00021642209321726114\n",
            "Train Loss: 9.443469025427476e-05\n",
            "Train Loss: 6.784536708437372e-06\n",
            "Train Loss: 6.454314188886201e-06\n",
            "Train Loss: 5.510678965947591e-05\n",
            "Train Loss: 0.00011413060565246269\n",
            "Test Loss: 0.0002577498016762547\n",
            "Epoch 416\n",
            " ------------------------\n",
            "Train Loss: 0.00021362544794101268\n",
            "Train Loss: 9.321491961600259e-05\n",
            "Train Loss: 6.69689643473248e-06\n",
            "Train Loss: 6.3708134803164285e-06\n",
            "Train Loss: 5.4393727623391896e-05\n",
            "Train Loss: 0.0001126542774727568\n",
            "Test Loss: 0.0002544178059906699\n",
            "Epoch 417\n",
            " ------------------------\n",
            "Train Loss: 0.00021086496417410672\n",
            "Train Loss: 9.201022476190701e-05\n",
            "Train Loss: 6.610398031625664e-06\n",
            "Train Loss: 6.288399163167924e-06\n",
            "Train Loss: 5.369057907955721e-05\n",
            "Train Loss: 0.00011119821283500642\n",
            "Test Loss: 0.0002511295388103463\n",
            "Epoch 418\n",
            " ------------------------\n",
            "Train Loss: 0.00020814045274164528\n",
            "Train Loss: 9.082173346541822e-05\n",
            "Train Loss: 6.525011031044414e-06\n",
            "Train Loss: 6.207078968145652e-06\n",
            "Train Loss: 5.2996423619333655e-05\n",
            "Train Loss: 0.0001097603962989524\n",
            "Test Loss: 0.0002478840615367517\n",
            "Epoch 419\n",
            " ------------------------\n",
            "Train Loss: 0.00020545149163808674\n",
            "Train Loss: 8.964835433289409e-05\n",
            "Train Loss: 6.4406422097817995e-06\n",
            "Train Loss: 6.12692838330986e-06\n",
            "Train Loss: 5.2311854233266786e-05\n",
            "Train Loss: 0.00010834186105057597\n",
            "Test Loss: 0.0002446811558911577\n",
            "Epoch 420\n",
            " ------------------------\n",
            "Train Loss: 0.00020279611635487527\n",
            "Train Loss: 8.848984725773335e-05\n",
            "Train Loss: 6.3575439526175614e-06\n",
            "Train Loss: 6.047569513611961e-06\n",
            "Train Loss: 5.1634979172376916e-05\n",
            "Train Loss: 0.00010694118100218475\n",
            "Test Loss: 0.00024151676188921556\n",
            "Epoch 421\n",
            " ------------------------\n",
            "Train Loss: 0.0002001761895371601\n",
            "Train Loss: 8.734620496397838e-05\n",
            "Train Loss: 6.275277883105446e-06\n",
            "Train Loss: 5.96944073549821e-06\n",
            "Train Loss: 5.096821041661315e-05\n",
            "Train Loss: 0.00010555944754742086\n",
            "Test Loss: 0.00023839543428039178\n",
            "Epoch 422\n",
            " ------------------------\n",
            "Train Loss: 0.000197588000446558\n",
            "Train Loss: 8.62172746565193e-05\n",
            "Train Loss: 6.1942228057887405e-06\n",
            "Train Loss: 5.892384706385201e-06\n",
            "Train Loss: 5.0309627113165334e-05\n",
            "Train Loss: 0.0001041957875713706\n",
            "Test Loss: 0.00023531621263828129\n",
            "Epoch 423\n",
            " ------------------------\n",
            "Train Loss: 0.0001950348523678258\n",
            "Train Loss: 8.510272164130583e-05\n",
            "Train Loss: 6.11400628258707e-06\n",
            "Train Loss: 5.816259545099456e-06\n",
            "Train Loss: 4.965946573065594e-05\n",
            "Train Loss: 0.0001028488768497482\n",
            "Test Loss: 0.00023227561177918687\n",
            "Epoch 424\n",
            " ------------------------\n",
            "Train Loss: 0.0001925141696119681\n",
            "Train Loss: 8.400304795941338e-05\n",
            "Train Loss: 6.03515445618541e-06\n",
            "Train Loss: 5.7410047702433076e-06\n",
            "Train Loss: 4.901766078546643e-05\n",
            "Train Loss: 0.00010152033064514399\n",
            "Test Loss: 0.00022927470854483545\n",
            "Epoch 425\n",
            " ------------------------\n",
            "Train Loss: 0.00019002643239218742\n",
            "Train Loss: 8.291783888125792e-05\n",
            "Train Loss: 5.9572162172116805e-06\n",
            "Train Loss: 5.666733159159776e-06\n",
            "Train Loss: 4.838358290726319e-05\n",
            "Train Loss: 0.00010020757326856256\n",
            "Test Loss: 0.00022631054162047803\n",
            "Epoch 426\n",
            " ------------------------\n",
            "Train Loss: 0.00018757175712380558\n",
            "Train Loss: 8.184622856788337e-05\n",
            "Train Loss: 5.88021111980197e-06\n",
            "Train Loss: 5.5936734497663565e-06\n",
            "Train Loss: 4.7759513108758256e-05\n",
            "Train Loss: 9.891464287647977e-05\n",
            "Test Loss: 0.00022338846611091867\n",
            "Epoch 427\n",
            " ------------------------\n",
            "Train Loss: 0.00018514615658205003\n",
            "Train Loss: 8.078746759565547e-05\n",
            "Train Loss: 5.8040245676238555e-06\n",
            "Train Loss: 5.521638286154484e-06\n",
            "Train Loss: 4.714351598522626e-05\n",
            "Train Loss: 9.763777779880911e-05\n",
            "Test Loss: 0.00022050421830499545\n",
            "Epoch 428\n",
            " ------------------------\n",
            "Train Loss: 0.00018275306501891464\n",
            "Train Loss: 7.974314939929172e-05\n",
            "Train Loss: 5.728979886043817e-06\n",
            "Train Loss: 5.450342086987803e-06\n",
            "Train Loss: 4.653473661164753e-05\n",
            "Train Loss: 9.637621406000108e-05\n",
            "Test Loss: 0.00021765569545095786\n",
            "Epoch 429\n",
            " ------------------------\n",
            "Train Loss: 0.00018039064889308065\n",
            "Train Loss: 7.87126409704797e-05\n",
            "Train Loss: 5.654970209434396e-06\n",
            "Train Loss: 5.379800313676242e-06\n",
            "Train Loss: 4.593249104800634e-05\n",
            "Train Loss: 9.512984252069145e-05\n",
            "Test Loss: 0.0002148401690647006\n",
            "Epoch 430\n",
            " ------------------------\n",
            "Train Loss: 0.0001780604216037318\n",
            "Train Loss: 7.769614603603259e-05\n",
            "Train Loss: 5.58195324629196e-06\n",
            "Train Loss: 5.3102330639376305e-06\n",
            "Train Loss: 4.533900209935382e-05\n",
            "Train Loss: 9.390024933964014e-05\n",
            "Test Loss: 0.0002120643257512711\n",
            "Epoch 431\n",
            " ------------------------\n",
            "Train Loss: 0.00017575948731973767\n",
            "Train Loss: 7.669179467484355e-05\n",
            "Train Loss: 5.5097939366532955e-06\n",
            "Train Loss: 5.241583039605757e-06\n",
            "Train Loss: 4.475303285289556e-05\n",
            "Train Loss: 9.268727444577962e-05\n",
            "Test Loss: 0.0002093238872475922\n",
            "Epoch 432\n",
            " ------------------------\n",
            "Train Loss: 0.0001734883844619617\n",
            "Train Loss: 7.570091838715598e-05\n",
            "Train Loss: 5.438676453195512e-06\n",
            "Train Loss: 5.17376201969455e-06\n",
            "Train Loss: 4.417479067342356e-05\n",
            "Train Loss: 9.148899698629975e-05\n",
            "Test Loss: 0.00020661853341152892\n",
            "Epoch 433\n",
            " ------------------------\n",
            "Train Loss: 0.00017124634177889675\n",
            "Train Loss: 7.472276774933562e-05\n",
            "Train Loss: 5.368311121856095e-06\n",
            "Train Loss: 5.106933713250328e-06\n",
            "Train Loss: 4.36032896686811e-05\n",
            "Train Loss: 9.030591900227591e-05\n",
            "Test Loss: 0.000203947740374133\n",
            "Epoch 434\n",
            " ------------------------\n",
            "Train Loss: 0.00016903319919947535\n",
            "Train Loss: 7.375715358648449e-05\n",
            "Train Loss: 5.298997166391928e-06\n",
            "Train Loss: 5.040913947595982e-06\n",
            "Train Loss: 4.303937384975143e-05\n",
            "Train Loss: 8.91389645403251e-05\n",
            "Test Loss: 0.00020131017663516104\n",
            "Epoch 435\n",
            " ------------------------\n",
            "Train Loss: 0.0001668489130679518\n",
            "Train Loss: 7.28037630324252e-05\n",
            "Train Loss: 5.2304476412246e-06\n",
            "Train Loss: 4.975879619451007e-06\n",
            "Train Loss: 4.248418554197997e-05\n",
            "Train Loss: 8.798800263321027e-05\n",
            "Test Loss: 0.00019871172844432294\n",
            "Epoch 436\n",
            " ------------------------\n",
            "Train Loss: 0.00016469178081024438\n",
            "Train Loss: 7.186258881120011e-05\n",
            "Train Loss: 5.162836714589503e-06\n",
            "Train Loss: 4.911630639981013e-06\n",
            "Train Loss: 4.1935258195735514e-05\n",
            "Train Loss: 8.685117063578218e-05\n",
            "Test Loss: 0.00019614509074017406\n",
            "Epoch 437\n",
            " ------------------------\n",
            "Train Loss: 0.00016256290837191045\n",
            "Train Loss: 7.093357271514833e-05\n",
            "Train Loss: 5.0960725275217555e-06\n",
            "Train Loss: 4.8481633712071925e-06\n",
            "Train Loss: 4.139395241509192e-05\n",
            "Train Loss: 8.573003287892789e-05\n",
            "Test Loss: 0.00019361101294634864\n",
            "Epoch 438\n",
            " ------------------------\n",
            "Train Loss: 0.00016046216478571296\n",
            "Train Loss: 7.001673657214269e-05\n",
            "Train Loss: 5.030219654145185e-06\n",
            "Train Loss: 4.7855965021881275e-06\n",
            "Train Loss: 4.0858718421077356e-05\n",
            "Train Loss: 8.462055848212913e-05\n",
            "Test Loss: 0.00019110669381916523\n",
            "Epoch 439\n",
            " ------------------------\n",
            "Train Loss: 0.00015838925901334733\n",
            "Train Loss: 6.91122404532507e-05\n",
            "Train Loss: 4.9652148845780175e-06\n",
            "Train Loss: 4.723622623714618e-06\n",
            "Train Loss: 4.033004734083079e-05\n",
            "Train Loss: 8.352701843250543e-05\n",
            "Test Loss: 0.00018863620789488778\n",
            "Epoch 440\n",
            " ------------------------\n",
            "Train Loss: 0.00015634286683052778\n",
            "Train Loss: 6.821980787208304e-05\n",
            "Train Loss: 4.9010900511348154e-06\n",
            "Train Loss: 4.66247001895681e-06\n",
            "Train Loss: 3.980865585617721e-05\n",
            "Train Loss: 8.244709169957787e-05\n",
            "Test Loss: 0.0001861991113401018\n",
            "Epoch 441\n",
            " ------------------------\n",
            "Train Loss: 0.00015432207146659493\n",
            "Train Loss: 6.733759073540568e-05\n",
            "Train Loss: 4.8378014980698936e-06\n",
            "Train Loss: 4.602263288688846e-06\n",
            "Train Loss: 3.929504964617081e-05\n",
            "Train Loss: 8.13828082755208e-05\n",
            "Test Loss: 0.00018379539687884971\n",
            "Epoch 442\n",
            " ------------------------\n",
            "Train Loss: 0.00015232726582325995\n",
            "Train Loss: 6.646706606261432e-05\n",
            "Train Loss: 4.775179604621371e-06\n",
            "Train Loss: 4.542974693322321e-06\n",
            "Train Loss: 3.8787617086200044e-05\n",
            "Train Loss: 8.033130143303424e-05\n",
            "Test Loss: 0.00018142000772058964\n",
            "Epoch 443\n",
            " ------------------------\n",
            "Train Loss: 0.00015035872638691217\n",
            "Train Loss: 6.560813926625997e-05\n",
            "Train Loss: 4.71344583274913e-06\n",
            "Train Loss: 4.484258624870563e-06\n",
            "Train Loss: 3.8286143535515293e-05\n",
            "Train Loss: 7.92935534263961e-05\n",
            "Test Loss: 0.00017907543224282563\n",
            "Epoch 444\n",
            " ------------------------\n",
            "Train Loss: 0.00014841534721199423\n",
            "Train Loss: 6.475990085164085e-05\n",
            "Train Loss: 4.652573807106819e-06\n",
            "Train Loss: 4.426384748512646e-06\n",
            "Train Loss: 3.7792011426063254e-05\n",
            "Train Loss: 7.826941146049649e-05\n",
            "Test Loss: 0.000176763060153462\n",
            "Epoch 445\n",
            " ------------------------\n",
            "Train Loss: 0.00014649702643509954\n",
            "Train Loss: 6.392302748281509e-05\n",
            "Train Loss: 4.5923839024908375e-06\n",
            "Train Loss: 4.369177531771129e-06\n",
            "Train Loss: 3.730364187504165e-05\n",
            "Train Loss: 7.72587227402255e-05\n",
            "Test Loss: 0.0001744784094626084\n",
            "Epoch 446\n",
            " ------------------------\n",
            "Train Loss: 0.00014460440434049815\n",
            "Train Loss: 6.309727905318141e-05\n",
            "Train Loss: 4.533053470368031e-06\n",
            "Train Loss: 4.312694727559574e-06\n",
            "Train Loss: 3.6821085814153776e-05\n",
            "Train Loss: 7.625929720234126e-05\n",
            "Test Loss: 0.00017222359747393057\n",
            "Epoch 447\n",
            " ------------------------\n",
            "Train Loss: 0.00014273628767114133\n",
            "Train Loss: 6.22821316937916e-05\n",
            "Train Loss: 4.474490651773522e-06\n",
            "Train Loss: 4.256968168192543e-06\n",
            "Train Loss: 3.634577296907082e-05\n",
            "Train Loss: 7.527361594839022e-05\n",
            "Test Loss: 0.00016999770741676912\n",
            "Epoch 448\n",
            " ------------------------\n",
            "Train Loss: 0.00014089127944316715\n",
            "Train Loss: 6.147668318590149e-05\n",
            "Train Loss: 4.416662250150694e-06\n",
            "Train Loss: 4.202101081318688e-06\n",
            "Train Loss: 3.587672836147249e-05\n",
            "Train Loss: 7.430309051414952e-05\n",
            "Test Loss: 0.00016780295845819637\n",
            "Epoch 449\n",
            " ------------------------\n",
            "Train Loss: 0.0001390693214489147\n",
            "Train Loss: 6.068161383154802e-05\n",
            "Train Loss: 4.359536887932336e-06\n",
            "Train Loss: 4.147718755120877e-06\n",
            "Train Loss: 3.541240221238695e-05\n",
            "Train Loss: 7.334192923735827e-05\n",
            "Test Loss: 0.0001656326203374192\n",
            "Epoch 450\n",
            " ------------------------\n",
            "Train Loss: 0.00013727368786931038\n",
            "Train Loss: 5.9898706240346655e-05\n",
            "Train Loss: 4.303255082049873e-06\n",
            "Train Loss: 4.094053565495415e-06\n",
            "Train Loss: 3.4954791772179306e-05\n",
            "Train Loss: 7.239310070872307e-05\n",
            "Test Loss: 0.0001634916552575305\n",
            "Epoch 451\n",
            " ------------------------\n",
            "Train Loss: 0.0001354995183646679\n",
            "Train Loss: 5.912426422582939e-05\n",
            "Train Loss: 4.247594915796071e-06\n",
            "Train Loss: 4.041124611831037e-06\n",
            "Train Loss: 3.450308940955438e-05\n",
            "Train Loss: 7.145802373997867e-05\n",
            "Test Loss: 0.0001613805907254573\n",
            "Epoch 452\n",
            " ------------------------\n",
            "Train Loss: 0.0001337482826784253\n",
            "Train Loss: 5.83602195547428e-05\n",
            "Train Loss: 4.192758296994725e-06\n",
            "Train Loss: 3.988900061813183e-06\n",
            "Train Loss: 3.405682946322486e-05\n",
            "Train Loss: 7.053402077872306e-05\n",
            "Test Loss: 0.00015929254732327536\n",
            "Epoch 453\n",
            " ------------------------\n",
            "Train Loss: 0.00013202121772337705\n",
            "Train Loss: 5.760684871347621e-05\n",
            "Train Loss: 4.138636086281622e-06\n",
            "Train Loss: 3.937313067581272e-06\n",
            "Train Loss: 3.361698691151105e-05\n",
            "Train Loss: 6.962251791264862e-05\n",
            "Test Loss: 0.00015723390970379114\n",
            "Epoch 454\n",
            " ------------------------\n",
            "Train Loss: 0.00013031538401264697\n",
            "Train Loss: 5.6862430938053876e-05\n",
            "Train Loss: 4.085092314198846e-06\n",
            "Train Loss: 3.88643911719555e-06\n",
            "Train Loss: 3.318207382108085e-05\n",
            "Train Loss: 6.872286030557007e-05\n",
            "Test Loss: 0.00015520151282544248\n",
            "Epoch 455\n",
            " ------------------------\n",
            "Train Loss: 0.00012863169831689447\n",
            "Train Loss: 5.6128053984139115e-05\n",
            "Train Loss: 4.032464858028106e-06\n",
            "Train Loss: 3.836068117379909e-06\n",
            "Train Loss: 3.275269045843743e-05\n",
            "Train Loss: 6.78339711157605e-05\n",
            "Test Loss: 0.0001531953239464201\n",
            "Epoch 456\n",
            " ------------------------\n",
            "Train Loss: 0.00012697000056505203\n",
            "Train Loss: 5.540286656469107e-05\n",
            "Train Loss: 3.980385372415185e-06\n",
            "Train Loss: 3.786476099776337e-06\n",
            "Train Loss: 3.232966992072761e-05\n",
            "Train Loss: 6.695670890621841e-05\n",
            "Test Loss: 0.0001512151757196989\n",
            "Epoch 457\n",
            " ------------------------\n",
            "Train Loss: 0.00012532906839624047\n",
            "Train Loss: 5.4686941439285874e-05\n",
            "Train Loss: 3.92889933209517e-06\n",
            "Train Loss: 3.737527094926918e-06\n",
            "Train Loss: 3.191142968717031e-05\n",
            "Train Loss: 6.609100091736764e-05\n",
            "Test Loss: 0.00014926044605090283\n",
            "Epoch 458\n",
            " ------------------------\n",
            "Train Loss: 0.00012370954209472984\n",
            "Train Loss: 5.39806169399526e-05\n",
            "Train Loss: 3.8781913644925226e-06\n",
            "Train Loss: 3.6892452044412494e-06\n",
            "Train Loss: 3.149928306811489e-05\n",
            "Train Loss: 6.523716729134321e-05\n",
            "Test Loss: 0.0001473304109822493\n",
            "Epoch 459\n",
            " ------------------------\n",
            "Train Loss: 0.00012211121793370694\n",
            "Train Loss: 5.3283303714124486e-05\n",
            "Train Loss: 3.828115040960256e-06\n",
            "Train Loss: 3.64143238584802e-06\n",
            "Train Loss: 3.109100725851022e-05\n",
            "Train Loss: 6.43917519482784e-05\n",
            "Test Loss: 0.00014542491408064961\n",
            "Epoch 460\n",
            " ------------------------\n",
            "Train Loss: 0.00012053397222189233\n",
            "Train Loss: 5.259496538201347e-05\n",
            "Train Loss: 3.778704240176012e-06\n",
            "Train Loss: 3.594226427594549e-06\n",
            "Train Loss: 3.068883961532265e-05\n",
            "Train Loss: 6.355898949550465e-05\n",
            "Test Loss: 0.00014354355880641378\n",
            "Epoch 461\n",
            " ------------------------\n",
            "Train Loss: 0.0001189760077977553\n",
            "Train Loss: 5.191547825234011e-05\n",
            "Train Loss: 3.729875970748253e-06\n",
            "Train Loss: 3.5477373785397504e-06\n",
            "Train Loss: 3.0291586881503463e-05\n",
            "Train Loss: 6.27368499408476e-05\n",
            "Test Loss: 0.0001416880295437295\n",
            "Epoch 462\n",
            " ------------------------\n",
            "Train Loss: 0.0001174389835796319\n",
            "Train Loss: 5.1244936912553385e-05\n",
            "Train Loss: 3.6816725241806125e-06\n",
            "Train Loss: 3.5017837944906205e-06\n",
            "Train Loss: 2.989980021084193e-05\n",
            "Train Loss: 6.192614819156006e-05\n",
            "Test Loss: 0.00013985607074573636\n",
            "Epoch 463\n",
            " ------------------------\n",
            "Train Loss: 0.00011592088412726298\n",
            "Train Loss: 5.058263923274353e-05\n",
            "Train Loss: 3.6341259601613274e-06\n",
            "Train Loss: 3.456507101873285e-06\n",
            "Train Loss: 2.9513772460632026e-05\n",
            "Train Loss: 6.112630217103288e-05\n",
            "Test Loss: 0.00013805007620248944\n",
            "Epoch 464\n",
            " ------------------------\n",
            "Train Loss: 0.00011442254617577419\n",
            "Train Loss: 4.9928406951949e-05\n",
            "Train Loss: 3.5870841657015262e-06\n",
            "Train Loss: 3.4119839256163687e-06\n",
            "Train Loss: 2.913288517447654e-05\n",
            "Train Loss: 6.033724275766872e-05\n",
            "Test Loss: 0.00013626760483020917\n",
            "Epoch 465\n",
            " ------------------------\n",
            "Train Loss: 0.00011294249998172745\n",
            "Train Loss: 4.9282272811979055e-05\n",
            "Train Loss: 3.540705165505642e-06\n",
            "Train Loss: 3.36797529598698e-06\n",
            "Train Loss: 2.875702739402186e-05\n",
            "Train Loss: 5.9558824432315305e-05\n",
            "Test Loss: 0.00013450690312311053\n",
            "Epoch 466\n",
            " ------------------------\n",
            "Train Loss: 0.00011148309567943215\n",
            "Train Loss: 4.864534639636986e-05\n",
            "Train Loss: 3.494850943752681e-06\n",
            "Train Loss: 3.3246267321374035e-06\n",
            "Train Loss: 2.8385804398567416e-05\n",
            "Train Loss: 5.878955562366173e-05\n",
            "Test Loss: 0.00013277088510221802\n",
            "Epoch 467\n",
            " ------------------------\n",
            "Train Loss: 0.0001100417721318081\n",
            "Train Loss: 4.8016274377005175e-05\n",
            "Train Loss: 3.449698169788462e-06\n",
            "Train Loss: 3.281673798483098e-06\n",
            "Train Loss: 2.801901791826822e-05\n",
            "Train Loss: 5.802985106129199e-05\n",
            "Test Loss: 0.00013105403922963887\n",
            "Epoch 468\n",
            " ------------------------\n",
            "Train Loss: 0.00010861941700568423\n",
            "Train Loss: 4.739550422527827e-05\n",
            "Train Loss: 3.4051267903123517e-06\n",
            "Train Loss: 3.239259740439593e-06\n",
            "Train Loss: 2.765737008303404e-05\n",
            "Train Loss: 5.728050018660724e-05\n",
            "Test Loss: 0.00012936234634253196\n",
            "Epoch 469\n",
            " ------------------------\n",
            "Train Loss: 0.00010721530998125672\n",
            "Train Loss: 4.6783072320977226e-05\n",
            "Train Loss: 3.361069275342743e-06\n",
            "Train Loss: 3.197449132130714e-06\n",
            "Train Loss: 2.7299674911773764e-05\n",
            "Train Loss: 5.653960397467017e-05\n",
            "Test Loss: 0.0001276886287087109\n",
            "Epoch 470\n",
            " ------------------------\n",
            "Train Loss: 0.00010583028051769361\n",
            "Train Loss: 4.6178676711861044e-05\n",
            "Train Loss: 3.3176891065522796e-06\n",
            "Train Loss: 3.1560114166495623e-06\n",
            "Train Loss: 2.6946585421683267e-05\n",
            "Train Loss: 5.580930155701935e-05\n",
            "Test Loss: 0.00012604040966834873\n",
            "Epoch 471\n",
            " ------------------------\n",
            "Train Loss: 0.00010446200758451596\n",
            "Train Loss: 4.5581658923765644e-05\n",
            "Train Loss: 3.2747273053246317e-06\n",
            "Train Loss: 3.11525582219474e-06\n",
            "Train Loss: 2.6598505428410135e-05\n",
            "Train Loss: 5.508731192094274e-05\n",
            "Test Loss: 0.0001244100567419082\n",
            "Epoch 472\n",
            " ------------------------\n",
            "Train Loss: 0.00010311273217666894\n",
            "Train Loss: 4.499281567404978e-05\n",
            "Train Loss: 3.2324289804819273e-06\n",
            "Train Loss: 3.0749595225643134e-06\n",
            "Train Loss: 2.6254638214595616e-05\n",
            "Train Loss: 5.437529398477636e-05\n",
            "Test Loss: 0.0001228023502335418\n",
            "Epoch 473\n",
            " ------------------------\n",
            "Train Loss: 0.00010178012598771602\n",
            "Train Loss: 4.4411564886104316e-05\n",
            "Train Loss: 3.1906497497402597e-06\n",
            "Train Loss: 3.0352634894370567e-06\n",
            "Train Loss: 2.5915514925145544e-05\n",
            "Train Loss: 5.367314588511363e-05\n",
            "Test Loss: 0.00012121648614993319\n",
            "Epoch 474\n",
            " ------------------------\n",
            "Train Loss: 0.00010046482930192724\n",
            "Train Loss: 4.3837215343955904e-05\n",
            "Train Loss: 3.1494262202613754e-06\n",
            "Train Loss: 2.9961054224258987e-06\n",
            "Train Loss: 2.5580970032024197e-05\n",
            "Train Loss: 5.298034375300631e-05\n",
            "Test Loss: 0.00011965018711634912\n",
            "Epoch 475\n",
            " ------------------------\n",
            "Train Loss: 9.916706767398864e-05\n",
            "Train Loss: 4.32712840847671e-05\n",
            "Train Loss: 3.10877658193931e-06\n",
            "Train Loss: 2.957395736302715e-06\n",
            "Train Loss: 2.525012314436026e-05\n",
            "Train Loss: 5.229548332863487e-05\n",
            "Test Loss: 0.00011810317300842144\n",
            "Epoch 476\n",
            " ------------------------\n",
            "Train Loss: 9.788580791791901e-05\n",
            "Train Loss: 4.271232319297269e-05\n",
            "Train Loss: 3.068562818953069e-06\n",
            "Train Loss: 2.9191239718784345e-06\n",
            "Train Loss: 2.4923598175519146e-05\n",
            "Train Loss: 5.161892477190122e-05\n",
            "Test Loss: 0.0001165768553619273\n",
            "Epoch 477\n",
            " ------------------------\n",
            "Train Loss: 9.662053344072774e-05\n",
            "Train Loss: 4.215989974909462e-05\n",
            "Train Loss: 3.0288408652268117e-06\n",
            "Train Loss: 2.881543650801177e-06\n",
            "Train Loss: 2.4602311896160245e-05\n",
            "Train Loss: 5.095359665574506e-05\n",
            "Test Loss: 0.00011507273302413523\n",
            "Epoch 478\n",
            " ------------------------\n",
            "Train Loss: 9.537149890093133e-05\n",
            "Train Loss: 4.161496690358035e-05\n",
            "Train Loss: 2.989677795994794e-06\n",
            "Train Loss: 2.844330765583436e-06\n",
            "Train Loss: 2.4284396204166114e-05\n",
            "Train Loss: 5.029512249166146e-05\n",
            "Test Loss: 0.00011358588744769804\n",
            "Epoch 479\n",
            " ------------------------\n",
            "Train Loss: 9.413857333129272e-05\n",
            "Train Loss: 4.1076749766943976e-05\n",
            "Train Loss: 2.951052692878875e-06\n",
            "Train Loss: 2.807628334267065e-06\n",
            "Train Loss: 2.3970786060090177e-05\n",
            "Train Loss: 4.964512481819838e-05\n",
            "Test Loss: 0.00011211873425054364\n",
            "Epoch 480\n",
            " ------------------------\n",
            "Train Loss: 9.292158210882917e-05\n",
            "Train Loss: 4.054552482557483e-05\n",
            "Train Loss: 2.912859599746298e-06\n",
            "Train Loss: 2.7713529107131762e-06\n",
            "Train Loss: 2.3661590603296645e-05\n",
            "Train Loss: 4.9004360334947705e-05\n",
            "Test Loss: 0.00011067075320170261\n",
            "Epoch 481\n",
            " ------------------------\n",
            "Train Loss: 9.172112186206505e-05\n",
            "Train Loss: 4.002188870799728e-05\n",
            "Train Loss: 2.8752360776707064e-06\n",
            "Train Loss: 2.7355451948096743e-06\n",
            "Train Loss: 2.335532735742163e-05\n",
            "Train Loss: 4.837024607695639e-05\n",
            "Test Loss: 0.00010923889567493461\n",
            "Epoch 482\n",
            " ------------------------\n",
            "Train Loss: 9.053594112629071e-05\n",
            "Train Loss: 3.950485188397579e-05\n",
            "Train Loss: 2.8381059564708266e-06\n",
            "Train Loss: 2.7001058242603904e-06\n",
            "Train Loss: 2.3053687982610427e-05\n",
            "Train Loss: 4.7745223128004e-05\n",
            "Test Loss: 0.00010782702520373277\n",
            "Epoch 483\n",
            " ------------------------\n",
            "Train Loss: 8.936600352171808e-05\n",
            "Train Loss: 3.899421062669717e-05\n",
            "Train Loss: 2.8014235340378946e-06\n",
            "Train Loss: 2.665187366801547e-06\n",
            "Train Loss: 2.2755470126867294e-05\n",
            "Train Loss: 4.712833106168546e-05\n",
            "Test Loss: 0.00010643287168932147\n",
            "Epoch 484\n",
            " ------------------------\n",
            "Train Loss: 8.821066148811951e-05\n",
            "Train Loss: 3.8490088627440855e-05\n",
            "Train Loss: 2.765282488326193e-06\n",
            "Train Loss: 2.6307855023333104e-06\n",
            "Train Loss: 2.2461495973402634e-05\n",
            "Train Loss: 4.651954077417031e-05\n",
            "Test Loss: 0.00010505885074962862\n",
            "Epoch 485\n",
            " ------------------------\n",
            "Train Loss: 8.707010420039296e-05\n",
            "Train Loss: 3.7992456782376394e-05\n",
            "Train Loss: 2.72943339041376e-06\n",
            "Train Loss: 2.5968111003749073e-06\n",
            "Train Loss: 2.2171152522787452e-05\n",
            "Train Loss: 4.591752076521516e-05\n",
            "Test Loss: 0.00010369937444920652\n",
            "Epoch 486\n",
            " ------------------------\n",
            "Train Loss: 8.594500104663894e-05\n",
            "Train Loss: 3.750144605874084e-05\n",
            "Train Loss: 2.6942320801026653e-06\n",
            "Train Loss: 2.563211182859959e-06\n",
            "Train Loss: 2.1884638044866733e-05\n",
            "Train Loss: 4.532425373326987e-05\n",
            "Test Loss: 0.00010235917579848319\n",
            "Epoch 487\n",
            " ------------------------\n",
            "Train Loss: 8.483447891194373e-05\n",
            "Train Loss: 3.701707100844942e-05\n",
            "Train Loss: 2.6594354949338594e-06\n",
            "Train Loss: 2.5300328161392827e-06\n",
            "Train Loss: 2.1601395928882994e-05\n",
            "Train Loss: 4.473842273000628e-05\n",
            "Test Loss: 0.00010103604290634394\n",
            "Epoch 488\n",
            " ------------------------\n",
            "Train Loss: 8.37387633509934e-05\n",
            "Train Loss: 3.653896783362143e-05\n",
            "Train Loss: 2.625104343678686e-06\n",
            "Train Loss: 2.497329433026607e-06\n",
            "Train Loss: 2.1322082830010913e-05\n",
            "Train Loss: 4.415998046169989e-05\n",
            "Test Loss: 9.97305141936522e-05\n",
            "Epoch 489\n",
            " ------------------------\n",
            "Train Loss: 8.265683572972193e-05\n",
            "Train Loss: 3.606677273637615e-05\n",
            "Train Loss: 2.5911319880833616e-06\n",
            "Train Loss: 2.4651128569530556e-06\n",
            "Train Loss: 2.1047098925919272e-05\n",
            "Train Loss: 4.359001832199283e-05\n",
            "Test Loss: 9.844240048551e-05\n",
            "Epoch 490\n",
            " ------------------------\n",
            "Train Loss: 8.158844866557047e-05\n",
            "Train Loss: 3.5600631235865876e-05\n",
            "Train Loss: 2.557641664679977e-06\n",
            "Train Loss: 2.4332628072443185e-06\n",
            "Train Loss: 2.077494718832895e-05\n",
            "Train Loss: 4.3026106141041964e-05\n",
            "Test Loss: 9.717022840050049e-05\n",
            "Epoch 491\n",
            " ------------------------\n",
            "Train Loss: 8.05341187515296e-05\n",
            "Train Loss: 3.514086711220443e-05\n",
            "Train Loss: 2.524619958421681e-06\n",
            "Train Loss: 2.4017263058340177e-06\n",
            "Train Loss: 2.0506366126937792e-05\n",
            "Train Loss: 4.2470142943784595e-05\n",
            "Test Loss: 9.591460548108444e-05\n",
            "Epoch 492\n",
            " ------------------------\n",
            "Train Loss: 7.949294376885518e-05\n",
            "Train Loss: 3.46863926097285e-05\n",
            "Train Loss: 2.491943405402708e-06\n",
            "Train Loss: 2.3708123535470804e-06\n",
            "Train Loss: 2.024155946855899e-05\n",
            "Train Loss: 4.1921259253285825e-05\n",
            "Test Loss: 9.467445124755614e-05\n",
            "Epoch 493\n",
            " ------------------------\n",
            "Train Loss: 7.84658914199099e-05\n",
            "Train Loss: 3.423825910431333e-05\n",
            "Train Loss: 2.459799588905298e-06\n",
            "Train Loss: 2.3400068585033296e-06\n",
            "Train Loss: 1.997976505663246e-05\n",
            "Train Loss: 4.137940413784236e-05\n",
            "Test Loss: 9.345129001303576e-05\n",
            "Epoch 494\n",
            " ------------------------\n",
            "Train Loss: 7.745174661977217e-05\n",
            "Train Loss: 3.379550253157504e-05\n",
            "Train Loss: 2.4279856916109566e-06\n",
            "Train Loss: 2.309893488927628e-06\n",
            "Train Loss: 1.9721768694580533e-05\n",
            "Train Loss: 4.0844883187673986e-05\n",
            "Test Loss: 9.224391760653816e-05\n",
            "Epoch 495\n",
            " ------------------------\n",
            "Train Loss: 7.645038567716256e-05\n",
            "Train Loss: 3.3358606742694974e-05\n",
            "Train Loss: 2.396540594418184e-06\n",
            "Train Loss: 2.2800429633207386e-06\n",
            "Train Loss: 1.9467013771645725e-05\n",
            "Train Loss: 4.031724893138744e-05\n",
            "Test Loss: 9.105206481763162e-05\n",
            "Epoch 496\n",
            " ------------------------\n",
            "Train Loss: 7.546189590357244e-05\n",
            "Train Loss: 3.292711335234344e-05\n",
            "Train Loss: 2.365566842854605e-06\n",
            "Train Loss: 2.2505926153826294e-06\n",
            "Train Loss: 1.9215274733141996e-05\n",
            "Train Loss: 3.9796053897589445e-05\n",
            "Test Loss: 8.987452383735217e-05\n",
            "Epoch 497\n",
            " ------------------------\n",
            "Train Loss: 7.448728865711018e-05\n",
            "Train Loss: 3.250199370086193e-05\n",
            "Train Loss: 2.3350307856162544e-06\n",
            "Train Loss: 2.221420118075912e-06\n",
            "Train Loss: 1.8966969946632162e-05\n",
            "Train Loss: 3.9281985664274544e-05\n",
            "Test Loss: 8.871405589161441e-05\n",
            "Epoch 498\n",
            " ------------------------\n",
            "Train Loss: 7.35243083909154e-05\n",
            "Train Loss: 3.208170164725743e-05\n",
            "Train Loss: 2.304810323039419e-06\n",
            "Train Loss: 2.19289950109669e-06\n",
            "Train Loss: 1.8722324966802262e-05\n",
            "Train Loss: 3.877496419590898e-05\n",
            "Test Loss: 8.75690566317644e-05\n",
            "Epoch 499\n",
            " ------------------------\n",
            "Train Loss: 7.257420656969771e-05\n",
            "Train Loss: 3.166711758240126e-05\n",
            "Train Loss: 2.275033239129698e-06\n",
            "Train Loss: 2.164447550967452e-06\n",
            "Train Loss: 1.8480255675967783e-05\n",
            "Train Loss: 3.8273832615232095e-05\n",
            "Test Loss: 8.643607725389302e-05\n",
            "Epoch 500\n",
            " ------------------------\n",
            "Train Loss: 7.16367649147287e-05\n",
            "Train Loss: 3.125813600490801e-05\n",
            "Train Loss: 2.2457068098447053e-06\n",
            "Train Loss: 2.1365037810028298e-06\n",
            "Train Loss: 1.824113496695645e-05\n",
            "Train Loss: 3.7778889236506075e-05\n",
            "Test Loss: 8.53189267218113e-05\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with model\n",
        "with torch.inference_mode():\n",
        "    X_test = X_test.to(device)\n",
        "    pred = model(X_test)\n",
        "\n",
        "plot_data(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Iu3amQ71o5XO",
        "outputId": "dbd233e3-7f4d-4190-f921-2c457c2d8bbc"
      },
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the model"
      ],
      "metadata": {
        "id": "a8YbSL2D14c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "hWF00OGG18iS",
        "outputId": "0af9bbe9-4fba-4b7e-f245-fb12036914dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘models’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"models/model_states.pth\")\n",
        "print(\"Saved Model State to model_states.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlAhtUpDpCHH",
        "outputId": "5c3b42b2-81e1-4af8-c2ba-f2459fda181f"
      },
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Model State to model_states.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> Q6: Can I save the model structure along the dict states ? </font>\n",
        "\n",
        "Answer: Yeah, you can"
      ],
      "metadata": {
        "id": "Ndevfbwh2DVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"models/model.pt\") # By convention we use .pt or .pth \n",
        "print(\"Saved Entire Model to model.pt\")"
      ],
      "metadata": {
        "id": "ysInwFou1_A7",
        "outputId": "1f703631-71a4-4700-c26a-a9720f6746a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Entire Model to model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the Model"
      ],
      "metadata": {
        "id": "mdGJp0RK2nfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegressionModel()\n",
        "model.load_state_dict(torch.load(\"models/model_states.pth\"))"
      ],
      "metadata": {
        "id": "SxLPPgfw2i2E",
        "outputId": "ccf7cca1-5ed7-416f-a02a-a8b957326726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 413,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 413
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Entire model\n",
        "\n",
        "model=torch.load(\"models/model.pt\")"
      ],
      "metadata": {
        "id": "06t2g6WM2sJf"
      },
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Prediction"
      ],
      "metadata": {
        "id": "kMwPLmsS3Qo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model (nn Module) in evaluation mode\n",
        "model.eval()\n",
        "x, y = X_test[0], y_test[0]\n",
        "with torch.no_grad():\n",
        "  pred = model(x)\n",
        "  print(f\"Predicted: {pred}, Actual: {y}\")"
      ],
      "metadata": {
        "id": "NeUCXOa-2vyM",
        "outputId": "0563b69b-3c3d-4f6f-ba44-c5d4eaff48d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: tensor([0.8645], device='cuda:0'), Actual: tensor([0.8693])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> Q7: Yoour turn, use the model to make new prediction ? </font>"
      ],
      "metadata": {
        "id": "RW36SMFP3qPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Code here ----------------------"
      ],
      "metadata": {
        "id": "t6B5yN0d35kc"
      },
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it!!! You made this far"
      ],
      "metadata": {
        "id": "4OOPzzJc3h3t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqpW5DPR3nTD"
      },
      "execution_count": 416,
      "outputs": []
    }
  ]
}